{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "-qHc6ppHjcrP"
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2I85RhnjcrQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "CACHE = {}\n",
    "class SkipExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        print(\"Skip execution\")\n",
    "        clear_output()\n",
    "\n",
    "try:\n",
    "    import python_speech_features\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    raise SkipExecution\n",
    "\n",
    "!pip install kaggle python_speech_features dtw fastdtw dtaidistance AudAugio pyrubberband --upgrade -q\n",
    "!apt install soundstretch rubberband-cli librubberband2 libsndfile1 > /dev/null\n",
    "# !kaggle datasets download -d mfekadu/darpa-timit-acousticphonetic-continuous-speech > /dev/null\n",
    "!gdown -O data.zip --id \"15MxBckNzyEjO7cpY38O38NaWnssShl2l\"\n",
    "!unzip data.zip > /dev/null\n",
    "!git clone https://github.com/vegetablejuiceftw/soft-pointer-networks spn\n",
    "# !mv soft-pointer-networks/* .\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "j7gLLrj8jcrV",
    "outputId": "c98d74e0-e8ea-4b11-f3a1-d8d585280643",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# packges\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from python_speech_features import logfbank, mfcc\n",
    "from matplotlib import cm\n",
    "from matplotlib.pyplot import figure\n",
    "from collections import Counter\n",
    "import json\n",
    "from time import sleep\n",
    "from google.colab import drive\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import cProfile\n",
    "from typing import NamedTuple, List\n",
    "from random import choice, sample, Random\n",
    "import random\n",
    "import math\n",
    "import contextlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import soundfile as sf\n",
    "import pyrubberband as pyrb\n",
    "import scipy.io.wavfile as wavfile\n",
    "import librosa\n",
    "from IPython.display import Audio, Image, display\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import namedtuple\n",
    "from torchtext.legacy.data import BucketIterator, Field\n",
    "\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from matplotlib.pyplot import figure\n",
    "import json\n",
    "import gc\n",
    "import uuid\n",
    "\n",
    "from pprint import pprint\n",
    "from collections import OrderedDict, defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.legacy.data import BucketIterator, RawField\n",
    "from collections import namedtuple\n",
    "from torchtext.legacy.data import BucketIterator, Field\n",
    "\n",
    "from dtw import dtw, accelerated_dtw as adtw\n",
    "from fastdtw import fastdtw, dtw as slowdtw\n",
    "from dtaidistance import dtw as dtaidtw\n",
    "from dtaidistance.dtw_ndim import warping_paths\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# caching\n",
    "duration_model = None\n",
    "duration_cache = {}\n",
    "\n",
    "BASE_PATH = '/content/TIMIT-PLUS'\n",
    "FULL_FODLER_PATH = join(BASE_PATH, 'data')\n",
    "INPUT_SIZE = 26\n",
    "\n",
    "# WIN_STEP = 0.015\n",
    "WIN_STEP = 0.010\n",
    "# WIN_STEP = 0.005\n",
    "\n",
    "WIN_SIZE = 0.025\n",
    "# WIN_SIZE = 0.030\n",
    "# WIN_SIZE = 0.020\n",
    "# WIN_SIZE = 0.010\n",
    "# WIN_SIZE = 0.015\n",
    "# WIN_SIZE = 0.020\n",
    "\n",
    "DURATION_SCALER = 256.  # duration models do not play well with 123ms bot not also with 0.1sec ...\n",
    "\n",
    "FOUND_LABELS = dict([\n",
    "  ('h#', 12600), ('ix', 11587), ('s', 10114), ('iy', 9663), ('n', 9569), ('r', 9064),\n",
    "  ('tcl', 8978), ('l', 8157), ('kcl', 7823), ('ih', 6760), ('dcl', 6585), ('k', 6488),\n",
    "  ('t', 5899), ('m', 5429), ('ae', 5404), ('eh', 5293), ('z', 5046), ('ax', 4956), ('q', 4834),\n",
    "  ('d', 4793), ('axr', 4790), ('w', 4379), ('aa', 4197), ('ao', 4096), ('dh', 3879),\n",
    "  ('dx', 3649), ('pcl', 3609), ('p', 3545), ('ay', 3242), ('ah', 3185), ('f', 3128),\n",
    "  ('ey', 3088), ('b', 3067), ('sh', 3034), ('gcl', 3031), ('ow', 2913), ('er', 2846),\n",
    "  ('g', 2772), ('v', 2704), ('bcl', 2685), ('ux', 2488), ('y', 2349), ('epi', 2000),\n",
    "  ('ng', 1744), ('jh', 1581), ('hv', 1523), ('pau', 1343), ('nx', 1331), ('hh', 1313),\n",
    "  ('el', 1294), ('ch', 1081), ('th', 1018), ('en', 974), ('oy', 947), ('aw', 945),\n",
    "  ('uh', 756), ('uw', 725), ('ax-h', 493), ('zh', 225), ('em', 171), ('eng', 43), ])\n",
    "\n",
    "TRANSFORM_MAPPING = {\n",
    "    # First, the sentence-beginning and sentence-ending pause symbols /h#/ were mapped to pause (/pau/).\n",
    "    'h#': 'pau',\n",
    "    # Epenthetic silence (/epi/) was also mapped to pause.\n",
    "    'epi': 'pau',\n",
    "    # The syllabic phonemes /em/, /en/, /eng/, and /el/ were mapped to their non-syllabic counterparts /m/, /n/, /ng/, and /l/, respectively.\n",
    "    \"em\" : \"m\",\n",
    "    \"en\" : \"n\",\n",
    "    \"eng\" : \"ng\",\n",
    "    \"el\" : \"l\",\n",
    "    # The glottal closure symbol /q/ was merged based on weird rules\n",
    "    \"q\": None,\n",
    "}\n",
    "\n",
    "NO_BORDER_MAPPING = {\n",
    "    'pau', 'pcl', 'bcl', 'tcl', 'dcl', 'kcl', 'gcl',\n",
    "}\n",
    "\n",
    "\n",
    "KNOWN_LABELS = list(sorted(set(TRANSFORM_MAPPING.get(k, k) for k in sorted(FOUND_LABELS.keys()) if TRANSFORM_MAPPING.get(k, k))))\n",
    "KNOWN_LABELS_COUNT = len(KNOWN_LABELS)\n",
    "MAP_LABELS = {\n",
    "    label: (\n",
    "        [int(KNOWN_LABELS.index(label) == i) for i in range(KNOWN_LABELS_COUNT)],\n",
    "        KNOWN_LABELS.index(label),\n",
    "    )\n",
    "    for label in KNOWN_LABELS\n",
    "}\n",
    "\n",
    "class AudioCaching:\n",
    "    BASE_FOLDER = '/content/TIMIT-PLUS'\n",
    "    CACHE_FOLDER = BASE_FOLDER + '-CACHE'\n",
    "    # AUDIO_SCALING = 32768.\n",
    "    AUDIO_RATE = 16000\n",
    "\n",
    "    @classmethod\n",
    "    def to_file_path(cls, file_path, key):\n",
    "        filename = os.path.basename(file_path)\n",
    "        cache_filename = f\"[{'__'.join(str(k) for k in key)}]{filename}\"\n",
    "        cache_path = os.path.dirname(file_path).replace(cls.BASE_FOLDER, cls.CACHE_FOLDER)\n",
    "        cache_file_path = os.path.join(cache_path, cache_filename)\n",
    "        return cache_file_path\n",
    "\n",
    "    @classmethod\n",
    "    def set(cls, file_path, key, audio):\n",
    "        cache_file_path = cls.to_file_path(file_path, key)\n",
    "        # cached version exists :D\n",
    "        if not os.path.isfile(cache_file_path):\n",
    "            os.makedirs(os.path.dirname(cache_file_path), exist_ok=True)\n",
    "            sf.write(cache_file_path, audio, cls.AUDIO_RATE)\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, file_path, key):\n",
    "        cache_file_path = cls.to_file_path(file_path, key)\n",
    "        # cached version exists :D\n",
    "        if os.path.isfile(cache_file_path):\n",
    "            # loader = wavfile.read(cache_file_path)[1]\n",
    "            # audio = audio.astype(np.float32) / audio_scaling\n",
    "            print(\"+\", cache_file_path)\n",
    "            return cls.load(cache_file_path)\n",
    "        else:\n",
    "            print(\"-\", file_path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file_path):\n",
    "        audio = sf.read(file_path)[0]\n",
    "        return audio\n",
    "\n",
    "class UtteranceBatch(NamedTuple):\n",
    "    padded: torch.tensor\n",
    "    masks: torch.tensor\n",
    "    lengths: torch.tensor\n",
    "\n",
    "class Utterance(NamedTuple):\n",
    "    features: torch.tensor\n",
    "    labels: torch.tensor\n",
    "    transcription: torch.tensor\n",
    "    label_vec: torch.tensor\n",
    "    out_map: List\n",
    "    out_duration: torch.tensor\n",
    "    in_transcription: torch.tensor\n",
    "    position: torch.tensor\n",
    "    border: torch.tensor\n",
    "    weight: torch.tensor\n",
    "\n",
    "    index: int\n",
    "    key: str\n",
    "    audio_file: str\n",
    "    label_file: str\n",
    "\n",
    "def stack(arr, tensor):\n",
    "    return [tensor(a).to(device) for a in arr]\n",
    "\n",
    "def dedupe(tags):\n",
    "    labels = []\n",
    "    last = None\n",
    "    for x in tags:\n",
    "        if last is not None and last == x:\n",
    "            continue\n",
    "        last = x\n",
    "        labels.append(x)\n",
    "    return labels\n",
    "\n",
    "def find_borders(output_ids, original_mapping, ms_per_step=int(WIN_STEP * 1000)):\n",
    "    # add half to the border, as it is between the two frames\n",
    "    arr = (np.where(output_ids[:-1] != output_ids[1:])[0] + 0.5) * ms_per_step\n",
    "    last = output_ids.shape[0] * ms_per_step\n",
    "    arr = np.append(arr, last)\n",
    "    flat_ids = np.array(dedupe(output_ids))\n",
    "    ids = np.array([voc for voc, end in original_mapping])\n",
    "    assert (ids.shape == flat_ids.shape) and (ids != flat_ids).sum() == 0, f\"[error] Mapping and Output have same composition {ids.shape} {flat_ids.shape}\"\n",
    "    a, b = np.array([end for voc, end in original_mapping]), arr\n",
    "\n",
    "    diff = (a - b)\n",
    "    return a, b, diff\n",
    "\n",
    "class PositionalEncodingLabeler(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, scale=1, max_len=2048):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        if not self.scale:\n",
    "            return\n",
    "        max_len = int(max_len * scale)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        m = nn.Upsample(scale_factor=(1. / scale, 1), mode='bilinear', align_corners=True)\n",
    "        shape = pe.shape\n",
    "        pe = pe.view(1, 1, *shape)\n",
    "        pe = m(pe).view(-1, d_model)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1).transpose(0, 1)[0]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, out_duration):\n",
    "        durations = torch.cumsum(out_duration, dim=0) * DURATION_SCALER / ms_per_step\n",
    "        return self.pe[durations.round().long()], durations\n",
    "\n",
    "ms_per_step = WIN_STEP * 1000\n",
    "POS_DIM = 32\n",
    "POS_SCALE = 1\n",
    "pos_prep = PositionalEncodingLabeler(POS_DIM, scale=POS_SCALE)\n",
    "MERGE_DOUBLES = False\n",
    "unholy = set()\n",
    "\n",
    "class DirectMaskDataset(Dataset):\n",
    "    base = '/content'\n",
    "\n",
    "    @classmethod\n",
    "    def load_csv(cls, prefix, sa=False):\n",
    "        file_path = join(cls.base, f'{prefix}_data.csv')\n",
    "        \"\"\" Filters out the files names of phonetic and sound data in pairs\"\"\"\n",
    "        df = pd.read_csv(file_path, delimiter=',', nrows = None)\n",
    "        df = df.sort_values(by=['path_from_data_dir'])\n",
    "        # audio_mask = df.is_converted_audio == True\n",
    "        audio_mask = (df.is_audio == True) & (df.is_converted_audio == True)\n",
    "        phn_mask = df.filename.str.contains('.PHN') == True\n",
    "        SA_mask = df.filename.str.contains('SA') == False\n",
    "        df = df.loc[audio_mask | phn_mask]\n",
    "        print(\"SA\", sa)\n",
    "        if not sa:\n",
    "            df = df.loc[SA_mask]\n",
    "\n",
    "        ipd.display(df.head())\n",
    "        nRow, nCol = df.shape\n",
    "        # print(f'There are {nRow} rows and {nCol} columns')\n",
    "        A, B = df.loc[phn_mask].path_from_data_dir, df.loc[audio_mask].path_from_data_dir\n",
    "        assert len(A) == len(B)\n",
    "        files = list(zip(A, B))\n",
    "        return files\n",
    "\n",
    "    @staticmethod\n",
    "    def get_name(file_name):\n",
    "        _, a, b, name = file_name.split('/')\n",
    "        name = name.split('.')[0]\n",
    "        return a, b, name\n",
    "\n",
    "    def json_to_vec(arr):\n",
    "        return [np.array([MAP_LABELS[tag][0] for tag in tags]) for tags in arr]\n",
    "\n",
    "    def process_audio(self, labels: list, length: int, step: float) -> List[List]:\n",
    "        tag_ints, tag_vecs, tag_mapping, transcription = [], [], [], []\n",
    "\n",
    "        current, prev = 0, None\n",
    "        for end_ms, tag in labels:\n",
    "\n",
    "            tag = TRANSFORM_MAPPING.get(tag, tag)\n",
    "            q_tag = False # /q/ tag\n",
    "            if tag is None:\n",
    "                tag = prev\n",
    "                q_tag = True\n",
    "                end_ms = (current + end_ms) / 2\n",
    "\n",
    "            if current >= end_ms:\n",
    "                continue\n",
    "\n",
    "            unholy_combination = prev in NO_BORDER_MAPPING and tag in NO_BORDER_MAPPING\n",
    "            if unholy_combination:\n",
    "                unholy.add((prev, tag))\n",
    "\n",
    "            if prev == tag and MERGE_DOUBLES or unholy_combination or q_tag:\n",
    "                tag_id, ems = tag_mapping[-1]\n",
    "                tag_mapping[-1] = (tag_id, end_ms)\n",
    "\n",
    "            else:\n",
    "                tag_id = MAP_LABELS[tag][1]\n",
    "                tag_vec = np.array(MAP_LABELS[tag][0])\n",
    "\n",
    "                tag_mapping.append((tag_id, end_ms))\n",
    "                transcription.append(tag_vec)\n",
    "\n",
    "            prev = tag  # handle same tag occurence\n",
    "\n",
    "            tag_ints.append(tag_id)\n",
    "            tag_vecs.append(tag_vec)\n",
    "            current += step\n",
    "\n",
    "            while current < end_ms and len(tag_ints) < length:\n",
    "                tag_ints.append(tag_id)\n",
    "                tag_vecs.append(tag_vec)\n",
    "                current += step\n",
    "\n",
    "        if length > len(tag_vecs):\n",
    "            tag_ints.append(tag_id)\n",
    "            tag_vecs.append(tag_vec)\n",
    "\n",
    "        return tag_ints, tag_vecs, tag_mapping, transcription\n",
    "\n",
    "    def get_set(self, key, func):\n",
    "        value = CACHE.get(key)\n",
    "        value = value if value is not None else func()\n",
    "        CACHE[key] = value\n",
    "        return value\n",
    "\n",
    "    def __init__(self, files, limit=None, mask=None, augment=False, duplicate=1, seed=\"42\"):\n",
    "\n",
    "        random = Random(seed)  # init random generator\n",
    "        self.counts = []\n",
    "\n",
    "        base = self.base\n",
    "\n",
    "        if limit is not None:\n",
    "            files = files[:limit]\n",
    "\n",
    "        inp, out_vec, out_int, out_map, out_dur, out_trans = [], [], [], [], [], []\n",
    "        position, border, weight = [], [], []\n",
    "\n",
    "        duplicate_set = set()\n",
    "        self.files = []\n",
    "\n",
    "        for i, (label_file, audio_file) in enumerate(files * duplicate):\n",
    "            assert self.get_name(label_file) == self.get_name(audio_file)\n",
    "            a, b, c = self.get_name(label_file)\n",
    "            identifier = f'{a}_{b}_{c}_{i}'\n",
    "\n",
    "            label_file = os.path.join(base, 'data', label_file)\n",
    "            audio_file = os.path.join(base, 'data', audio_file)\n",
    "\n",
    "            loader = lambda: AudioCaching.load(audio_file)\n",
    "            audio = self.get_set(audio_file, loader)\n",
    "            audio_scaling, rate = 32768. / 512, 16000\n",
    "            audio_base_len = len(audio)\n",
    "\n",
    "            stretch = 1\n",
    "            pure_key = (audio_file, \"pure_key\")\n",
    "            if pure_key not in duplicate_set:\n",
    "                duplicate_set.add(pure_key)\n",
    "            elif augment:\n",
    "                # pitch = random.choice([-6, -4, -1, 1, 4, 6])\n",
    "                # stretch = random.choice([0.85, 0.9, 0.95, 1.05, 1.1, 1.15])\n",
    "                # pitch = random.choice([-4, -1, 1, 4])\n",
    "                pitch = random.choice([-1, 0, 1])\n",
    "                stretch = random.choice([0.9, 0.95, 1.05, 1.1])\n",
    "\n",
    "                key_stretch = \"time_stretch\", stretch\n",
    "                key_pitch = \"pitch_shift\", pitch, stretch\n",
    "\n",
    "                duplication_key = (audio_file, key_pitch)\n",
    "                if duplication_key in duplicate_set:\n",
    "                    continue\n",
    "                duplicate_set.add(duplication_key)\n",
    "\n",
    "                audio_pitch_shift = lambda: pyrb.pitch_shift(audio, rate, pitch)\n",
    "\n",
    "                cache_audio = AudioCaching.get(audio_file, key_pitch)\n",
    "                got_pitch = cache_audio is not None\n",
    "                cache_audio = cache_audio if got_pitch else AudioCaching.get(audio_file, key_stretch)\n",
    "                got_stretch = cache_audio is not None\n",
    "\n",
    "                audio = cache_audio if cache_audio is not None else audio\n",
    "\n",
    "                if not got_stretch:\n",
    "                    audio = pyrb.time_stretch(audio, rate, stretch)\n",
    "                    AudioCaching.set(audio_file, key_stretch, audio)\n",
    "\n",
    "                if not got_pitch:\n",
    "                    audio = pyrb.pitch_shift(audio, rate, pitch)\n",
    "                    AudioCaching.set(audio_file, key_pitch, audio)\n",
    "\n",
    "                stretch =  len(audio) / audio_base_len\n",
    "\n",
    "            fbank_feat = logfbank(audio, rate, winlen=WIN_SIZE, winstep=WIN_STEP, nfilt=INPUT_SIZE)  # TODO: remove scaling\n",
    "            # some audio instances are too short for the audio transcription and the winlen cut :(\n",
    "            fbank_feat = np.vstack([fbank_feat] + [fbank_feat[-1]] * 10)\n",
    "\n",
    "            step_size = (WIN_STEP * 1000)\n",
    "            with open(label_file) as f:\n",
    "                lines = list(f.readlines())\n",
    "                length = fbank_feat.shape[0]\n",
    "                length_ms = length * step_size\n",
    "                labels = []\n",
    "                ms_samples = 16\n",
    "\n",
    "                for line in lines:\n",
    "                    _, end, tag = line.split()\n",
    "                    end_ms = float(end) / ms_samples * stretch\n",
    "                    end_ms = min(end_ms, length_ms)\n",
    "                    labels.append((end_ms, tag))\n",
    "\n",
    "                length = int((end_ms / step_size))\n",
    "\n",
    "            tag_ints, tag_vecs, tag_mapping, transcription = self.process_audio(labels, length, step_size)\n",
    "            fbank_feat = fbank_feat[:len(tag_ints)]\n",
    "            length = fbank_feat.shape[0]\n",
    "            length_ms = length * step_size\n",
    "\n",
    "            w = [200. / FOUND_LABELS[KNOWN_LABELS[_pid]] for _pid, _ms in tag_mapping]\n",
    "\n",
    "            if i % 150 == 0:\n",
    "                print(i)\n",
    "                gc.collect()\n",
    "\n",
    "            if length == len(tag_vecs) and length == len(tag_ints):\n",
    "                original = stack([tag_vecs], torch.FloatTensor)[0].cpu().numpy()\n",
    "                original_ids = np.argmax(original, axis=1)\n",
    "                if MERGE_DOUBLES:\n",
    "                    a, b, diff = find_borders(original_ids, tag_mapping)\n",
    "                    d = abs(diff).max()\n",
    "                    if d > 15:\n",
    "                        print(f\"[DIFF-ERROR] diff is bigger {d} > 15\", np.where(abs(diff) > 15), diff.shape)\n",
    "                        print(\"\\t\", tag_mapping[-1], length_ms)\n",
    "                        print(\"\\t\", np.round(a[-5:],0))\n",
    "                        print(\"\\t\", np.round(b[-5:],0))\n",
    "                        continue\n",
    "                self.counts.append(length)\n",
    "                tag_duration = []\n",
    "                start = 0\n",
    "                for _, end_ms in tag_mapping:\n",
    "                    end_time = end_ms / DURATION_SCALER\n",
    "                    tag_duration.append(end_time - start)\n",
    "                    start = end_time  # CUMSUM vs DURATION\n",
    "\n",
    "                pos, bor = pos_prep(torch.FloatTensor(tag_duration[:-1]).to(device))\n",
    "                position.append(pos)\n",
    "                border.append(bor)\n",
    "                weight.append(w)\n",
    "\n",
    "                out_dur.append(tag_duration)\n",
    "                inp.append(fbank_feat)\n",
    "                out_vec.append(tag_vecs)\n",
    "                out_int.append(tag_ints)\n",
    "                out_trans.append(transcription)\n",
    "                out_map.append(tag_mapping)\n",
    "                self.files.append((label_file, audio_file))\n",
    "            else:\n",
    "                print(f\"[ERROR] len not match {length} != {len(tag_vecs)} != {len(tag_ints)} \\n\\t - {label_file}\\n\\t - {audio_file}\")\n",
    "\n",
    "        self.inp = stack(inp, torch.FloatTensor)\n",
    "        self.out_vec = stack(out_vec, torch.FloatTensor)\n",
    "        self.out_int = stack(out_int, torch.LongTensor)\n",
    "        self.transcription = stack(out_trans, torch.FloatTensor)\n",
    "        self.out_map = out_map\n",
    "        self.out_duration = stack(out_dur, torch.FloatTensor)\n",
    "        self.in_transcription = stack(out_trans, torch.FloatTensor)\n",
    "        self.key = [uuid.uuid4().urn for i in range(len(inp))]\n",
    "        self.position = position\n",
    "        self.border = border\n",
    "        self.weight = stack(weight, torch.FloatTensor)\n",
    "\n",
    "        FEATURES = RawField(postprocessing=self.features_batch_process)\n",
    "        LABEL = RawField(postprocessing=self.features_batch_process)\n",
    "        TRANSCRIPTION = RawField()\n",
    "        LABEL_VEC = RawField()\n",
    "        OUT_MAP = RawField()\n",
    "        OUT_DUR = RawField(postprocessing=self.features_batch_process)\n",
    "        IN_TRANS = RawField(postprocessing=self.features_batch_process)\n",
    "        INDEX = RawField()\n",
    "        KEY = RawField()\n",
    "        POSITION = RawField(postprocessing=self.features_batch_process)\n",
    "        BORDER = RawField(postprocessing=self.features_batch_process)\n",
    "        WEIGHT = RawField(postprocessing=self.features_batch_process)\n",
    "\n",
    "        setattr(FEATURES, \"is_target\", False)\n",
    "        setattr(LABEL_VEC, \"is_target\", False)\n",
    "        setattr(OUT_MAP, \"is_target\", False)\n",
    "        setattr(TRANSCRIPTION, \"is_target\", False)\n",
    "        setattr(OUT_DUR, \"is_target\", False)\n",
    "        setattr(IN_TRANS, \"is_target\", False)\n",
    "        setattr(LABEL, \"is_target\", True)\n",
    "        setattr(INDEX, \"is_target\", False)\n",
    "        setattr(KEY, \"is_target\", False)\n",
    "        setattr(POSITION, \"is_target\", False)\n",
    "        setattr(BORDER, \"is_target\", False)\n",
    "        setattr(WEIGHT, \"is_target\", False)\n",
    "\n",
    "        self.fields = {\n",
    "            \"features\" : FEATURES,\n",
    "            \"labels\" : LABEL,\n",
    "            \"transcription\": TRANSCRIPTION,\n",
    "            \"label_vec\": LABEL_VEC,\n",
    "            \"out_map\": OUT_MAP,\n",
    "            \"out_duration\": OUT_DUR,\n",
    "            \"in_transcription\": IN_TRANS,\n",
    "            \"index\": INDEX,\n",
    "            \"key\": KEY,\n",
    "            \"position\": POSITION,\n",
    "            \"border\": BORDER,\n",
    "            \"weight\": WEIGHT,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inp)\n",
    "\n",
    "    def __getitem__(self, idx) -> Utterance:\n",
    "        label_file, audio_file = self.files[idx]\n",
    "        return Utterance(\n",
    "            self.inp[idx], self.out_int[idx], self.transcription[idx], self.out_vec[idx],\n",
    "            self.out_map[idx],\n",
    "            self.out_duration[idx], self.in_transcription[idx],\n",
    "            self.position[idx], self.border[idx], self.weight[idx],\n",
    "            idx, self.key[idx],\n",
    "            label_file, audio_file,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def features_batch_process(batch) -> UtteranceBatch:\n",
    "        # this is used when a list of data items is transformed into a batch\n",
    "        # TODO: could we, should we use pack_padded_sequence\n",
    "        padded = nn.utils.rnn.pad_sequence(batch, batch_first=True).to(device)\n",
    "        lens = torch.tensor([len(item) for item in batch]).to(device)\n",
    "        b, max_len, *f = padded.shape\n",
    "        return UtteranceBatch(\n",
    "            padded,\n",
    "            torch.arange(max_len).expand(len(lens), max_len).to(device) < lens.unsqueeze(1),\n",
    "            lens\n",
    "        )\n",
    "\n",
    "    def batch(self, batch_size=128, sort_key=lambda x: len(x.features), sort=False, shuffle=True, sort_within_batch=True):\n",
    "        return BucketIterator(self, batch_size=batch_size, sort_key=sort_key, sort=sort, shuffle=shuffle, sort_within_batch=sort_within_batch)\n",
    "\n",
    "\n",
    "limit = None  # None means unlimited\n",
    "\n",
    "train_files = DirectMaskDataset.load_csv('train', sa=False)\n",
    "test_files = DirectMaskDataset.load_csv('test', sa=False)\n",
    "\n",
    "train_files, train_eval_files = train_test_split(train_files, test_size=500, random_state=42)\n",
    "\n",
    "train_dataset = DirectMaskDataset(train_files, limit=limit)\n",
    "train_eval_dataset = DirectMaskDataset(train_eval_files, limit=limit)\n",
    "\n",
    "test_dataset = DirectMaskDataset(test_files, limit=limit)\n",
    "\n",
    "toy_dataset = DirectMaskDataset(train_files, limit=1000)\n",
    "\n",
    "\n",
    "boundary_counts = []\n",
    "factors = []\n",
    "# for utterance in chain(train_dataset, train_eval_dataset):\n",
    "for utterance in test_dataset:\n",
    "    utterance: Utterance = utterance\n",
    "    audio_steps, _ = utterance.features.shape\n",
    "    transcription_steps, _ = utterance.transcription.shape\n",
    "\n",
    "    factor = audio_steps / transcription_steps\n",
    "    factors.append(factor)\n",
    "\n",
    "    boundary_counts.append(len(utterance.out_map))\n",
    "\n",
    "print(len(boundary_counts), sum(boundary_counts), sum(boundary_counts) / len(boundary_counts))\n",
    "\n",
    "factors = np.array(factors)\n",
    "hist, bins = np.histogram(abs(factors))\n",
    "hist = np.round(hist / len(factors) * 100, 1)\n",
    "\n",
    "print(sum(factors) / len(factors))\n",
    "print(1 / (sum(factors) / len(factors)))\n",
    "\n",
    "POS_TRANSCRIPTION_SCALE = sum(factors) / len(factors)\n",
    "\n",
    "for prev, tag in sorted(unholy):\n",
    "    print(prev, '->', tag)\n",
    "\n",
    "# train_augment_dataset = DirectMaskDataset(train_files, limit=None, augment=True, duplicate=3)\n",
    "# !7z a TIMIT_CACHE.zip /content/data\n",
    "# !cp TIMIT_CACHE.zip \"/content/drive/My Drive/dataset/TIMIT_CACHE.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "kDPYEFWejcrZ"
   },
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-6AfklXjcra",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !cp \"/content/drive/My Drive/dataset/TIMIT_CACHE.zip\" TIMIT_CACHE.zip\n",
    "# !7z x -aos TIMIT_CACHE.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-w5Px4Cjcre",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train_augment_dataset = DirectMaskDataset(train_files, limit=None, augment=True, duplicate=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "OBgjckq0jcri"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "wSocb3E8jcri",
    "outputId": "b4f397b1-d119-4faa-a361-a5fa9a128138",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def nullcontext():\n",
    "    yield None\n",
    "\n",
    "def load(model, path, ignore: list = None):\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = torch.load(path)\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and (not ignore or not any((i in k) for i in ignore))}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "def export_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# # # # # # # # # # # # # # # # # #\n",
    "\n",
    "dist = 2\n",
    "# dist = lambda x, y: norm(x - y, ord=1)\n",
    "# dist = 'cosine'\n",
    "\n",
    "\n",
    "def get_aligned_result(result: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    step_count, feature_count = result.shape\n",
    "    labelss = labels\n",
    "    # result = result + np.random.random_sample(result.shape) / 5\n",
    "    # labelss = labels + np.random.random_sample(labels.shape) / 5\n",
    "    # distance, path = __dtw(result, labelss, dist=dist)\n",
    "    distance, path = slowdtw(result, labelss, dist=dist)\n",
    "    # warped_result = np.vstack([labels[index,:] for _, index in path])\n",
    "    # return warped_result, None\n",
    "    # if len(path) != step_count:\n",
    "    #     d, cost_matrix, acc_cost_matrix, (result_to_labels, labels_to_result) = adtw(result, labelss, dist=euclidean_norm)\n",
    "    #     warped_result = np.vstack([labels[index,:] for index in labels_to_result])\n",
    "\n",
    "    # d, cost_matrix, acc_cost_matrix, (result_to_labels, labels_to_result) = adtw(result, labelss, dist='cosine')\n",
    "    # warped_result = np.vstack([labels[index,:] for index in labels_to_result])\n",
    "    # print(len(result_to_labels), len(labels_to_result))\n",
    "    # path = list(zip(result_to_labels, labels_to_result))\n",
    "\n",
    "    stack = [None] * step_count\n",
    "    path_for_id = []\n",
    "    label_ids = np.argmax(labels, axis=1)\n",
    "    for index, label_id in path:\n",
    "        stack[index] = labels[label_id,:]\n",
    "        path_for_id.append((index, label_ids[label_id]))\n",
    "\n",
    "    warped_result = np.vstack(stack)\n",
    "    return warped_result, path_for_id\n",
    "\n",
    "\n",
    "def generate_duration_transcription(transcriptions: np.ndarray, durations: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Durations be scaled\"\"\"\n",
    "    # durations *= DURATION_SCALER\n",
    "    # return transcriptions\n",
    "    ms_per_step = WIN_STEP * 1000\n",
    "    stack = []\n",
    "    for feature, duration in zip(transcriptions, durations):\n",
    "        adj_dur = duration / ms_per_step\n",
    "\n",
    "        steps = max((adj_dur).round().item(), 1)\n",
    "        for i in range(int(steps)):\n",
    "            stack.append(feature)\n",
    "\n",
    "    transcription_with_duration = np.vstack(stack)\n",
    "    return transcription_with_duration\n",
    "\n",
    "def find_borders_pathed(path: list, original_mapping: list):\n",
    "    # add 0.5 to the border, as it is between the two frames\n",
    "\n",
    "    borders = []\n",
    "    last = path[0][1]\n",
    "\n",
    "    for idx, label_id in path:\n",
    "        if last != label_id:\n",
    "            pos = idx - .5\n",
    "            borders.append(pos * ms_per_step)\n",
    "        last = label_id\n",
    "    length = idx + 1\n",
    "\n",
    "    # group = defaultdict(list)\n",
    "    # for index, label_id in path:\n",
    "    #     group[index].append(label_id)\n",
    "\n",
    "    # length = index + 1\n",
    "    # for index in range(length):\n",
    "    #     labels = group[index]\n",
    "    #     total = len(labels)\n",
    "\n",
    "    #     for i, label_id in enumerate(labels):\n",
    "    #         if last != label_id:\n",
    "    #             pos = index - 0.459 + (.5 / total) * i\n",
    "    #             borders.append((pos) * ms_per_step)\n",
    "    #         last = label_id\n",
    "\n",
    "    # borders.append((length - 0.459) * ms_per_step)  # add last border\n",
    "\n",
    "    borders_pred = np.array(borders)\n",
    "\n",
    "    borders_truth = np.array([end for i, (voc, end) in enumerate(original_mapping) if original_mapping[min(i+1, len(original_mapping) - 1)][0] != voc])\n",
    "\n",
    "    assert (borders_pred.shape == borders_truth.shape) , f\"[error] Mapping and Output have same composition {borders_pred.shape} {borders_truth.shape}\"\n",
    "\n",
    "    diff = (borders_truth - borders_pred)\n",
    "    return None, None, diff\n",
    "\n",
    "\n",
    "def evaluate_result(model, iterator, lower=True, duration_model=None):\n",
    "    dtw_errors = []\n",
    "    detection_errors = []\n",
    "\n",
    "    print(\"[standard]\" if (duration_model is None) else \"[duration model]\")\n",
    "    diff_ranking = []\n",
    "    diffs = []\n",
    "    cache_hits = 0\n",
    "    for batch in iterator:\n",
    "        features_audio = batch.features.padded\n",
    "        batch_s, time_s, feat_s = features_audio.shape\n",
    "        masks_audio = batch.features.masks\n",
    "        features_transcription = batch.in_transcription.padded\n",
    "        masks_transcription = batch.in_transcription.masks\n",
    "        labels = batch.labels.padded\n",
    "\n",
    "        full_result = model(features_transcription, masks_transcription, features_audio, masks_audio)\n",
    "        full_result = F.softmax(full_result, dim=2)\n",
    "\n",
    "        full_result_cls = torch.argmax(full_result, dim=2)\n",
    "        full_result = full_result.cpu().detach().numpy()\n",
    "\n",
    "        for i in range(batch_s):\n",
    "            idx = batch.index[i]\n",
    "            key = batch.key[i]\n",
    "            length = batch.labels.lengths[i].cpu().detach().numpy()\n",
    "            transcription = batch.transcription[i].cpu().detach().numpy()\n",
    "\n",
    "            result = full_result[i,:length,:]\n",
    "            result_cls = full_result_cls[i,:length]\n",
    "            labels_cls = labels[i,:length]\n",
    "\n",
    "            audio = features_audio[i,:length,:]\n",
    "            truth = batch.label_vec[i].cpu().detach().numpy()\n",
    "\n",
    "            if duration_model:\n",
    "                if key in duration_cache:\n",
    "                    cache_hits += 1\n",
    "                    transcription = duration_cache[key]\n",
    "                else:\n",
    "                    res_batch = duration_model.forward(batch.transcription[i].unsqueeze(0), None, audio.unsqueeze(0), None)\n",
    "                    prediction = (res_batch).squeeze(0).detach().cpu().numpy() * DURATION_SCALER\n",
    "                    transcription = generate_duration_transcription(transcription, prediction)\n",
    "                    duration_cache[key] = transcription\n",
    "\n",
    "            # labels is a sequence of vocals in order of their creation with no duration\n",
    "            warped_result, path = get_aligned_result(result, transcription)\n",
    "            wmax = np.argmax(warped_result, axis=1)\n",
    "\n",
    "            error = (labels_cls.cpu().detach().numpy() != wmax[:length]).sum()\n",
    "            dtw_error = (error / length) * 100\n",
    "\n",
    "            if dtw_error > 25:\n",
    "                print(f\"danger: dtw_error {dtw_error:.1f}% wrong idx:{idx}\")\n",
    "                print(f\"- warped_result: {warped_result.shape}\\n- truth:{truth.shape}\")\n",
    "                # f, axarr = plt.subplots(5, figsize=(24, 6))\n",
    "                # axarr[0].text(5, 5, 'Audio', bbox={'facecolor': 'white', 'pad': 10})\n",
    "                # axarr[0].imshow(audio.detach().cpu().numpy().T, origin=\"lower\", aspect='auto', cmap=cm.winter)\n",
    "                # axarr[1].imshow(truth.T, origin=\"lower\", aspect='auto', cmap=cm.winter)\n",
    "                # axarr[1].text(5, 5, 'truth', bbox={'facecolor': 'white', 'pad': 10})\n",
    "                # axarr[2].imshow(transcription.T, origin=\"lower\", aspect='auto', cmap=cm.winter)\n",
    "                # axarr[2].text(5, 5, 'transcription', bbox={'facecolor': 'white', 'pad': 10})\n",
    "                # axarr[3].imshow(warped_result.T, origin=\"lower\", aspect='auto', cmap=cm.winter)\n",
    "                # axarr[3].text(5, 5, 'warped_result', bbox={'facecolor': 'white', 'pad': 10})\n",
    "                # axarr[4].imshow(result.T, origin=\"lower\", aspect='auto', cmap=cm.winter)\n",
    "                # axarr[4].text(5, 5, 'result - probs', bbox={'facecolor': 'white', 'pad': 10})\n",
    "\n",
    "\n",
    "            if dtw_error < 1 and lower:\n",
    "                print(f\"winner: dtw_error {dtw_error:.1f}% good\")\n",
    "            dtw_errors.append(dtw_error)\n",
    "\n",
    "            error = (labels_cls != result_cls).sum().cpu().detach().numpy()\n",
    "            detection_errors.append((error / length) * 100)\n",
    "\n",
    "            try:\n",
    "                if path:\n",
    "                    a, b, d = find_borders_pathed(path, batch.out_map[i])\n",
    "                else:\n",
    "                    a, b, d = find_borders(wmax, batch.out_map[i])\n",
    "            except Exception as e:\n",
    "                print(\"[Exception]\")\n",
    "                # print(np.argmax(transcription, axis=1))\n",
    "                print(labels_cls.shape, wmax.shape, transcription.shape, len(batch.out_map[i]))\n",
    "                raise e\n",
    "            diff_ranking.append((abs(d.max()), idx))\n",
    "            diffs.append(d)\n",
    "\n",
    "    diff_ranking = sorted(diff_ranking, key=lambda x: x[0])\n",
    "    print(diff_ranking[-5:])\n",
    "    print(f\"cache_hits: {cache_hits}\")\n",
    "    diff = np.concatenate(diffs)\n",
    "    return dtw_errors, detection_errors, diff\n",
    "\n",
    "def display_error(errors, name=\"\"):\n",
    "    print(f\"[{name}]AVERAGE ERROR: {sum(errors) / len(errors):.2f}% COUNT:{len(errors)}\")\n",
    "\n",
    "def display_diff(errors, name=\"\", unit='ms', plotting=False):\n",
    "    errors = errors.copy()\n",
    "    hist, bins = np.histogram(abs(errors), bins=[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60,65,70,75,80,85,90, 95, 100, 105, 9999])\n",
    "    hist = np.round(hist / len(errors) * 100, 2)\n",
    "    hist = np.cumsum(hist)\n",
    "\n",
    "    print(f\"[{name}] DIFF abs mean: {abs(errors).mean():.2f}{unit} ({errors.mean():.2f}) min:{abs(errors).min():.2f}{unit} max:{abs(errors).max():.2f}{unit}\")\n",
    "    rows = list(zip(hist, bins, bins[1:]))\n",
    "    for R  in zip(rows[::2], rows[1::2]):\n",
    "        s = \"\"\n",
    "        for h, b, e in R:\n",
    "            s += f\"\\t{h:.2f}%\\t < {e:.0f}{unit}\\t\"\n",
    "        print(s)\n",
    "\n",
    "    print(*[f'{h:2.2f}' for h, b, e in rows][:-2], \"\", sep=\"% \")\n",
    "    # print([e for h, b, e in rows])\n",
    "\n",
    "    if plotting:\n",
    "        f, axarr = plt.subplots(1, 2, figsize=(10, 3))\n",
    "        axarr[0].bar(range(len(bins)-1), hist, )\n",
    "        axarr[0].set_xticklabels(bins, fontdict=None, minor=False)\n",
    "        axarr[1].hist(np.clip(errors, -70, 70), bins=5)\n",
    "\n",
    "def draw_duration(model, dataset, index):\n",
    "    model.eval()\n",
    "    inp = dataset[index]\n",
    "    prediction = model.forward(inp.in_transcription.unsqueeze(0), None, inp.features.unsqueeze(0), None).squeeze(0).detach().cpu().numpy() * DURATION_SCALER\n",
    "\n",
    "    inputs = inp.features.detach().cpu().numpy()\n",
    "    transcription_truth = dataset.out_vec[index].detach().cpu().numpy()\n",
    "    truth = inp.out_duration.detach().cpu().numpy() * DURATION_SCALER\n",
    "\n",
    "    total_duration = inputs.shape[0] * ms_per_step\n",
    "    print(total_duration, sum(prediction))\n",
    "    prediction = prediction / sum(prediction) * total_duration\n",
    "    transcription_with_duration = generate_duration_transcription(inp.in_transcription.detach().cpu().numpy(), prediction)\n",
    "\n",
    "    f, axarr = plt.subplots(4, figsize=(8, 8))\n",
    "    axarr[0].title.set_text('1. Audio input')\n",
    "    axarr[1].title.set_text('2. Duration predictions')\n",
    "    axarr[2].title.set_text('3. Phoneme occurrence predicted by scaling transcription with durations')\n",
    "    axarr[3].title.set_text('4. Phoneme occurrence ground truth')\n",
    "\n",
    "    axarr[0].imshow(inputs.T, origin=\"lower\", aspect='auto', cmap=cm.winter)\n",
    "    axarr[1].plot(prediction, 'r', label=\"Predicted durations\")\n",
    "    axarr[1].plot(truth, 'g', label=\"Actual durations\")\n",
    "    axarr[1].legend(loc='upper left')\n",
    "\n",
    "    axarr[2].imshow(transcription_with_duration.T, origin=\"lower\", aspect='auto')\n",
    "    axarr[3].imshow(transcription_truth.T, origin=\"lower\", aspect='auto')\n",
    "\n",
    "    axarr[0].set_ylabel('Audio features')\n",
    "    axarr[0].set_xlabel('Audio frame index')\n",
    "    axarr[1].set_ylabel('Duration in ms')\n",
    "    axarr[1].set_xlabel('Phoneme index')\n",
    "    axarr[2].set_xlabel('Audio frame index')\n",
    "    axarr[2].set_ylabel('Phoneme one-hot encoding')\n",
    "    axarr[3].set_xlabel('Audio frame index')\n",
    "    axarr[3].set_ylabel('Phoneme one-hot encoding')\n",
    "\n",
    "    f.tight_layout()\n",
    "\n",
    "def draw_audio(model, dataset, index):\n",
    "    model.eval()\n",
    "    ds = dataset[index]\n",
    "    features_audio = ds.features\n",
    "    transcription_truth = dataset.out_vec[index].detach().cpu().numpy()\n",
    "\n",
    "    trans = ds.in_transcription\n",
    "\n",
    "    res_batch = model.forward(trans.unsqueeze(0), None, features_audio.unsqueeze(0), None)\n",
    "\n",
    "    inputs = ds.features.detach().cpu().numpy()\n",
    "\n",
    "    prediction = (res_batch).squeeze(0).detach().cpu().numpy()\n",
    "    truth = (ds.out_duration).detach().cpu().numpy()  * DURATION_SCALER\n",
    "    trans = trans.detach().cpu().numpy()\n",
    "\n",
    "    f, axarr = plt.subplots(4, figsize=(10, 4))\n",
    "    axarr[0].imshow(inputs.T, origin=\"lower\", aspect='auto', cmap=cm.winter)\n",
    "    axarr[1].plot(truth, 'g', label=\"Truth\")\n",
    "    axarr[1].plot(prediction, 'r', label=\"Prediction\")\n",
    "    axarr[1].legend(loc=\"upper right\")\n",
    "    axarr[2].imshow(transcription_with_duration.T, origin=\"lower\", aspect='auto', cmap=cm.winter)\n",
    "    axarr[3].imshow(transcription_truth.T, origin=\"lower\", aspect='auto', cmap=cm.winter)\n",
    "\n",
    "def show_audio(model, dataset, name, plot_only = False, duration_model=None):\n",
    "    print(f\"\\n[{name}]\")\n",
    "    model.eval()\n",
    "    ### show one\n",
    "    i = 36\n",
    "    inp = test_dataset[i].features\n",
    "    out_vec = test_dataset.out_vec[i]\n",
    "    transcription = test_dataset.in_transcription[i]\n",
    "    _res = model.forward(transcription.unsqueeze(0), None, inp.unsqueeze(0), None)\n",
    "    _res = F.softmax(_res, dim=2)\n",
    "    result = _res[0,:,:]\n",
    "    result_maximized = result.clone().detach().cpu().numpy()\n",
    "    ids = np.argmax(result_maximized, axis=1)\n",
    "    result_maximized = result_maximized * 0\n",
    "    for t, i in enumerate(ids):\n",
    "        result_maximized[t, i] = 1\n",
    "    warped_result, path = get_aligned_result(result.detach().cpu().numpy(), transcription.detach().cpu().numpy())\n",
    "\n",
    "    f, axarr = plt.subplots(4, figsize=(12, 12), sharex=True)\n",
    "    axarr[0].imshow(inp.cpu().numpy().T, origin=\"lower\", aspect='auto')#, cmap=cm.winter)\n",
    "    axarr[1].imshow(result.detach().cpu().numpy().T, origin=\"lower\", aspect='auto')#, cmap=cm.winter)\n",
    "    axarr[2].imshow(result_maximized.T, origin=\"lower\", aspect='auto')#, cmap=cm.winter)\n",
    "    axarr[3].imshow(out_vec.cpu().numpy().T, origin=\"lower\", aspect='auto')#, cmap=cm.winter)\n",
    "    axarr[0].title.set_text('Audio input')\n",
    "    axarr[1].title.set_text('CTC phoneme probabilities')\n",
    "    axarr[2].title.set_text('CTC most probable phoneme')\n",
    "    axarr[3].title.set_text('Phoneme ground truth')\n",
    "\n",
    "    f, axarr = plt.subplots(3, figsize=(12, 8), sharex=True)\n",
    "    axarr[0].imshow(result.detach().cpu().numpy().T, origin=\"lower\", aspect='auto')#, cmap=cm.winter)\n",
    "    axarr[1].imshow(warped_result.T, origin=\"lower\", aspect='auto')#, cmap=cm.winter)\n",
    "    axarr[2].imshow(out_vec.cpu().numpy().T, origin=\"lower\", aspect='auto')#, cmap=cm.winter)\n",
    "    axarr[0].title.set_text('CTC phoneme probabilities')\n",
    "    axarr[1].title.set_text('Transcription phoneme sequence alligned over CTC result with DTW')\n",
    "    axarr[2].title.set_text('Phoneme ground truth')\n",
    "\n",
    "    if plot_only: return\n",
    "\n",
    "    # # # # # #\n",
    "    # difference percentages\n",
    "    dataset_iter = BucketIterator(dataset, batch_size=64, sort_key=lambda x: len(x.features), sort=False, shuffle=True, sort_within_batch=True)\n",
    "    dtw_errors, detection_errors, diff = evaluate_result(model, dataset_iter, lower=False, duration_model=duration_model)\n",
    "    display_error(dtw_errors, \"DETECTION+DTW\")\n",
    "    display_error(detection_errors, \"DETECTION\")\n",
    "    display_diff(diff, name, plotting=True)\n",
    "\n",
    "def show_duration(model, dataset, sample_size=2000):\n",
    "    model.eval()\n",
    "    print(\"dataset len\", len(dataset))\n",
    "\n",
    "    # draw_duration(model, dataset, 0)\n",
    "    # # # # # #\n",
    "    # difference percentages\n",
    "    diffs = []\n",
    "    sums = []\n",
    "    for i in sample(range(len(dataset)), min(len(dataset), sample_size)):\n",
    "        inp = dataset[i].in_transcription\n",
    "        inp_audio = dataset[i].features\n",
    "\n",
    "        out = dataset[i].out_duration * DURATION_SCALER  # durations have been scaled with DURATION_SCALER\n",
    "        res = model.forward(inp.view(1, *inp.shape), None, inp_audio.unsqueeze(0), None) * DURATION_SCALER\n",
    "\n",
    "        resc = torch.cumsum(res, dim=1).view(-1)\n",
    "        res = res.detach().view(-1).cpu().numpy()\n",
    "\n",
    "        outc = torch.cumsum(out, dim=0)\n",
    "        out = out.detach().cpu().numpy()\n",
    "\n",
    "        diff = out-res\n",
    "        if diff.max() > 1000:\n",
    "            print(i, diff.max())\n",
    "            draw_duration(model, dataset, i)\n",
    "            # continue\n",
    "        diffs.append(diff)\n",
    "        sums.append((outc-resc).detach().cpu().numpy())\n",
    "\n",
    "    diff = np.concatenate(diffs)\n",
    "    sums = np.concatenate(sums)\n",
    "    display_diff(diff, \"duration\", plotting=True)\n",
    "    display_diff(sums, \"position\")\n",
    "\n",
    "show_duration_og = show_duration\n",
    "\n",
    "def show_position(model, dataset, duration_combined_model=None, sample_size=2000, report_error=750, skip=False):\n",
    "    model.eval()\n",
    "    if duration_combined_model is not None:\n",
    "        duration_combined_model.eval()\n",
    "\n",
    "    pos_map = pos_prep.pe\n",
    "    c = torch.cumsum(torch.ones(2 ** 14), 0).unsqueeze(1) - 1\n",
    "    print(\"dataset len\", len(dataset))\n",
    "\n",
    "    diffs = []\n",
    "    att = Attention(POS_DIM)\n",
    "\n",
    "    for idx in sample(range(len(dataset)), min(len(dataset), sample_size)):\n",
    "        inp: Utterance = dataset[idx]\n",
    "\n",
    "        audio = inp.features\n",
    "        transcription = inp.in_transcription\n",
    "        border = inp.border\n",
    "\n",
    "        length = audio.shape[0] + 0\n",
    "        pos_feature = pos_map[:length,:]\n",
    "\n",
    "        borders_predicted = model(transcription.unsqueeze(0), None, audio.unsqueeze(0), None)[0]\n",
    "\n",
    "        # b = borders_predicted.detach().cpu().numpy() * ms_per_step\n",
    "        # prev = 0\n",
    "        # switched = False\n",
    "        # for i, v in enumerate(b):\n",
    "        #     if v < prev:\n",
    "        #         switched = True\n",
    "        #         after = b[i+1] if i + 1 < len(b) else prev + 10\n",
    "        #         v = (prev + after) / 2\n",
    "        #     b[i] = v\n",
    "        #     prev = v\n",
    "\n",
    "        if duration_combined_model is None:\n",
    "            new = borders_predicted.detach().cpu().numpy() * ms_per_step\n",
    "            switched = False\n",
    "            prev = 0\n",
    "            for i, v in enumerate(new):\n",
    "                if abs(v - prev) > 500 or  v < prev:\n",
    "                    after = new[i+1] if i + 1 < len(new) else prev\n",
    "                    v = (prev + after) / 2\n",
    "                    switched = True\n",
    "                new[i] = v\n",
    "                prev = v\n",
    "            b = new\n",
    "        else:\n",
    "            duration = None\n",
    "            prediction_position = borders_predicted.detach().cpu().numpy() * ms_per_step\n",
    "            new = prediction_position.copy()\n",
    "            switched = False\n",
    "            prev = 0\n",
    "            for i, v in enumerate(new):\n",
    "                after = new[i+1] if i + 1 < len(new) else v\n",
    "                if v < prev or v > after:\n",
    "                    if duration is None:\n",
    "                        duration = (duration_combined_model(transcription.unsqueeze(0), None, audio.unsqueeze(0), None) * DURATION_SCALER).view(-1).detach().cpu().numpy()[:-1]\n",
    "                    v =  prev + duration[i - 1]\n",
    "                    switched = True\n",
    "                new[i] = v\n",
    "                prev = v\n",
    "            b = new\n",
    "\n",
    "        diff = (border.detach().cpu().numpy() * ms_per_step - b)\n",
    "        if np.abs(diff).max() > report_error:\n",
    "            print(f\"[id:{idx:3d}]  [{diff.min():5.0f} {diff.max():5.0f}]  {length:4d} {switched}\")\n",
    "            if skip:\n",
    "                continue\n",
    "        if switched and skip:\n",
    "            continue\n",
    "\n",
    "        diffs.append(diff)\n",
    "\n",
    "    diff = np.concatenate(diffs)\n",
    "    display_diff(diff, \"position\", plotting=True)\n",
    "\n",
    "\n",
    "def location_fix(positions, truth, durations, end_of_audio):\n",
    "    switched = False\n",
    "\n",
    "    prev = 0\n",
    "    difos = []\n",
    "    visited = []\n",
    "    for _ in range(10):\n",
    "        # worst_diff, worst_index = max([abs(v - Y), i] for i, (v, Y) in enumerate(zip(positions, truth) )if i not in visited)\n",
    "        worst_diff, worst_index = 0, 0\n",
    "        for i, v in enumerate(positions):\n",
    "            if i in visited: continue\n",
    "            prev = positions[i-1] if i - 1 >= 0 else 0\n",
    "            after = positions[i+1] if i + 1 < len(positions) else end_of_audio\n",
    "            Y = (prev + after - 0.0001) / 2\n",
    "            Y = prev + durations[i]\n",
    "            diff = abs(v - Y)\n",
    "            if diff > worst_diff:\n",
    "                worst_diff, worst_index = diff, i\n",
    "\n",
    "        if worst_diff < 300:\n",
    "            continue\n",
    "        visited.append(worst_index)\n",
    "        difos.append([worst_diff, [positions[worst_index], truth[worst_index]], worst_index])\n",
    "\n",
    "        i = worst_index\n",
    "        v = positions[i]\n",
    "        prev = positions[i-1] if i - 1 >= 0 else 0\n",
    "        after = positions[i+1] if i + 1 < len(positions) else end_of_audio\n",
    "\n",
    "        if prev < after:\n",
    "            v = (prev + after - 0.0001) / 2\n",
    "        else:\n",
    "            v = prev + 0.001\n",
    "\n",
    "        positions[worst_index] = v\n",
    "\n",
    "    prev = 0\n",
    "    for i, _ in enumerate(positions):\n",
    "        v = positions[i]\n",
    "\n",
    "        if v < prev:\n",
    "            switched = True\n",
    "            after = positions[i+1] if i + 1 < len(positions) else prev + 0.01\n",
    "\n",
    "            if v < prev:\n",
    "                if prev < after:\n",
    "                    v = (prev + after - 0.0001) / 2\n",
    "                else:\n",
    "                    v = prev + 0.001\n",
    "\n",
    "        positions[i] = v\n",
    "        prev = v\n",
    "\n",
    "    prev = 0\n",
    "    for i, v in enumerate(positions):\n",
    "        assert v >= prev, f\"This should never happen! {i}\"\n",
    "        prev = v\n",
    "    return positions, difos\n",
    "\n",
    "def show_position_batched(model, dataset, duration_combined_model=None, report_error=750, skip=False, plotting=False, dumb_limit=500):\n",
    "    model.eval()\n",
    "    if duration_combined_model is not None:\n",
    "        duration_combined_model.eval()\n",
    "\n",
    "    pos_map = pos_prep.pe\n",
    "    c = torch.cumsum(torch.ones(2 ** 14), 0).unsqueeze(1) - 1\n",
    "    print(\"dataset len\", len(dataset))\n",
    "\n",
    "    diffs = []\n",
    "    label_ids = []\n",
    "    att = Attention(POS_DIM)\n",
    "\n",
    "    for batch in dataset.batch(32):\n",
    "        features_audio = batch.features.padded\n",
    "        masks_audio = batch.features.masks\n",
    "\n",
    "        features_transcription = batch.in_transcription.padded\n",
    "        masks_transcription = batch.in_transcription.masks\n",
    "\n",
    "        borders = batch.border.padded.cpu().detach().numpy()\n",
    "        border_lengths = batch.border.lengths.cpu().detach().numpy()\n",
    "\n",
    "        batch_s, time_s, feat_s = features_audio.shape\n",
    "\n",
    "        borders_predicted = model(features_transcription, masks_transcription, features_audio, masks_audio).cpu().detach().numpy()\n",
    "\n",
    "        if duration_combined_model is not None:\n",
    "            duration_batch = (duration_combined_model(features_transcription, masks_transcription, features_audio, masks_audio) * DURATION_SCALER).detach().cpu().numpy()\n",
    "\n",
    "        for i in range(batch_s):\n",
    "            label_id = [l_id for l_id, ms in batch.out_map[i]]\n",
    "            idx = batch.index[i]\n",
    "            key = batch.key[i]\n",
    "\n",
    "            length = border_lengths[i]\n",
    "\n",
    "            predicted_border = borders_predicted[i,:length]\n",
    "            truth_border = borders[i,:length]\n",
    "            end_of_audio = length * ms_per_step\n",
    "\n",
    "            b = predicted_border * ms_per_step\n",
    "\n",
    "            # b, difos = location_fix(b.copy(), truth_border * ms_per_step, duration_batch[i].reshape(-1), end_of_audio)\n",
    "            switched = False\n",
    "            prev = 0\n",
    "            if duration_combined_model is not None:\n",
    "                duration = duration_batch[i].reshape(-1)\n",
    "\n",
    "            for i, v in enumerate(b):\n",
    "                v = b[i]\n",
    "                if v < prev:\n",
    "                    switched = True\n",
    "                    after = b[i+1] if i + 1 < len(b) else end_of_audio # prev + 0.01 # end of file? end_of_audio\n",
    "                    if duration_combined_model is not None:\n",
    "                        v = after - duration[i - 1]\n",
    "\n",
    "                    if v < prev:\n",
    "                        if prev < after:\n",
    "                            v = (prev + after - 0.0001) / 2\n",
    "                        else:\n",
    "                            v = prev + 0.001\n",
    "                b[i] = v\n",
    "                prev = v\n",
    "\n",
    "            prev = 0\n",
    "            for i, v in enumerate(b):\n",
    "                assert v >= prev, f\"This should never happen! {i}\"\n",
    "                prev = v\n",
    "\n",
    "            diff = (truth_border * ms_per_step - b)\n",
    "            if np.abs(diff).max() > report_error:\n",
    "                print(f\"[id:{idx:3d}]  [{diff.min():5.0f} {diff.max():5.0f}]  {length:4d} {switched}\")\n",
    "                # print(*difos, sep=\"\\n\")\n",
    "            #     if skip:\n",
    "            #         continue\n",
    "            # if switched and skip:\n",
    "            #     continue\n",
    "\n",
    "            diffs.append(diff)\n",
    "            label_ids.append(label_id)\n",
    "\n",
    "    diff = np.concatenate(diffs)\n",
    "    label_ids = np.concatenate(label_ids)\n",
    "\n",
    "    phoneme_map = defaultdict(list)\n",
    "    for pdur, pid in zip(diff, label_ids):\n",
    "        phoneme_map[pid].append(abs(pdur))\n",
    "\n",
    "    for func in [np.max, np.mean]:\n",
    "        print(func)\n",
    "        mean_phoneme_dur = sorted([[f\"{KNOWN_LABELS[pid].ljust(4)}\", func(val) ] for pid, val in phoneme_map.items()], key=lambda x: x[1])\n",
    "        print(len(mean_phoneme_dur))\n",
    "        for row in zip(mean_phoneme_dur[::5], mean_phoneme_dur[1::5], mean_phoneme_dur[2::5], mean_phoneme_dur[3::5], mean_phoneme_dur[4::5]):\n",
    "            for i, (p, c) in enumerate(row):\n",
    "                print(f\"{p} {c:5.2f}ms\".ljust(20), end=\"\", sep=\"\")\n",
    "            print()\n",
    "        for i, (p, c) in enumerate(mean_phoneme_dur[-4:]):\n",
    "            print(f\"{p} {c:5.2f}ms\".ljust(20), end=\"\", sep=\"\")\n",
    "        print(\"\\n.\")\n",
    "        figure()\n",
    "        plt.plot(*zip(*[[len(val), func(val)] for pid, val in phoneme_map.items()]), 'wo')\n",
    "        for pid, val in phoneme_map.items():\n",
    "            plt.annotate(KNOWN_LABELS[pid], xy=(len(val), func(val)))\n",
    "        plt.xlabel('Occurence count', fontsize=13)\n",
    "        plt.ylabel(f'Mean error' if func is np.mean else \"Max error\", fontsize=13)\n",
    "        plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%dms'))\n",
    "        plt.show()\n",
    "\n",
    "    print(\"TOTAL\", np.abs(diff).sum())\n",
    "    display_diff(diff, \"position\", plotting=plotting)\n",
    "\n",
    "# + + + + ++ + + + + + + + + ++ + + + + + + + + + + + + + + + ++  +\n",
    "\n",
    "def explore_inherit_border_error(dataset):\n",
    "    print(\"\"\"[PLOT] How much do the borders of processed audio transcription differ from the original timesteps?\"\"\")\n",
    "    time_iter = BucketIterator(dataset, batch_size=64, sort_key=lambda x: len(x.features), sort=False, shuffle=True, sort_within_batch=True)\n",
    "\n",
    "    diffs = []\n",
    "    for batch in time_iter:\n",
    "        batch_s = len(batch.label_vec)\n",
    "        for i in range(batch_s):\n",
    "            original = batch.label_vec[i].cpu().numpy()\n",
    "            trans = batch.transcription[i].cpu().numpy()\n",
    "            original_ids = np.argmax(original, axis=1)\n",
    "            mapping = batch.out_map[i]\n",
    "\n",
    "            try:\n",
    "                a, b, d = find_borders(original_ids, mapping)\n",
    "                diffs.append(d)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                arr = (np.where(original_ids[:-1] != original_ids[1:])[0] + 0.5) * WIN_STEP * 1000\n",
    "                last = original_ids.shape[0] * WIN_STEP * 1000\n",
    "                arr = np.append(arr, last)\n",
    "                print(len(mapping), len(dedupe(original_ids)), arr.shape, trans.shape, original_ids[:10], original_ids[-10:])\n",
    "                prev = 0\n",
    "                for (voc, t), oid in zip(mapping, dedupe(original_ids)+[99, 99, 99], ):\n",
    "                    print(f\"{int(voc)}-{int(oid)} \\t{t:.0f}\\t{t - prev:.0f}\")\n",
    "                    prev = t\n",
    "\n",
    "    diff = np.concatenate(diffs)\n",
    "    display_diff(diff, \"trans. vs. timesteps\", plotting=True)\n",
    "\n",
    "def draw_counts(counts, name):\n",
    "    print(f'[dataset rows]{name}: {len(counts)}')\n",
    "    f, axarr = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    axarr[0].hist(counts, bins=25)\n",
    "    axarr[1].hist(counts, bins=10);\n",
    "\n",
    "\n",
    "# explore_inherit_border_error(train_dataset)\n",
    "\n",
    "# draw_counts(train_dataset.counts, 'train')  # 4573\n",
    "# draw_counts(test_dataset.counts, 'test')  # 1656\n",
    "# [dataset rows]train: 4606 sa\n",
    "# [dataset rows]train: 3684 n sa\n",
    "\n",
    "# no changes\n",
    "# 56.32% 82.07% 90.99% 94.64% 96.63% 97.78% 98.46% 98.88% 99.13% 99.30% 99.44% 99.51% 99.58% 99.62% 99.65% 99.67% 99.69% 99.71% 99.72% 99.74%\n",
    "# special\n",
    "# 56.14% 81.81% 90.71% 94.35% 96.32% 97.46% 98.13% 98.55% 98.80% 98.97% 99.11% 99.18% 99.25% 99.29% 99.32% 99.35% 99.37% 99.39% 99.40% 99.42%\n",
    "# vanilla\n",
    "# 56.32% 82.07% 90.99% 94.63% 96.61% 97.76% 98.43% 98.85% 99.11% 99.28% 99.42% 99.49% 99.56% 99.60% 99.63% 99.65% 99.67% 99.69% 99.70% 99.72%\n",
    "# activation\n",
    "# 56.31% 82.06% 90.97% 94.61% 96.59% 97.74% 98.41% 98.83% 99.08% 99.25% 99.39% 99.46% 99.53% 99.57% 99.61% 99.63% 99.65% 99.67% 99.68% 99.70%\n",
    "# duration\n",
    "# 56.14% 81.81% 90.71% 94.35% 96.32% 97.46% 98.13% 98.55% 98.80% 98.97% 99.11% 99.18% 99.25% 99.29% 99.32% 99.35% 99.37% 99.39% 99.40% 99.42%\n",
    "# combined\n",
    "# 56.31% 82.06% 90.98% 94.62% 96.60% 97.75% 98.43% 98.85% 99.11% 99.28% 99.42% 99.49% 99.57% 99.61% 99.64% 99.66% 99.68% 99.70% 99.71% 99.73%\n",
    "# final\n",
    "# 56.31% 82.06% 90.98% 94.62% 96.60% 97.75% 98.43% 98.85% 99.11% 99.28% 99.42% 99.49% 99.57% 99.61% 99.64% 99.66% 99.68% 99.70% 99.71% 99.73%\n",
    "# for dumb in range(50, 1000, 50):\n",
    "# show_position_batched(position_model.with_gradient, test_dataset, duration_combined_model=duration_combined_model, dumb_limit=250)\n",
    "# show_position_batched(position_model.with_gradient, test_dataset, duration_combined_model=None, dumb_limit=250)\n",
    "# show_position_batched(position_model.with_gradient, test_dataset, duration_combined_model=None, skip=True)\n",
    "\n",
    "# show_position_batched(position_model.with_gradient, train_dataset, duration_combined_model=duration_combined_model, dumb_limit=500)\n",
    "\n",
    "# 51.07% 78.61% 89.44% 93.93% 96.40% 97.71% 98.44% 98.90% 99.17% 99.38% 99.49% 99.57% 99.64% 99.69% 99.72% 99.75% 99.77% 99.78% 99.79% 99.80%\n",
    "show_position_batched(position_model.with_gradient, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "Z_BTpr7ijcrm"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "771SWlXijcrn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MaskedLoss(nn.Module):\n",
    "    mse = nn.MSELoss()\n",
    "    def forward(self, pred, target, mask):\n",
    "        # \"flatten\" all logits and targets by putting all subsequences together\n",
    "        # pred = torch.log1p(pred).contiguous().view(-1)\n",
    "        # target = torch.log1p(target).contiguous().view(-1)\n",
    "        pred = torch.log1p(pred).contiguous().view(-1)\n",
    "        target = torch.log1p(target).contiguous().view(-1)\n",
    "        # pred = torch.log10(1 + pred).contiguous().view(-1)\n",
    "        # target = torch.log10(1 + target).contiguous().view(-1)\n",
    "        mask = mask.view(-1)\n",
    "        pred = (mask * pred.T).T\n",
    "        return self.mse(pred, target)\n",
    "\n",
    "class LabelSmoothingLossAudio(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super().__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target, mask):\n",
    "        # print(pred.shape, target.shape, mask.shape)\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        # pred: torch.Size([32, 512, 61]) target: torch.Size([32, 512]) Mask:torch.Size([32, 512])\n",
    "        pred = (mask * pred.T).T\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "class PositionShuffleLoss(nn.Module):\n",
    "    mse = nn.MSELoss()\n",
    "    cos = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
    "    w = torch.FloatTensor([min(1.05 ** i, 10) for i in range(POS_DIM)]).to(device)\n",
    "\n",
    "    def forward(self, pred, target, mask):\n",
    "        pred = torch.mul(pred, mask.unsqueeze(2))\n",
    "        idx = random.randint(0, 5)\n",
    "        # idx = 8\n",
    "        if idx == 0:\n",
    "            return self.mse(pred, target)\n",
    "        # elif idx == 1:\n",
    "            # return self.mse(pred * self.w, target * self.w)\n",
    "        elif idx == 2:\n",
    "            return self.mse(pred, target) * (2. - self.cos(pred, target)).mean()\n",
    "        else:\n",
    "            return (1. - self.cos(pred, target)).mean()\n",
    "\n",
    "class PositionMSELoss(nn.Module):\n",
    "    mse = nn.MSELoss()\n",
    "    w = torch.FloatTensor([min(1.05 ** i, 10) for i in range(POS_DIM)]).to(device)\n",
    "\n",
    "    def forward(self, pred, target, mask):\n",
    "        pred = torch.mul(pred, mask.unsqueeze(2))\n",
    "        idx = random.randint(0, 1)\n",
    "        if idx == 0:\n",
    "            return self.mse(pred, target)\n",
    "        elif idx == 1:\n",
    "            return self.mse(pred * self.w, target * self.w)\n",
    "\n",
    "class CosineLoss(nn.Module):\n",
    "    cos = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
    "\n",
    "    def forward(self, pred, target, mask):\n",
    "        pred = torch.mul(pred, mask.unsqueeze(2))\n",
    "        return (1. - self.cos(pred, target)).mean()\n",
    "\n",
    "class MaskedMSE(nn.Module):\n",
    "    mse = nn.MSELoss()\n",
    "    def forward(self, pred, target, mask):\n",
    "        pred = torch.mul(pred, mask)\n",
    "        return self.mse(pred, target)\n",
    "\n",
    "class MaskedSoftL1(nn.Module):\n",
    "    loss = nn.SmoothL1Loss()\n",
    "\n",
    "    def __init__(self, factor=5):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, pred, target, mask):\n",
    "        pred = torch.mul(pred, mask)\n",
    "        return self.loss(pred / self.factor, target / self.factor)\n",
    "\n",
    "class LabelSmoothingLossAudio(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super().__init__()\n",
    "        assert (smoothing >= 0.0 and smoothing <= 1.0)\n",
    "\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target, mask):\n",
    "        # print(pred.shape, target.shape, mask.shape)\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        # pred: torch.Size([32, 512, 61]) target: torch.Size([32, 512]) Mask:torch.Size([32, 512])\n",
    "        pred = (mask * pred.T).T\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "\n",
    "def position_encode_trainer(batch: UtteranceBatch, model: nn.Module, loss_function: nn.Module):\n",
    "    features_audio = batch.features.padded\n",
    "    masks_audio = batch.features.masks\n",
    "    features = batch.in_transcription.padded\n",
    "    masks = batch.in_transcription.masks\n",
    "    target = batch.position.padded\n",
    "    target_mask = batch.position.masks\n",
    "\n",
    "    result = model(features, masks, features_audio, masks_audio)\n",
    "    return loss_function(result, target, target_mask)\n",
    "\n",
    "\n",
    "def position_gradient_trainer(batch: UtteranceBatch, model: nn.Module, loss_function: nn.Module):\n",
    "    features_audio = batch.features.padded\n",
    "    masks_audio = batch.features.masks\n",
    "    features = batch.in_transcription.padded\n",
    "    masks = batch.in_transcription.masks\n",
    "\n",
    "    result = model(features, masks, features_audio, masks_audio)\n",
    "\n",
    "    target = batch.border.padded\n",
    "    target_mask = batch.border.masks\n",
    "    return loss_function(result, target, target_mask, batch.weight.padded)\n",
    "\n",
    "def audio_detection_trainer(batch: UtteranceBatch, model: nn.Module, loss_function: nn.Module):\n",
    "    features_audio = batch.features.padded\n",
    "    masks_audio = batch.features.masks\n",
    "    features_transcription = batch.in_transcription.padded\n",
    "    masks_transcription = batch.in_transcription.masks\n",
    "    target = batch.labels.padded\n",
    "\n",
    "    result = model(features_transcription, masks_transcription, features_audio, masks_audio)\n",
    "\n",
    "    batch_s, time_s, feat_s = result.shape\n",
    "    # \"flatten\" all logits and targets by putting all subsequences together\n",
    "    flattened_result = result.contiguous().view(batch_s * time_s, -1)\n",
    "    flattened_targets = target.contiguous().view(-1)\n",
    "    flattened_masks = masks_audio.view(-1)\n",
    "\n",
    "    return loss_function(flattened_result, flattened_targets, flattened_masks)\n",
    "\n",
    "\n",
    "def duration_trainer(batch: UtteranceBatch, model: nn.Module, loss_function: nn.Module):\n",
    "    features_audio = batch.features.padded\n",
    "    masks_audio = batch.features.masks\n",
    "    features = batch.in_transcription.padded\n",
    "    masks = batch.in_transcription.masks\n",
    "\n",
    "    result = model(features, masks, features_audio, masks_audio)\n",
    "\n",
    "    target = batch.out_duration.padded\n",
    "    target_mask = batch.out_duration.masks\n",
    "    return loss_function(result.squeeze(2), target, target_mask)\n",
    "\n",
    "\n",
    "def train(model, num_epochs, data_iter, eval_iter=None, loss_function=CosineLoss(), train_function=position_encode_trainer, lr_decay=0.9, lr=0.001, weight_decay=1e-5, repreat=0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.ASGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    print(optimizer)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=lr_decay)\n",
    "    eval_iter = eval_iter or data_iter\n",
    "    eval_iter = [eval_iter] if type(eval_iter) is not list else eval_iter\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        for e_iter in eval_iter:\n",
    "            train_acc = evaluate(model, e_iter, train_function, loss_function)\n",
    "\n",
    "        print(\"Starting epoch %d, learning rate is %f\" % (epoch, lr_scheduler.get_lr()[0]))\n",
    "        errors = []\n",
    "        for batch in data_iter:\n",
    "            model.zero_grad()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss = train_function(batch, model, loss_function)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            errors.append( (loss.clone().detach().cpu().numpy(), batch) )\n",
    "\n",
    "        if repreat:\n",
    "            errors = list(sorted(errors, key=lambda x: x[0])[-3:])\n",
    "            print(\"  \", *[round(s.item(), 3) for s,_ in errors])\n",
    "            for i in range(repeat):\n",
    "                for s, batch in errors:\n",
    "                    model.zero_grad()\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = train_function(batch, model, loss_function)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "                    optimizer.step()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    for e_iter in eval_iter:\n",
    "        train_acc = evaluate(model, e_iter, train_function, loss_function)\n",
    "\n",
    "def evaluate(model, data_iter, train_function=position_encode_trainer, loss_function=CosineLoss()):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    size = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            total_loss += abs(train_function(batch, model, loss_function).item())\n",
    "            size += 1\n",
    "    print(f\"  Evaluation[{getattr(data_iter, 'prefix', '')}] - avg_loss: {total_loss/size:.7f} count:{size} Total loss:{total_loss:.7f}\")\n",
    "\n",
    "\n",
    "# - - - - - - - - - - - - - -\n",
    "# - - - - - - - - - - - - - -\n",
    "# - - - - - - - - - - - - - -\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "g0GGzJpWjcrr"
   },
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fIXnpzVjcrr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, scale=1, max_len=2048):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        if not self.scale:\n",
    "            return\n",
    "        max_len = int(max_len * scale)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        m = nn.Upsample(scale_factor=(1. / scale, 1), mode='bilinear', align_corners=True)\n",
    "        shape = pe.shape\n",
    "        pe = pe.view(1, 1, *shape)\n",
    "        pe = m(pe).view(-1, d_model)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        # print(\"Dropout\", self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.scale:\n",
    "            return x\n",
    "        x = x.transpose(0, 1)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        x = x.transpose(0, 1)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    r\"\"\"\n",
    "    https://github.com/IBM/pytorch-seq2seq/blob/master/seq2seq/models/attention.py\n",
    "    Applies an attention mechanism on the output features from the decoder.\n",
    "    .. math::\n",
    "            \\begin{array}{ll}\n",
    "            x = context*output \\\\\n",
    "            attn = exp(x_i) / sum_j exp(x_j) \\\\\n",
    "            output = \\tanh(w * (attn * context) + b * output)\n",
    "            \\end{array}\n",
    "    Args:\n",
    "        dim(int): The number of expected features in the output\n",
    "    Inputs: output, context\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the output features from the decoder.\n",
    "        - **context** (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n",
    "    Outputs: output, attn\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the attended output features from the decoder.\n",
    "        - **attn** (batch, output_len, input_len): tensor containing attention weights.\n",
    "    Attributes:\n",
    "        linear_out (torch.nn.Linear): applies a linear transformation to the incoming data: :math:`y = Ax + b`.\n",
    "        mask (torch.Tensor, optional): applies a :math:`-inf` to the indices specified in the `Tensor`.\n",
    "    Examples::\n",
    "         >>> attention = seq2seq.models.Attention(256)\n",
    "         >>> context = Variable(torch.randn(5, 3, 256))\n",
    "         >>> output = Variable(torch.randn(5, 5, 256))\n",
    "         >>> output, attn = attention(output, context)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.dim = dim\n",
    "        if self.dim:\n",
    "            self.linear_out = nn.Linear(dim * 2, dim)\n",
    "\n",
    "    def forward(self, output, context, mask = None):\n",
    "        # https://arxiv.org/abs/1706.03762\n",
    "        # context & mask is what we attend to\n",
    "        batch_size, hidden_size, input_size = output.size(0), output.size(2), context.size(1)\n",
    "        # (batch, out_len, dim) * (batch, in_len, dim) -> (batch, out_len, in_len)\n",
    "        # matrix by matrix product https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
    "        attn = torch.bmm(output, context.transpose(1, 2))\n",
    "        # TODO: scale step missing?\n",
    "\n",
    "        if mask is not None:\n",
    "            attn.data.masked_fill_(~mask.unsqueeze(1), -float('inf'))\n",
    "\n",
    "        attn = F.softmax(attn.view(-1, input_size), dim=1).view(batch_size, -1, input_size)\n",
    "        if not self.dim:\n",
    "            return attn\n",
    "\n",
    "        mix = torch.bmm(attn, context)  # (batch, out_len, in_len) * (batch, in_len, dim) -> (batch, out_len, dim)\n",
    "        combined = torch.cat((mix, output), dim=2)  # concat -> (batch, out_len, 2*dim)\n",
    "\n",
    "        # output -> (batch, out_len, dim)\n",
    "        output = torch.tanh(self.linear_out(combined.view(-1, 2 * hidden_size))).view(batch_size, -1, hidden_size)\n",
    "        return output, attn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    r\"\"\"\n",
    "    https://github.com/IBM/pytorch-seq2seq/blob/master/seq2seq/models/attention.py\n",
    "    Applies an attention mechanism on the output features from the decoder.\n",
    "    .. math::\n",
    "            \\begin{array}{ll}\n",
    "            x = context*output \\\\\n",
    "            attn = exp(x_i) / sum_j exp(x_j) \\\\\n",
    "            output = \\tanh(w * (attn * context) + b * output)\n",
    "            \\end{array}\n",
    "    Args:\n",
    "        dim(int): The number of expected features in the output\n",
    "    Inputs: output, context\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the output features from the decoder.\n",
    "        - **context** (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n",
    "    Outputs: output, attn\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the attended output features from the decoder.\n",
    "        - **attn** (batch, output_len, input_len): tensor containing attention weights.\n",
    "    Attributes:\n",
    "        linear_out (torch.nn.Linear): applies a linear transformation to the incoming data: :math:`y = Ax + b`.\n",
    "        mask (torch.Tensor, optional): applies a :math:`-inf` to the indices specified in the `Tensor`.\n",
    "    Examples::\n",
    "         >>> attention = seq2seq.models.Attention(256)\n",
    "         >>> context = Variable(torch.randn(5, 3, 256))\n",
    "         >>> output = Variable(torch.randn(5, 5, 256))\n",
    "         >>> output, attn = attention(output, context)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.dim = dim\n",
    "        if self.dim:\n",
    "            self.linear_out = nn.Linear(dim * 2, dim)\n",
    "\n",
    "    def forward(self, output, mask_output, context, mask_context):\n",
    "        # https://arxiv.org/abs/1706.03762\n",
    "        # context & mask is what we attend to\n",
    "        batch_size, hidden_size, input_size = output.size(0), output.size(2), context.size(1)\n",
    "        # (batch, out_len, dim) * (batch, in_len, dim) -> (batch, out_len, in_len)\n",
    "        # matrix by matrix product https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
    "        attn = torch.bmm(output, context.transpose(1, 2))\n",
    "        # TODO: scale step missing?\n",
    "\n",
    "        if mask_context is not None:\n",
    "            if mask_output is not None:\n",
    "                attn = attn.transpose(1, 2)\n",
    "                attn.data.masked_fill_(~mask_output.unsqueeze(1), 0)\n",
    "                attn = attn.transpose(1, 2)\n",
    "\n",
    "            attn.data.masked_fill_(~mask_context.unsqueeze(1), -float('inf'))\n",
    "\n",
    "        attn = F.softmax(attn.view(-1, input_size), dim=1).view(batch_size, -1, input_size)\n",
    "        if not self.dim:\n",
    "            return attn\n",
    "\n",
    "        mix = torch.bmm(attn, context)  # (batch, out_len, in_len) * (batch, in_len, dim) -> (batch, out_len, dim)\n",
    "        combined = torch.cat((mix, output), dim=2)  # concat -> (batch, out_len, 2*dim)\n",
    "\n",
    "        # output -> (batch, out_len, dim)\n",
    "        output = torch.tanh(self.linear_out(combined.view(-1, 2 * hidden_size))).view(batch_size, -1, hidden_size)\n",
    "        return output, attn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, num_layers=2, dropout=0.1, time_scale=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attention(hidden_size)\n",
    "\n",
    "        self.pos_encode = PositionalEncoding(hidden_size, dropout, scale=time_scale)\n",
    "\n",
    "    def forward(self, previous, mask_trans, hidden_state, encoder_outputs, mask_audio):\n",
    "        rnn_output, hidden_state = self.gru(previous, hidden_state)\n",
    "\n",
    "        rnn_output = self.pos_encode(rnn_output)\n",
    "\n",
    "        # print(rnn_output.shape if rnn_output is not None else rnn_output)\n",
    "        # print(mask_trans.shape if mask_trans is not None else mask_trans)\n",
    "        # print(encoder_outputs.shape if encoder_outputs is not None else encoder_outputs)\n",
    "        # print(mask_audio.shape if mask_audio is not None else mask_audio)\n",
    "        output, attn = self.attn(rnn_output, mask_trans, encoder_outputs, mask_audio)\n",
    "        output = self.out(output)\n",
    "        return output, hidden_state\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, out_dim=None, num_layers=2, dropout=0.1, time_scale=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.batchnorm = nn.BatchNorm1d(embedding_size)\n",
    "        # Embedding layer that will be shared with Decoder\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, out_dim or hidden_size)\n",
    "\n",
    "        self.pos_encode = PositionalEncoding(out_dim or hidden_size, dropout, scale=time_scale)\n",
    "\n",
    "    def forward(self, x, *ignore, skip_pos_encode=False):\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        x = self.batchnorm(x)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        x, hidden = self.gru(x)\n",
    "        # remove bi directional artifacts\n",
    "        hidden = hidden[:2,:,:] + hidden[2:,:,:]\n",
    "        x = self.fc(x)\n",
    "        # x = F.tanh(x) # # # # # # #  # # # #  # # # # #  # # #  # # # # #  # # # # # TODO\n",
    "        # x = torch.log1p(F.relu(x))\n",
    "        if not skip_pos_encode:\n",
    "            x = self.pos_encode(x)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "wOTXcTmFjcrv"
   },
   "source": [
    "# Duration: multy context attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Qr5SrYWDjcrv",
    "outputId": "85bb2588-74df-48df-c691-ce6da04018d9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultyContextAttention(nn.Module):\n",
    "    def __init__(self, embedding_size, embedding_audio_size, hidden_size, vocab_size, device, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.encoder = Encoder(hidden_size=embedding_size, embedding_size=embedding_size, num_layers=2, dropout=dropout, time_scale=None)\n",
    "\n",
    "        self.encoder_transcription = Encoder(hidden_size=hidden_size, embedding_size=embedding_size, num_layers=2, dropout=dropout, time_scale=None)\n",
    "        self.decoder_transcription = Decoder(embedding_size, hidden_size, hidden_size, num_layers=2, dropout=dropout, time_scale=None)\n",
    "\n",
    "        self.encoder_audio = Encoder(hidden_size=hidden_size, embedding_size=embedding_audio_size, num_layers=2, dropout=dropout, time_scale=None)\n",
    "        self.decoder_audio = Decoder(embedding_size, hidden_size, hidden_size, num_layers=2, dropout=dropout, time_scale=None)\n",
    "\n",
    "        self.direct = nn.Linear(embedding_size, hidden_size)\n",
    "        self.fast = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.chain = nn.Linear(hidden_size, embedding_size)\n",
    "        self.shuffle = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.device = device\n",
    "        self.mode = \"fast\"\n",
    "\n",
    "        load(self, \"/content/spn/trained_weights/duration-final-[10-25].pth\")\n",
    "        # load(self,  \"/content/spn/trained_weights/duration-final-[5-25].pth\")\n",
    "\n",
    "    def forward(self, input_sequence, mask, features_audio, masks_audio):\n",
    "        batch_size, out_seq_len, *features = input_sequence.shape\n",
    "\n",
    "        encoded_inputs, hidden = self.encoder(input_sequence)\n",
    "\n",
    "        if self.mode not in ['direct', 'audio']:\n",
    "            encoder_transcription_outputs, hidden_transcription = self.encoder_transcription(input_sequence)\n",
    "\n",
    "        if self.mode not in ['direct', 'trans']:\n",
    "            encoder_audio_outputs, hidden_audio = self.encoder_audio(features_audio)\n",
    "\n",
    "        if self.mode == \"direct\":\n",
    "            output = self.direct(encoded_inputs)\n",
    "        elif self.mode == \"audio\":\n",
    "            output, hidden_audio = self.decoder_audio(encoded_inputs, hidden_audio, encoder_audio_outputs, masks_audio)\n",
    "        elif self.mode == \"trans\":\n",
    "            output, hidden_transcription = self.decoder_transcription(encoded_inputs, hidden_transcription, encoder_transcription_outputs, mask)\n",
    "\n",
    "        elif self.mode == \"fast\":\n",
    "            output_transcription, hidden_transcription = self.decoder_transcription(encoded_inputs, mask, hidden_transcription, encoder_transcription_outputs, mask)\n",
    "            output_audio, hidden_audio = self.decoder_audio(encoded_inputs, mask, hidden_audio, encoder_audio_outputs, masks_audio)\n",
    "            output = torch.cat((output_transcription, output_audio), 2)\n",
    "            output = self.fast(output)\n",
    "\n",
    "        elif self.mode == \"chain\":\n",
    "            output_audio, hidden_audio = self.decoder_audio(encoded_inputs, hidden_audio, encoder_audio_outputs, masks_audio)\n",
    "            # mangle the output to be acceptable for one more pass through the encoders\n",
    "            output_audio, hidden_audio = self.chain(output_audio), hidden_audio + hidden_transcription\n",
    "            # pass hidden audio as a hint\n",
    "            output, hidden_transcription = self.decoder_transcription(output_audio, hidden_audio, encoder_transcription_outputs, mask)\n",
    "\n",
    "        elif self.mode == \"iter\":\n",
    "            output = torch.zeros(batch_size, out_seq_len, self.vocab_size).to(self.device) # tensor to store decoder outputs\n",
    "            for t in range(out_seq_len):\n",
    "                decoder_input = encoded_inputs[:,t:(t+1),:]\n",
    "\n",
    "                output_transcription, hidden_transcription = self.decoder_transcription(decoder_input, hidden_transcription, encoder_transcription_outputs, mask)\n",
    "                output_audio, hidden_audio = self.decoder_audio(decoder_input, hidden_audio, encoder_audio_outputs, masks_audio)\n",
    "\n",
    "                out = torch.cat((output_transcription, output_audio), 2)\n",
    "                output[:,t:(t+1),:] = self.out(out)\n",
    "            return F.relu(output)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"wrong mode\")\n",
    "\n",
    "        output = torch.log1p(F.relu(output))\n",
    "        output = self.shuffle(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.out(output)\n",
    "        output = F.relu(output)\n",
    "\n",
    "        total_duration = masks_audio.sum(1) * ms_per_step if masks_audio is not None else features_audio.shape[1] * ms_per_step\n",
    "        if mask is not None:\n",
    "            # print(output.shape, mask.shape)\n",
    "            # output = torch.bmm(output, mask.unsqueeze(2).float())\n",
    "            output = (output * mask.unsqueeze(2).float())\n",
    "\n",
    "        current_dur = (output.sum(1) * DURATION_SCALER).squeeze(1)\n",
    "        # print(total_duration.shape, current_dur.shape)\n",
    "\n",
    "        current_duration_scalers = total_duration / (current_dur)\n",
    "\n",
    "        # print(current_duration_scalers.shape, current_duration_scalers)\n",
    "        output = (output.squeeze(2) * current_duration_scalers.unsqueeze(1)).unsqueeze(2)\n",
    "        # print(output.shape)\n",
    "        # print((output.sum(1) * DURATION_SCALER).squeeze(1))\n",
    "        # print(total_duration.shape, total_duration)\n",
    "        # print(output.sum(1).shape, )\n",
    "        # output = output * (total_duration / output.sum(1))\n",
    "        # raise Exception\n",
    "        return output\n",
    "\n",
    "duration_combined_model = MultyContextAttention(KNOWN_LABELS_COUNT, INPUT_SIZE, 256, 1, device).to(device).eval()\n",
    "evaluate(duration_combined_model, train_eval_dataset.batch(64), loss_function=MaskedLoss(), train_function=duration_trainer)\n",
    "show_duration_og(duration_combined_model, test_dataset, sample_size=2000)\n",
    "torch.cuda.empty_cache()\n",
    "#  Evaluation on train - avg_loss: 0.0064 count:58 Total loss:0.3695\n",
    "\"\"\"\n",
    "[duration scaled] DIFF abs mean: 22.21ms (14.83) min:0.00ms max:3443.59ms\n",
    "\t38.2%\t < 10ms\n",
    "\t63.0%\t < 20ms\n",
    "\t77.4%\t < 30ms\n",
    "\t85.6%\t < 40ms\n",
    "\t90.4%\t < 50ms\n",
    "\t94.4%\t < 65ms\n",
    "\t97.0%\t < 85ms\n",
    "\t98.4%\t < 110ms\n",
    "\t99.2%\t < 140ms\n",
    "\n",
    "[duration] DIFF abs mean: 22.15ms (14.53) min:0.00ms max:3456.12ms\n",
    "\t20.6%\t < 5ms\t\t38.7%\t < 10ms\n",
    "\t52.6%\t < 15ms\t\t63.5%\t < 20ms\n",
    "\t71.8%\t < 25ms\t\t78.0%\t < 30ms\n",
    "\t82.5%\t < 35ms\t\t86.2%\t < 40ms\n",
    "\t88.9%\t < 45ms\t\t91.0%\t < 50ms\n",
    "\t92.6%\t < 55ms\t\t93.8%\t < 60ms\n",
    "\t94.8%\t < 65ms\t\t95.7%\t < 70ms\n",
    "\t96.3%\t < 75ms\t\t96.8%\t < 80ms\n",
    "\t97.3%\t < 85ms\t\t97.7%\t < 90ms\n",
    "\t97.9%\t < 95ms\t\t98.1%\t < 100ms\n",
    "\t98.3%\t < 105ms\t\t100.0%\t < 9999ms\n",
    "20.55% 38.66% 52.65% 63.49% 71.78% 78.00% 82.47% 86.16% 88.88% 90.98% 92.59% 93.85% 94.85% 95.68% 96.29% 96.81% 97.32% 97.66% 97.89% 98.09%\n",
    "[position] DIFF abs mean: 284.06ms (276.45) min:0.01ms max:4280.40ms\n",
    "\t2.7%\t < 5ms\t\t5.2%\t < 10ms\n",
    "\t7.6%\t < 15ms\t\t9.7%\t < 20ms\n",
    "\t11.8%\t < 25ms\t\t13.6%\t < 30ms\n",
    "\t15.5%\t < 35ms\t\t17.2%\t < 40ms\n",
    "\t18.7%\t < 45ms\t\t20.3%\t < 50ms\n",
    "\t21.7%\t < 55ms\t\t23.1%\t < 60ms\n",
    "\t24.5%\t < 65ms\t\t25.8%\t < 70ms\n",
    "\t27.2%\t < 75ms\t\t28.4%\t < 80ms\n",
    "\t29.6%\t < 85ms\t\t30.8%\t < 90ms\n",
    "\t31.9%\t < 95ms\t\t33.0%\t < 100ms\n",
    "\t34.0%\t < 105ms\t\t100.0%\t < 9999ms\n",
    "2.68% 5.23% 7.58% 9.69% 11.78% 13.65% 15.46% 17.16% 18.74% 20.29% 21.74% 23.13% 24.50% 25.82% 27.18% 28.41% 29.58% 30.80% 31.88% 33.02%\n",
    "\n",
    "=====  AFTER SCALING\n",
    "[duration] DIFF abs mean: 19.59ms (0.00) min:0.00ms max:1616.81ms\n",
    "\t20.4%\t < 5ms\t\t39.1%\t < 10ms\n",
    "\t54.5%\t < 15ms\t\t66.5%\t < 20ms\n",
    "\t75.4%\t < 25ms\t\t81.9%\t < 30ms\n",
    "\t86.4%\t < 35ms\t\t89.8%\t < 40ms\n",
    "\t92.1%\t < 45ms\t\t93.7%\t < 50ms\n",
    "\t95.0%\t < 55ms\t\t96.0%\t < 60ms\n",
    "\t96.7%\t < 65ms\t\t97.2%\t < 70ms\n",
    "\t97.7%\t < 75ms\t\t98.0%\t < 80ms\n",
    "\t98.3%\t < 85ms\t\t98.5%\t < 90ms\n",
    "\t98.7%\t < 95ms\t\t98.8%\t < 100ms\n",
    "\t98.9%\t < 105ms\t\t100.0%\t < 9999ms\n",
    "20.37% 39.06% 54.51% 66.53% 75.40% 81.89% 86.43% 89.76% 92.07% 93.74% 95.00% 96.00% 96.69% 97.24% 97.65% 97.98% 98.27% 98.47% 98.67% 98.82%\n",
    "[position] DIFF abs mean: 71.81ms (-10.80) min:0.00ms max:1616.81ms\n",
    "\t8.1%\t < 5ms\t\t13.3%\t < 10ms\n",
    "\t18.5%\t < 15ms\t\t23.4%\t < 20ms\n",
    "\t28.5%\t < 25ms\t\t33.1%\t < 30ms\n",
    "\t37.7%\t < 35ms\t\t42.1%\t < 40ms\n",
    "\t46.2%\t < 45ms\t\t50.0%\t < 50ms\n",
    "\t53.7%\t < 55ms\t\t57.1%\t < 60ms\n",
    "\t60.2%\t < 65ms\t\t63.1%\t < 70ms\n",
    "\t65.8%\t < 75ms\t\t68.3%\t < 80ms\n",
    "\t70.7%\t < 85ms\t\t73.0%\t < 90ms\n",
    "\t75.2%\t < 95ms\t\t77.0%\t < 100ms\n",
    "\t78.8%\t < 105ms\t\t100.0%\t < 9999ms\n",
    "8.06% 13.26% 18.49% 23.43% 28.47% 33.12% 37.68% 42.06% 46.24% 50.02% 53.69% 57.05% 60.17% 63.12% 65.76% 68.33% 70.73% 73.04% 75.16% 76.99%\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzsIX77Wjcrz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss_function = MaskedSoftL1(5)\n",
    "# loss_function = MaskedMSE()\n",
    "loss_function = MaskedLoss()\n",
    "# loss_function = DivMaskedMSE(5)\n",
    "\n",
    "train_batch_size = 64\n",
    "if WIN_STEP < 0.010:\n",
    "    train_batch_size = 16\n",
    "\n",
    "train(duration_combined_model, 4, train_dataset.batch(train_batch_size), [train_eval_dataset.batch(64)], loss_function=loss_function, train_function=duration_trainer, lr_decay=0.98, lr=0.000015, weight_decay=1e-05)\n",
    "\n",
    "# export_model(duration_combined_model, \"/content/drive/My Drive/dataset/multy-attention-duration-normalized-3-[10-25].pth\")\n",
    "show_duration_og(duration_combined_model, test_dataset, sample_size=2000)\n",
    "duration_combined_model.eval()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CV8aumF_jcr2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# draw_duration(duration_combined_model, test_dataset, 36)\n",
    "# export_model(duration_combined_model, \"/content/drive/My Drive/dataset/multy-attention-duration-final-[5-25].pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "NiddiLOCjcr5"
   },
   "source": [
    "# POS = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4ZCtjzkcjcr5",
    "outputId": "7370c1d9-510d-448a-e372-62b72133defe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PositionSimple(nn.Module):\n",
    "    class Mode:\n",
    "        weights = \"weights\"\n",
    "        position = \"position\"\n",
    "        gradient = \"gradient\"\n",
    "        argmax = \"argmax\"\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        cleaned = item.replace(\"is_\", \"\").replace(\"with_\", \"\")\n",
    "        if hasattr(self.Mode, cleaned):\n",
    "            mode = getattr(self.Mode, cleaned)\n",
    "            if \"is_\" in item:\n",
    "                return self.mode == mode\n",
    "            elif \"with_\" in item:\n",
    "                self.mode = mode\n",
    "                return self\n",
    "        return super().__getattr__(item)\n",
    "\n",
    "    def __dir__(self):\n",
    "        return super().__dir__() + [f\"is_{k}\" for k in self.Mode.keys] + [f\"with_{k}\" for k in self.Mode.keys]\n",
    "\n",
    "    def __init__(self, embedding_size, embedding_audio_size, hidden_size, vocab_size, device, attention_size=None, dropout=0.35):\n",
    "        super().__init__()\n",
    "        out_dim = hidden_size # vocab_size\n",
    "        self.encoder_transcription = Encoder(hidden_size, embedding_size, out_dim=out_dim, num_layers=2, dropout=dropout, time_scale=POS_TRANSCRIPTION_SCALE)\n",
    "\n",
    "        self.encoder_audio = Encoder(hidden_size, embedding_audio_size, out_dim=out_dim, num_layers=2, dropout=dropout, time_scale=POS_SCALE)\n",
    "\n",
    "        # self.encoder_transcription_2 = Encoder(hidden_size, out_dim, out_dim=vocab_size, num_layers=2, dropout=dropout, time_scale=None)\n",
    "        # self.encoder_audio_2 = Encoder(hidden_size, out_dim, out_dim=vocab_size, num_layers=2, dropout=dropout, time_scale=POS_SCALE)\n",
    "\n",
    "        self.attn = Attention(None)\n",
    "        self.gradient = (torch.cumsum(torch.ones(2 ** 14), 0).unsqueeze(1) - 1).cuda()\n",
    "        self.zero = torch.zeros(256, 2048, vocab_size).to(device)\n",
    "        self.pos_encode = PositionalEncoding(vocab_size, dropout, scale=POS_SCALE)\n",
    "\n",
    "        print(\"scale:\", POS_TRANSCRIPTION_SCALE)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        self.mode = self.Mode.gradient\n",
    "        self.flags = {}\n",
    "        self.use_iter = True\n",
    "        self.use_pos_encode = True\n",
    "\n",
    "        self.t_transformer = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 32),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(32, embedding_size),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "\n",
    "        self.a_transformer = nn.Sequential(\n",
    "            nn.Linear(embedding_audio_size, 32),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(32, embedding_audio_size),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "    def weights_to_positions(self, weights, argmax=False):\n",
    "        batch_size, audio_size, input_size = weights.shape\n",
    "\n",
    "        batch, trans_len, seq_len = weights.shape\n",
    "        if argmax:\n",
    "            return weights.max(2)[1][:,:-1]\n",
    "        positions = (self.gradient[:seq_len] * weights.transpose(1, 2)).sum(1)[:,:-1]\n",
    "        return positions\n",
    "\n",
    "    def forward(self, features_transcription, mask_transcription, features_audio, mask_audio):\n",
    "        features_transcription = features_transcription.clone()\n",
    "        features_transcription[:,:-1] += features_transcription[:,1:] * 0.55\n",
    "\n",
    "        features_transcription = self.t_transformer(features_transcription)\n",
    "        features_audio = self.a_transformer(features_audio)\n",
    "\n",
    "        encoder_transcription_outputs, _ = self.encoder_transcription(features_transcription, skip_pos_encode=not self.use_pos_encode) # # # #\n",
    "\n",
    "        encoder_audio_outputs, _ = self.encoder_audio(features_audio, skip_pos_encode=not self.use_pos_encode)\n",
    "\n",
    "\n",
    "        # w = self.attn(F.tanh(encoder_transcription_outputs), mask_transcription, F.tanh(encoder_audio_outputs), mask_audio)\n",
    "\n",
    "        if \"iter\" and True:\n",
    "            encoder_transcription_outputs = F.relu(encoder_transcription_outputs)\n",
    "            encoder_audio_outputs = F.relu(encoder_audio_outputs)\n",
    "            # tensor to store decoder outputs\n",
    "            batch_size, out_seq_len, _ = features_transcription.shape\n",
    "            w = torch.zeros(batch_size, out_seq_len, features_audio.shape[1]).to(self.device)\n",
    "\n",
    "            w_masks, w_mask, iter_mask_audio = [], None, mask_audio\n",
    "            for t in range(out_seq_len):\n",
    "                iter_input = encoder_transcription_outputs[:,t:(t+1),:]\n",
    "                iter_memory = encoder_audio_outputs\n",
    "\n",
    "                if len(w_masks) > 1:\n",
    "                    w_mask = w_masks[0]\n",
    "                    w_mask_b = w_masks[1]\n",
    "\n",
    "                    w_mask = torch.clamp(w_mask, min=0.0, max=1)\n",
    "                    w_mask[w_mask < 0.1] = 0\n",
    "                    w_mask[w_mask > 0.1] = 1\n",
    "\n",
    "                    w_mask_b = torch.clamp(w_mask_b, min=0.0, max=1)\n",
    "                    w_mask_b[w_mask_b < 0.1] = 0\n",
    "\n",
    "                    pad = 0.00\n",
    "                    a, b = torch.split(iter_memory, 128, dim=2)\n",
    "                    a = a * (w_mask.unsqueeze(2) * (1 - pad) + pad)\n",
    "                    b = b * (w_mask_b.unsqueeze(2) * (1 - pad) + pad)\n",
    "                    iter_memory = torch.cat([a, b], dim=2)\n",
    "                    iter_mask_audio = mask_audio * ( w_mask > 0.1) if mask_audio is not None else w_mask > 0.1\n",
    "\n",
    "                    # iter_memory = iter_memory * (w_mask.unsqueeze(2) * (1 - pad) + pad)\n",
    "                # print(iter_input.shape, mask_transcription.shape, (iter_memory).shape, iter_mask_audio.shape)\n",
    "                iter_mask_transcription = mask_transcription[:,t:(t+1)] if mask_transcription is not None else None\n",
    "                w_slice = self.attn(iter_input, iter_mask_transcription, (iter_memory), iter_mask_audio)\n",
    "\n",
    "                if w_mask is not None:\n",
    "                    w[:,t:(t+1), :] = w_slice * w_mask.unsqueeze(1)\n",
    "                else:\n",
    "                    w[:,t:(t+1), :] = w_slice\n",
    "\n",
    "                w_mask = w_slice.squeeze(1).clone()\n",
    "                w_mask = torch.cumsum(w_mask, dim=1).detach()\n",
    "                w_masks.append(w_mask)\n",
    "                w_masks = w_masks[-2:]\n",
    "\n",
    "\n",
    "        # elif self.use_iter:\n",
    "        #     encoder_transcription_outputs = F.tanh(encoder_transcription_outputs)\n",
    "        #     encoder_audio_outputs = F.tanh(encoder_audio_outputs)\n",
    "        #     # tensor to store decoder outputs\n",
    "        #     batch_size, out_seq_len, _ = features_transcription.shape\n",
    "        #     w = torch.zeros(batch_size, out_seq_len, features_audio.shape[1]).to(self.device)\n",
    "\n",
    "        #     w_masks, w_mask, iter_mask_audio = [], None, mask_audio\n",
    "        #     for t in range(out_seq_len):\n",
    "        #         iter_input = encoder_transcription_outputs[:,t:(t+1),:]\n",
    "        #         iter_memory = encoder_audio_outputs\n",
    "\n",
    "        #         if len(w_masks) > 1:\n",
    "        #             w_mask = w_masks[0]\n",
    "        #             w_mask_b = w_masks[1]\n",
    "\n",
    "        #             # w_mask = torch.clamp(w_mask, min=0.0, max=1)\n",
    "        #             # w_mask[w_mask < 0.45] = 0\n",
    "        #             w_mask[w_mask > 0.1] = 1\n",
    "        #             # TODO: combine masks of previos two\n",
    "\n",
    "        #             # w_mask_b = torch.clamp(w_mask_b, min=0.0, max=1)\n",
    "        #             # w_mask_b[w_mask_b < 0.1] = 0\n",
    "\n",
    "        #             pad = 0.0\n",
    "        #             # a, b = torch.split(iter_memory, 128, dim=2)\n",
    "        #             # a = a * (w_mask.unsqueeze(2) * (1 - pad) + pad)\n",
    "        #             # b = b * (w_mask_b.unsqueeze(2) * (1 - pad) + pad)\n",
    "        #             # iter_memory = torch.cat([a, b], dim=2)\n",
    "        #             low = 0.1\n",
    "        #             iter_mask_audio = mask_audio * (w_mask > low) if mask_audio is not None else w_mask > low\n",
    "\n",
    "        #             iter_memory = iter_memory * (w_mask.unsqueeze(2) * (1 - pad) + pad)\n",
    "\n",
    "        #         w_slice = self.attn(iter_input, mask_transcription[:,t:(t+1)], iter_memory, iter_mask_audio)\n",
    "\n",
    "        #         if w_mask is not None and 0:\n",
    "        #             w[:,t:(t+1), :] = w_slice * w_mask.unsqueeze(1)\n",
    "        #         else:\n",
    "        #             w[:,t:(t+1), :] = w_slice\n",
    "\n",
    "        #         w_mask = w_slice.squeeze(1).clone()\n",
    "        #         w_mask = torch.cumsum(w_mask, dim=1).detach()\n",
    "        #         w_masks.append(w_mask)\n",
    "        #         w_masks = w_masks[-2:]\n",
    "\n",
    "        if self.is_weights:\n",
    "            return w\n",
    "\n",
    "        if self.is_gradient or self.is_argmax:\n",
    "            return self.weights_to_positions(w, argmax=self.is_argmax)\n",
    "\n",
    "        batch, seq_len, dimensions = encoder_audio_outputs.shape\n",
    "        processsed_audio = self.zero[:batch,:seq_len,:self.vocab_size]\n",
    "        pos = self.pos_encode(processsed_audio)\n",
    "        position_encodes = torch.bmm(w, pos)\n",
    "\n",
    "        if self.is_position:\n",
    "            return position_encodes[:,:-1]\n",
    "\n",
    "\n",
    "position_model = PositionSimple(KNOWN_LABELS_COUNT, INPUT_SIZE, 256, POS_DIM, device).to(device).with_gradient\n",
    "load(position_model, \"/content/spn/trained_weights/position_model-final.pth\")\n",
    "\n",
    "\"\"\"\n",
    "TOTAL 344903.16\n",
    "[position] DIFF abs mean: 7.01ms (-0.39) min:0.00ms max:780.91ms\n",
    "\t55.97%\t < 5ms\t\t81.93%\t < 10ms\n",
    "\t90.88%\t < 15ms\t\t94.61%\t < 20ms\n",
    "\t96.62%\t < 25ms\t\t97.79%\t < 30ms\n",
    "\t98.48%\t < 35ms\t\t98.87%\t < 40ms\n",
    "\t99.15%\t < 45ms\t\t99.35%\t < 50ms\n",
    "\t99.48%\t < 55ms\t\t99.57%\t < 60ms\n",
    "\t99.65%\t < 65ms\t\t99.67%\t < 70ms\n",
    "\t99.70%\t < 75ms\t\t99.72%\t < 80ms\n",
    "\t99.74%\t < 85ms\t\t99.76%\t < 90ms\n",
    "\t99.77%\t < 95ms\t\t99.79%\t < 100ms\n",
    "\t99.81%\t < 105ms\t\t100.01%\t < 9999ms\n",
    "55.97% 81.93% 90.88% 94.61% 96.62% 97.79% 98.48% 98.87% 99.15% 99.35% 99.48% 99.57% 99.65% 99.67% 99.70% 99.72% 99.74% 99.76% 99.77% 99.79%\n",
    "\"\"\"\n",
    "\n",
    "def eval_border_agreement(duration_combined_model=None):\n",
    "    show_position_batched(position_model.with_gradient, test_dataset, duration_combined_model=duration_combined_model)\n",
    "    show_position_batched(position_model.with_gradient, train_dataset, duration_combined_model=duration_combined_model)\n",
    "    if duration_combined_model is not None:\n",
    "        print(\" -  -  -  -  -  - WITHOUT -  -  -  -  -  - \")\n",
    "        show_position_batched(position_model.with_gradient, test_dataset)\n",
    "        show_position_batched(position_model.with_gradient, train_dataset)\n",
    "\n",
    "# evaluate(position_model, train_dataset.batch(64), loss_function=MaskedMSE(), train_function=position_gradient_trainer) # 0.15\n",
    "# evaluate(position_model, test_dataset.batch(64), loss_function=MaskedMSE(), train_function=position_gradient_trainer) # 2.1\n",
    "eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "I4REmPpzjcr8"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4YdiAE6jcr9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "class DivMaskedMSE(nn.Module):\n",
    "    mse = nn.MSELoss()\n",
    "    l1 = nn.L1Loss()\n",
    "    def __init__(self, cutoff, flip=False):\n",
    "        super().__init__()\n",
    "        self.cutoff = cutoff\n",
    "        self.flip = flip\n",
    "        print(\"CUTOFF\", cutoff)\n",
    "\n",
    "    def forward(self, pred, target, mask, weight):\n",
    "        # print(weight.shape)\n",
    "        diff = torch.abs(pred - target)\n",
    "        if not self.flip:\n",
    "            diff = diff > (self.cutoff or random.randint(0, 3))\n",
    "        else:\n",
    "            diff = diff < (self.cutoff or random.randint(0, 3))\n",
    "\n",
    "        pred = pred * weight[:,:-1]\n",
    "        target = target * weight[:,:-1]\n",
    "\n",
    "        mask_diff = mask & diff\n",
    "        pred = torch.mul(pred, mask_diff)\n",
    "        target = torch.mul(target, mask_diff)\n",
    "        mse = self.mse(pred, target)\n",
    "        # return mse\n",
    "\n",
    "        mask_diff = mask & ~diff\n",
    "        pred = torch.mul(pred, mask_diff)\n",
    "        target = torch.mul(target, mask_diff)\n",
    "        l1 = self.l1(pred, target)\n",
    "\n",
    "        return mse + l1\n",
    "\n",
    "class MaskedL1(nn.Module):\n",
    "    l1 = nn.L1Loss()\n",
    "\n",
    "    def forward(self, pred, target, mask):\n",
    "        pred = torch.mul(pred, mask)\n",
    "        target = torch.mul(target, mask)\n",
    "        l1 = self.l1(pred, target)\n",
    "\n",
    "        return l1\n",
    "\n",
    "\n",
    "class MaskedThing(nn.Module):\n",
    "    l1 = nn.L1Loss()\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, target, mask):\n",
    "        pred = torch.log1p(F.relu(pred))\n",
    "        target = torch.log1p(F.relu(target))\n",
    "        pred = torch.mul(pred, mask)\n",
    "        target = torch.mul(target, mask)\n",
    "        l1 = self.mse(pred, target)\n",
    "\n",
    "        return l1\n",
    "\n",
    "# toy_dataset = DirectMaskDataset(train_files, limit=2043)\n",
    "work_dataset = train_dataset\n",
    "# work_dataset = train_augment_dataset\n",
    "\n",
    "evaluation = [toy_dataset.batch(64)]\n",
    "evaluation = [test_dataset.batch(64), train_eval_dataset.batch(64)]\n",
    "\n",
    "train_batch_size = 32\n",
    "if WIN_STEP < 0.010:\n",
    "    train_batch_size = 16\n",
    "\n",
    "f = .35\n",
    "n = 1\n",
    "# eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "\n",
    "# train(position_model.with_gradient, int(10 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=MaskedMSE(), train_function=position_gradient_trainer, lr_decay=0.951, lr= f * 0.000161 * 0.131 * .10, weight_decay=1e-05 * 14)\n",
    "# eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "\n",
    "# train(position_model.with_position, int(8 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=CosineLoss(), train_function=position_encode_trainer, lr_decay=0.93, lr= f * 0.000051 * 0.005)\n",
    "# eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "\n",
    "# train(position_model.with_gradient, int(8 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=MaskedMSE(), train_function=position_gradient_trainer, lr_decay=0.991, lr= f * 0.000161 * 0.131 * 3.1 * 0.01, weight_decay=1e-05 * 14)\n",
    "\n",
    "# export_model(position_model, \"/content/drive/My Drive/dataset/position_model-pure-derp.pth\")\n",
    "# show_position_batched(position_model, toy_dataset, report_error=750)\n",
    "# eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "\n",
    "# NORMAL\n",
    "# train(position_model.with_gradient, int(10 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=MaskedMSE(), train_function=position_gradient_trainer, lr_decay=0.98, lr= f * 0.000161 * 0.131 * .01, weight_decay=1e-05 * 14)\n",
    "# eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "\n",
    "# train(position_model.with_gradient, int(9 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=DivMaskedMSE(10, flip=True), train_function=position_gradient_trainer, lr_decay=0.98, lr= f * 0.000161 * 0.131 * .05, weight_decay=1e-05 * 14)\n",
    "# eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "\n",
    "# train(position_model.with_gradient, int(30 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=DivMaskedMSE(13, flip=False), train_function=position_gradient_trainer, lr_decay=0.98, lr= f * 0.000161 * 0.131 * .2, weight_decay=1e-05 * 14)\n",
    "# eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "\n",
    "# train(position_model.with_gradient, int(30 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=DivMaskedMSE(5, flip=False), train_function=position_gradient_trainer, lr_decay=0.98, lr= f * 0.000161 * 0.131 * .2, weight_decay=1e-05 * 14)\n",
    "# eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "\n",
    "# train(position_model.with_gradient, int(30 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=DivMaskedMSE(21, flip=False), train_function=position_gradient_trainer, lr_decay=0.98, lr= f * 0.000161 * 0.131 * .2, weight_decay=1e-05 * 14)\n",
    "# eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# for i in range(10, 0, -1):\n",
    "#     f = .25\n",
    "#     # n= 0.8\n",
    "#     load(position_model, \"/content/drive/My Drive/dataset/position_model-final-3.pth\")\n",
    "#     train(position_model.with_gradient, int(5 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=DivMaskedMSE(i, flip=False), train_function=position_gradient_trainer, lr_decay=0.985, lr= f * 0.000161 * 0.131 * .2, weight_decay=1e-05 * 14)\n",
    "#     eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "f = .023\n",
    "n= 1.8\n",
    "# load(position_model, \"/content/drive/My Drive/dataset/position_model-final-3.pth\")\n",
    "train(position_model.with_gradient, int(5 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=DivMaskedMSE(1.5, flip=False), train_function=position_gradient_trainer, lr_decay=0.985, lr= f * 0.000161 * 0.131 * .2, weight_decay=1e-05 * 14)\n",
    "# train(position_model.with_gradient, int(5 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=MaskedMSE(), train_function=position_gradient_trainer, lr_decay=0.985, lr= f * 0.000161 * 0.131 * .2, weight_decay=1e-05 * 14)\n",
    "eval_border_agreement(duration_combined_model=None)\n",
    "# export_model(position_model, \"/content/drive/My Drive/dataset/position_model-derp.pth\")\n",
    "\n",
    "# SIMPLE\n",
    "# train(position_model.with_gradient, int(30 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=MaskedMSE(), train_function=position_gradient_trainer, lr_decay=0.98, lr= f * 0.000161 * 0.131 * 3.1, weight_decay=1e-05 * 14)\n",
    "# eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "# train(position_model.with_gradient, int(10 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=DivMaskedMSE(5), train_function=position_gradient_trainer, lr_decay=0.98, lr= f * 0.000161 * 0.131 * .1, weight_decay=1e-05 * 14)\n",
    "# eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "# train(position_model.with_gradient, int(10 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=DivMaskedMSE(5), train_function=position_gradient_trainer, lr_decay=0.98, lr= f * 0.000161 * 0.131 * .1, weight_decay=1e-05 * 14)\n",
    "# train(position_model.with_position, int(30 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=CosineLoss(), train_function=position_encode_trainer, lr_decay=0.93, lr= f * 0.000051 * 0.03)\n",
    "# eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 49.97% 77.86% 89.11% 93.87% 96.27% 97.61% 98.35% 98.84% 99.13% 99.33% 99.46% 99.57% 99.61% 99.69% 99.73% 99.75% 99.77% 99.78% 99.79% 99.81%\n",
    "# 56.05% 81.85% 90.93% 94.65% 96.66% 97.77% 98.47% 98.88% 99.16% 99.34% 99.49% 99.54% 99.62% 99.65% 99.68% 99.70% 99.72% 99.74% 99.75% 99.77%\n",
    "# % % % % % % % % % % % % % % % % % % % %\n",
    "# 55.97% 81.93% 90.88% 94.61% 96.62% 97.79% 98.48% 98.87% 99.15% 99.35% 99.48% 99.57% 99.65% 99.67% 99.70% 99.72% 99.74% 99.76% 99.77% 99.79%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dr4dpATEjcsA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train(position_model.with_gradient, int(5 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=DivMaskedMSE(1.5, flip=False), train_function=position_gradient_trainer, lr_decay=0.985, lr= f * 0.000161 * 0.131 * .2, weight_decay=1e-05 * 14)\n",
    "# train(position_model.with_gradient, int(5 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=MaskedMSE(), train_function=position_gradient_trainer, lr_decay=0.985, lr= f * 0.000161 * 0.131 * .2, weight_decay=1e-05 * 14)\n",
    "eval_border_agreement(duration_combined_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYZkx2HWjcsD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "                                                            work_dataset = train_augment_dataset\n",
    "train(position_model.with_gradient, int(10 * n), work_dataset.batch(train_batch_size), evaluation, loss_function=DivMaskedMSE(5, flip=False), train_function=position_gradient_trainer, lr_decay=0.985, lr= f * 0.000161 * 0.131 * .1, weight_decay=1e-05 * 14)\n",
    "eval_border_agreement(duration_combined_model=duration_combined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pFtkir4FjcsG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# export_model(position_model, \"/content/drive/My Drive/dataset/position_model-final-4.pth\")\n",
    "\n",
    "# eval_border_agreement(duration_combined_model=duration_combined_model)\n",
    "# export_model(position_model, \"/content/drive/My Drive/dataset/position_model-simple-final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "HZBiTAepjcsJ"
   },
   "source": [
    "### Simple model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xrdV3i4jcsK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# [position] DIFF abs mean: 8.30ms (-2.00) min:0.00ms max:748.36ms\n",
    "# \t57.3%\t < 5ms\t\t82.1%\t < 10ms\n",
    "# \t90.6%\t < 15ms\t\t94.0%\t < 20ms\n",
    "# \t95.8%\t < 25ms\t\t96.8%\t < 30ms\n",
    "# \t97.5%\t < 35ms\t\t98.0%\t < 40ms\n",
    "# \t98.3%\t < 45ms\t\t98.5%\t < 50ms\n",
    "# \t98.6%\t < 55ms\t\t98.8%\t < 60ms\n",
    "# \t98.8%\t < 65ms\t\t98.9%\t < 70ms\n",
    "# \t99.0%\t < 75ms\t\t99.0%\t < 80ms\n",
    "# \t99.0%\t < 85ms\t\t99.1%\t < 90ms\n",
    "# \t99.1%\t < 95ms\t\t99.1%\t < 100ms\n",
    "# \t99.2%\t < 105ms\t\t100.0%\t < 9999ms\n",
    "# 57.28% 82.07% 90.56% 94.00% 95.77% 96.85% 97.54% 97.99% 98.29% 98.50% 98.64% 98.77% 98.85% 98.91% 98.96% 99.01% 99.05% 99.08% 99.12% 99.15%\n",
    "# dataset len 3196\n",
    "# [position] DIFF abs mean: 5.75ms (-0.25) min:0.00ms max:313.08ms\n",
    "# \t60.6%\t < 5ms\t\t85.6%\t < 10ms\n",
    "# \t93.6%\t < 15ms\t\t96.7%\t < 20ms\n",
    "# \t98.2%\t < 25ms\t\t98.9%\t < 30ms\n",
    "# \t99.3%\t < 35ms\t\t99.6%\t < 40ms\n",
    "# \t99.7%\t < 45ms\t\t99.8%\t < 50ms\n",
    "# \t99.8%\t < 55ms\t\t99.8%\t < 60ms\n",
    "# \t99.9%\t < 65ms\t\t99.9%\t < 70ms\n",
    "# \t99.9%\t < 75ms\t\t99.9%\t < 80ms\n",
    "# \t99.9%\t < 85ms\t\t99.9%\t < 90ms\n",
    "# \t99.9%\t < 95ms\t\t99.9%\t < 100ms\n",
    "# \t99.9%\t < 105ms\t\t100.0%\t < 9999ms\n",
    "# 60.61% 85.58% 93.64% 96.72% 98.17% 98.91% 99.34% 99.56% 99.69% 99.76% 99.81% 99.85% 99.87% 99.88% 99.89% 99.90% 99.91% 99.92% 99.92% 99.93%\n",
    "#  -  -  -  -  -  - WITHOUT -  -  -  -  -  -\n",
    "# dataset len 1344\n",
    "# [id:173]  [  -18  2302]    32 True\n",
    "# [position] DIFF abs mean: 7.32ms (0.32) min:0.00ms max:2302.34ms\n",
    "# \t57.5%\t < 5ms\t\t82.5%\t < 10ms\n",
    "# \t91.0%\t < 15ms\t\t94.5%\t < 20ms\n",
    "# \t96.3%\t < 25ms\t\t97.4%\t < 30ms\n",
    "# \t98.1%\t < 35ms\t\t98.5%\t < 40ms\n",
    "# \t98.8%\t < 45ms\t\t99.1%\t < 50ms\n",
    "# \t99.2%\t < 55ms\t\t99.3%\t < 60ms\n",
    "# \t99.4%\t < 65ms\t\t99.5%\t < 70ms\n",
    "# \t99.5%\t < 75ms\t\t99.6%\t < 80ms\n",
    "# \t99.6%\t < 85ms\t\t99.6%\t < 90ms\n",
    "# \t99.6%\t < 95ms\t\t99.7%\t < 100ms\n",
    "# \t99.7%\t < 105ms\t\t100.0%\t < 9999ms\n",
    "# 57.55% 82.51% 91.05% 94.54% 96.32% 97.40% 98.08% 98.55% 98.84% 99.06% 99.21% 99.33% 99.42% 99.48% 99.53% 99.56% 99.59% 99.60% 99.62% 99.65%\n",
    "# dataset len 3196\n",
    "# [id:953]  [  -24   802]    49 True\n",
    "# [id:2217]  [  -14   768]    64 True\n",
    "# [id:2836]  [  -10   784]    36 True\n",
    "# [position] DIFF abs mean: 6.02ms (0.35) min:0.00ms max:801.98ms\n",
    "# \t60.6%\t < 5ms\t\t85.6%\t < 10ms\n",
    "# \t93.6%\t < 15ms\t\t96.7%\t < 20ms\n",
    "# \t98.1%\t < 25ms\t\t98.9%\t < 30ms\n",
    "# \t99.3%\t < 35ms\t\t99.5%\t < 40ms\n",
    "# \t99.7%\t < 45ms\t\t99.7%\t < 50ms\n",
    "# \t99.8%\t < 55ms\t\t99.8%\t < 60ms\n",
    "# \t99.8%\t < 65ms\t\t99.8%\t < 70ms\n",
    "# \t99.9%\t < 75ms\t\t99.9%\t < 80ms\n",
    "# \t99.9%\t < 85ms\t\t99.9%\t < 90ms\n",
    "# \t99.9%\t < 95ms\t\t99.9%\t < 100ms\n",
    "# \t99.9%\t < 105ms\t\t100.0%\t < 9999ms\n",
    "# 60.60% 85.56% 93.61% 96.69% 98.15% 98.89% 99.32% 99.54% 99.67% 99.74% 99.79% 99.82% 99.84% 99.85% 99.86% 99.87% 99.87% 99.87% 99.87% 99.87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nr6c-65ZjcsN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# OTHER ?????\n",
    "# scale: 8.344777745411855\n",
    "#   Evaluation[] - avg_loss: 0.3700718 count:50 Total loss:18.5035897\n",
    "#   Evaluation[] - avg_loss: 3.0839038 count:21 Total loss:64.7619789\n",
    "# dataset len 1344\n",
    "# [id:696]  [ -922    12]    53 True\n",
    "# [id:848]  [-1497    79]    58 True\n",
    "# [id:202]  [ -813    13]    51 True\n",
    "# [id:933]  [  -13   760]    33 False\n",
    "# [position] DIFF abs mean: 8.85ms (-2.27) min:0.00ms max:1496.71ms\n",
    "# \t56.2%\t < 5ms\t\t81.8%\t < 10ms\n",
    "# \t90.7%\t < 15ms\t\t94.4%\t < 20ms\n",
    "# \t96.3%\t < 25ms\t\t97.5%\t < 30ms\n",
    "# \t98.1%\t < 35ms\t\t98.5%\t < 40ms\n",
    "# \t98.8%\t < 45ms\t\t99.0%\t < 50ms\n",
    "# \t99.1%\t < 55ms\t\t99.2%\t < 60ms\n",
    "# \t99.3%\t < 65ms\t\t99.3%\t < 70ms\n",
    "# \t99.3%\t < 75ms\t\t99.4%\t < 80ms\n",
    "# \t99.4%\t < 85ms\t\t99.4%\t < 90ms\n",
    "# \t99.4%\t < 95ms\t\t99.4%\t < 100ms\n",
    "# \t99.4%\t < 105ms\t\t100.0%\t < 9999ms\n",
    "# 56.17% 81.83% 90.74% 94.36% 96.32% 97.46% 98.13% 98.55% 98.80% 98.97% 99.11% 99.19% 99.26% 99.30% 99.33% 99.36% 99.38% 99.40% 99.41% 99.43%\n",
    "# dataset len 3196\n",
    "# [position] DIFF abs mean: 5.34ms (0.13) min:0.00ms max:550.61ms\n",
    "# \t60.4%\t < 5ms\t\t86.6%\t < 10ms\n",
    "# \t95.2%\t < 15ms\t\t98.2%\t < 20ms\n",
    "# \t99.4%\t < 25ms\t\t99.8%\t < 30ms\n",
    "# \t99.9%\t < 35ms\t\t100.0%\t < 40ms\n",
    "# \t100.0%\t < 45ms\t\t100.0%\t < 50ms\n",
    "# \t100.0%\t < 55ms\t\t100.0%\t < 60ms\n",
    "# \t100.0%\t < 65ms\t\t100.0%\t < 70ms\n",
    "# \t100.0%\t < 75ms\t\t100.0%\t < 80ms\n",
    "# \t100.0%\t < 85ms\t\t100.0%\t < 90ms\n",
    "# \t100.0%\t < 95ms\t\t100.0%\t < 100ms\n",
    "# \t100.0%\t < 105ms\t\t100.0%\t < 9999ms\n",
    "# 60.38% 86.56% 95.19% 98.19% 99.40% 99.80% 99.91% 99.95% 99.96% 99.96% 99.96% 99.96% 99.96% 99.96% 99.96% 99.96% 99.96% 99.96% 99.96% 99.96%\n",
    "#  -  -  -  -  -  - WITHOUT -  -  -  -  -  -\n",
    "# dataset len 1344\n",
    "# [id:696]  [ -922    23]    53 True\n",
    "# [id:173]  [  -13  2300]    32 True\n",
    "# [id:848]  [-1160    79]    58 True\n",
    "# [id:202]  [ -813   379]    51 True\n",
    "# [id:933]  [  -13   760]    33 False\n",
    "# [position] DIFF abs mean: 7.80ms (-0.17) min:0.00ms max:2299.52ms\n",
    "# \t56.3%\t < 5ms\t\t82.0%\t < 10ms\n",
    "# \t90.9%\t < 15ms\t\t94.5%\t < 20ms\n",
    "# \t96.5%\t < 25ms\t\t97.6%\t < 30ms\n",
    "# \t98.3%\t < 35ms\t\t98.7%\t < 40ms\n",
    "# \t99.0%\t < 45ms\t\t99.1%\t < 50ms\n",
    "# \t99.3%\t < 55ms\t\t99.3%\t < 60ms\n",
    "# \t99.4%\t < 65ms\t\t99.5%\t < 70ms\n",
    "# \t99.5%\t < 75ms\t\t99.5%\t < 80ms\n",
    "# \t99.5%\t < 85ms\t\t99.5%\t < 90ms\n",
    "# \t99.6%\t < 95ms\t\t99.6%\t < 100ms\n",
    "# \t99.6%\t < 105ms\t\t100.0%\t < 9999ms\n",
    "# 56.27% 81.96% 90.87% 94.49% 96.46% 97.61% 98.28% 98.70% 98.96% 99.13% 99.27% 99.34% 99.41% 99.45% 99.48% 99.50% 99.52% 99.54% 99.55% 99.57%\n",
    "# dataset len 3196\n",
    "# [id:953]  [  -22   803]    49 True\n",
    "# [id:2217]  [  -19   768]    64 True\n",
    "# [id:2836]  [  -13   788]    36 True\n",
    "# [position] DIFF abs mean: 5.65ms (0.68) min:0.00ms max:802.68ms\n",
    "# \t60.3%\t < 5ms\t\t86.5%\t < 10ms\n",
    "# \t95.1%\t < 15ms\t\t98.1%\t < 20ms\n",
    "# \t99.3%\t < 25ms\t\t99.7%\t < 30ms\n",
    "# \t99.8%\t < 35ms\t\t99.8%\t < 40ms\n",
    "# \t99.9%\t < 45ms\t\t99.9%\t < 50ms\n",
    "# \t99.9%\t < 55ms\t\t99.9%\t < 60ms\n",
    "# \t99.9%\t < 65ms\t\t99.9%\t < 70ms\n",
    "# \t99.9%\t < 75ms\t\t99.9%\t < 80ms\n",
    "# \t99.9%\t < 85ms\t\t99.9%\t < 90ms\n",
    "# \t99.9%\t < 95ms\t\t99.9%\t < 100ms\n",
    "# \t99.9%\t < 105ms\t\t100.0%\t < 9999ms\n",
    "# 60.33% 86.48% 95.09% 98.09% 99.30% 99.70% 99.81% 99.85% 99.86% 99.86% 99.86% 99.86% 99.86% 99.86% 99.86% 99.86% 99.86% 99.86% 99.86% 99.86%\n",
    "# Sat May  9 02:44:37 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "HqUHGv1kjctG"
   },
   "source": [
    "# Audio: prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbrY6Nt5jctH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "S = 30      # Target sequence length of longest target in batch\n",
    "S_min = 10  # Minimum target length, for demonstration purposes\n",
    "\n",
    "# Initialize random batch of input vectors, for *size = (T,N,C)\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n",
    "\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\n",
    "\n",
    "showTensor(target.detach())\n",
    "showTensor(input.detach())\n",
    "\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XhVSccMIjctJ",
    "outputId": "5d0ace89-9ec2-4f97-da1e-ef5c448e8408",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LightLSTM(nn.Module):\n",
    "    def __init__(self, feature_dim, out_dim, dropout_prob=0.05, with_hidden=False):\n",
    "        super().__init__()\n",
    "        self.batchnorm = nn.BatchNorm1d(feature_dim)\n",
    "        self.hidden_size = 128\n",
    "        self.rnn = nn.LSTM(feature_dim, self.hidden_size, batch_first=True, bidirectional=True, num_layers=2, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(self.hidden_size * 2, out_dim)\n",
    "        self.with_hidden = with_hidden\n",
    "        load(self, \"/content/spn/trained_weights/CTC-lstm-final.pth\")\n",
    "\n",
    "    def forward(self, features, masks, features_audio, masks_audio):\n",
    "        x = features_audio\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        x = self.batchnorm(x)\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        x, (hidden, _) = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        if self.with_hidden:\n",
    "            hidden = hidden[:2,:,:] + hidden[2:,:,:]\n",
    "            return x, hidden\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "audio_lstm = LightLSTM(INPUT_SIZE, KNOWN_LABELS_COUNT).to(device)\n",
    "show_audio(audio_lstm, test_dataset, \"test\", plot_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4y44LHmejctM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# work_dataset = train_dataset\n",
    "# evaluation = [test_dataset.batch(64), train_eval_dataset.batch(64)]\n",
    "# train(audio_lstm, 8, work_dataset.batch(32), evaluation, loss_function=LabelSmoothingLossAudio(KNOWN_LABELS_COUNT), train_function=audio_detection_trainer, lr_decay=0.98, lr=0.000161 * 0.131 * .1)\n",
    "# show_audio(audio_lstm, test_dataset, \"test\")\n",
    "# export_model(audio_lstm, \"/content/drive/My Drive/dataset/lstm-audio-final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "0AoUt7VLjctO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Audio detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2AlLwnCPjctP",
    "outputId": "76a356be-c5dd-4e20-d20c-129d0b9499d2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultyContextAttentionAudio(nn.Module):\n",
    "    def __init__(self, embedding_size, embedding_audio_size, hidden_size, vocab_size, device, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # self.encoder = Encoder(embedding_audio_size, embedding_audio_size, num_layers=2, dropout=dropout, time_scale=None)\n",
    "        self.encoder = LightLSTM(embedding_audio_size, vocab_size, with_hidden=True).to(device)\n",
    "\n",
    "        self.encoder_transcription = Encoder(hidden_size, embedding_size, num_layers=2, dropout=dropout, time_scale=None)\n",
    "        self.decoder_transcription = Decoder(vocab_size, hidden_size, vocab_size, num_layers=2, dropout=dropout, time_scale=None)\n",
    "\n",
    "        self.encoder_audio = Encoder(hidden_size, embedding_audio_size, num_layers=2, dropout=dropout, time_scale=1)\n",
    "        self.decoder_audio = Decoder(vocab_size, hidden_size, hidden_size, num_layers=2, dropout=dropout, time_scale=1)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size * 2, vocab_size)\n",
    "        self.out_single = nn.Linear(hidden_size, vocab_size)\n",
    "        self.out_chain = nn.Linear(hidden_size, vocab_size)\n",
    "        self.out_direct = nn.Linear(vocab_size, vocab_size)\n",
    "\n",
    "        self.fix_hidden = nn.Linear(self.encoder.hidden_size, hidden_size)\n",
    "        self.pos_encode = PositionalEncoding(vocab_size, dropout, scale=1)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.mode = \"chain\"\n",
    "        load(self, \"/content/spn/trained_weights/CTC-final.pth\")\n",
    "\n",
    "    def forward(self, features_transcription, mask_transcription, features_audio, mask_audio):\n",
    "        features_audio = features_audio * 32768.0\n",
    "        batch_size, audio_len, *features = features_audio.shape\n",
    "        batch_size, out_seq_len, *features = features_transcription.shape\n",
    "\n",
    "        # if mask_transcription is None: mask_transcription = torch.ones(batch_size, out_seq_len).bool().to(device)\n",
    "        # if mask_audio is None: mask_audio = torch.ones(batch_size, audio_len).bool().to(device)\n",
    "\n",
    "        encoded_inputs, hidden = self.encoder(features_transcription, mask_transcription, features_audio, mask_audio)\n",
    "\n",
    "\n",
    "        if self.mode == \"direct\":\n",
    "            return encoded_inputs\n",
    "\n",
    "        encoded_inputs = self.pos_encode(encoded_inputs)\n",
    "        hidden = self.fix_hidden(hidden)\n",
    "        encoder_transcription_outputs, hidden_transcription = self.encoder_transcription(features_transcription)\n",
    "\n",
    "        if self.mode == \"single\":\n",
    "            output_transcription, hidden_transcription = self.decoder_transcription(encoded_inputs, mask_audio, hidden_transcription + hidden, encoder_transcription_outputs, mask_transcription)\n",
    "            # return self.out_single(output_transcription)\n",
    "            return output_transcription\n",
    "\n",
    "        encoder_audio_outputs, hidden_audio = self.encoder_audio(features_audio)\n",
    "\n",
    "        if self.mode == \"fast\":\n",
    "            output_transcription, hidden_transcription = self.decoder_transcription(encoded_inputs, hidden_transcription, encoder_transcription_outputs, mask_transcription)\n",
    "            output_audio, hidden_audio = self.decoder_audio(encoded_inputs, hidden_audio, encoder_audio_outputs, mask_audio)\n",
    "            output = torch.cat((output_transcription, output_audio), 2)\n",
    "            return self.out(output)\n",
    "\n",
    "        if self.mode == \"chain\":\n",
    "            output_transcription, hidden_transcription = self.decoder_transcription(\n",
    "                # previous, mask_trans, hidden_state, encoder_outputs, mask_audio\n",
    "                encoded_inputs, mask_audio, hidden_transcription + hidden, encoder_transcription_outputs, mask_transcription,\n",
    "            )\n",
    "            # encoded_inputs = encoded_inputs + self.out_single(output_transcription)\n",
    "            # hidden_audio = hidden_audio + hidden_transcription + hidden\n",
    "            output_audio, hidden_audio = self.decoder_audio(output_transcription, mask_audio , hidden_audio, encoder_audio_outputs, mask_audio)\n",
    "            return self.out_chain(output_audio)\n",
    "\n",
    "        # # elif self.mode == \"iter\":\n",
    "        # #     outputs = torch.zeros(batch_size, out_seq_len, self.vocab_size).to(self.device) # tensor to store decoder outputs\n",
    "        # #     output = outputs[:,:1].clone()\n",
    "\n",
    "        # #     for t in range(out_seq_len):\n",
    "        # #         decoder_input = encoded_inputs[:,t:(t+1),:]\n",
    "\n",
    "        # #         output_transcription, hidden_transcription = self.decoder_transcription(decoder_input, hidden_transcription, encoder_transcription_outputs, mask)\n",
    "        # #         output_audio, hidden_audio = self.decoder_audio(decoder_input, hidden_audio, encoder_audio_outputs, masks_audio)\n",
    "\n",
    "        # #         output = torch.cat((output_transcription, output_audio), 2)\n",
    "        # #         output = self.out(output)\n",
    "        # #         outputs[:,t:(t+1),:] = output\n",
    "        # else:\n",
    "        #     raise Exception(\"wrong mode\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "multy_context_audio_model = MultyContextAttentionAudio(KNOWN_LABELS_COUNT, INPUT_SIZE, 256, KNOWN_LABELS_COUNT, device).to(device);\n",
    "# toy_dataset = DirectMaskDataset('train', limit=127)\n",
    "# train_iter = BucketIterator(train_dataset, batch_size=64, sort_key=lambda x: len(x.features), sort=False, shuffle=True, sort_within_batch=True)\n",
    "# evaluate_audio(\"train\", train_iter, multy_context_audio_model)\n",
    "#  Evaluation on train - avg_loss: 0.158870 count:72 Total loss:11.438619658350945\n",
    "show_audio(multy_context_audio_model, test_dataset, \"test\", plot_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mWZ_BH5jctT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# duration_model = duration_combined_model\n",
    "torch.cuda.empty_cache()\n",
    "work_dataset = train_dataset\n",
    "evaluation = [test_dataset.batch(64), train_eval_dataset.batch(64)]\n",
    "from time import time\n",
    "for i in range(1):\n",
    "\tstart = time()\n",
    "\ttrain(multy_context_audio_model, 8, work_dataset.batch(16), evaluation, loss_function=LabelSmoothingLossAudio(KNOWN_LABELS_COUNT), train_function=audio_detection_trainer, lr_decay=0.98, lr=0.000161 * 0.131 * 3.6)\n",
    "\tprint(time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-mKYkLGjctW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# evaluation = [test_dataset.batch(64), train_eval_dataset.batch(64)]\n",
    "train(multy_context_audio_model, 4, train_dataset.batch(16), evaluation, loss_function=LabelSmoothingLossAudio(KNOWN_LABELS_COUNT), train_function=audio_detection_trainer, lr_decay=0.98, lr=0.000161 * 0.131 * 3.6)\n",
    "show_audio(multy_context_audio_model, test_dataset, \"test\")\n",
    "show_audio(multy_context_audio_model, test_dataset, \"test\", duration_model=duration_combined_model)\n",
    "\n",
    "# [test]\n",
    "# [duration model]\n",
    "# danger: dtw_error 31.4% wrong idx:69\n",
    "# - warped_result: (376, 54)\n",
    "# - truth:(376, 54)\n",
    "# danger: dtw_error 31.3% wrong idx:933\n",
    "# - warped_result: (643, 54)\n",
    "# - truth:(643, 54)\n",
    "# [(233.0, 944), (251.75, 371), (340.0, 497), (390.0, 665), (715.0, 933)]\n",
    "# cache_hits: 0\n",
    "# [DETECTION+DTW]AVERAGE ERROR: 8.12% COUNT:1344\n",
    "# [DETECTION]AVERAGE ERROR: 8.73% COUNT:1344\n",
    "# [test] DIFF abs mean: 6.98ms (0.07) min:0.00ms max:715.00ms\n",
    "# \t49.3%\t < 5ms\t\t77.9%\t < 10ms\n",
    "# \t89.0%\t < 15ms\t\t93.6%\t < 20ms\n",
    "# \t95.9%\t < 25ms\t\t97.3%\t < 30ms\n",
    "# \t98.1%\t < 35ms\t\t98.7%\t < 40ms\n",
    "# \t99.0%\t < 45ms\t\t99.2%\t < 50ms\n",
    "# \t99.4%\t < 55ms\t\t99.5%\t < 60ms\n",
    "# \t99.6%\t < 65ms\t\t99.7%\t < 70ms\n",
    "# \t99.7%\t < 75ms\t\t99.8%\t < 80ms\n",
    "# \t99.8%\t < 85ms\t\t99.8%\t < 90ms\n",
    "# \t99.8%\t < 95ms\t\t99.9%\t < 100ms\n",
    "# \t99.9%\t < 105ms\t\t100.0%\t < 9999ms\n",
    "# 49.30% 77.92% 89.03% 93.57% 95.87% 97.27% 98.11% 98.66% 99.03% 99.24% 99.41% 99.53% 99.60% 99.67% 99.71% 99.75% 99.78% 99.81% 99.83% 99.86%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QIUPMTU2zF2P",
    "outputId": "0e563855-698e-4ab2-84fe-210b203ecd62"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "w9_WdkMxjcsQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "uwnK6mDojcsQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xutbEq7jcsR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WIN_STEP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 30>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mseparation\u001B[39m(num\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28mprint\u001B[39m((\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m - \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m32\u001B[39m \u001B[38;5;241m+\u001B[39m  \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m*\u001B[39m num)\n\u001B[0;32m---> 30\u001B[0m ms_per_step \u001B[38;5;241m=\u001B[39m \u001B[43mWIN_STEP\u001B[49m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'WIN_STEP' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import gridspec\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "def showTensor(aTensor, title=None, figsize=(12, 9), hide_x=False, labels=None, cmap=None, fig=True):\n",
    "    # print(aTensor.shape, aTensor.sum())\n",
    "    if len(aTensor.shape) > 2:\n",
    "        aTensor = aTensor[0]\n",
    "\n",
    "    if hasattr(aTensor, 'detach'): aTensor = aTensor.detach()\n",
    "    if hasattr(aTensor, 'cpu'): aTensor = aTensor.cpu()\n",
    "    if hasattr(aTensor, 'numpy'): aTensor = aTensor.numpy()\n",
    "\n",
    "    fig and plt.figure(figsize=figsize)\n",
    "    plt.imshow(aTensor.T, cmap=cmap)\n",
    "    title and plt.title(title)\n",
    "    hide_x and plt.xticks([])\n",
    "    if labels:\n",
    "        for x, y, label in labels:\n",
    "            plt.annotate(f\"{label}\",xy=(x + 2, y), color='white')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def duration_to_encoding(durations, pos_map):\n",
    "    return pos_map[durations.long()]\n",
    "\n",
    "def separation(num=5):\n",
    "    print((\" - \" * 32 +  \"\\n\") * num)\n",
    "\n",
    "ms_per_step = WIN_STEP * 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiQLBZcTjcsT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "FOUND_LABELS = dict([\n",
    "  ('h#', 12600), ('ix', 11587), ('s', 10114), ('iy', 9663), ('n', 9569), ('r', 9064),\n",
    "  ('tcl', 8978), ('l', 8157), ('kcl', 7823), ('ih', 6760), ('dcl', 6585), ('k', 6488),\n",
    "  ('t', 5899), ('m', 5429), ('ae', 5404), ('eh', 5293), ('z', 5046), ('ax', 4956), ('q', 4834),\n",
    "  ('d', 4793), ('axr', 4790), ('w', 4379), ('aa', 4197), ('ao', 4096), ('dh', 3879),\n",
    "  ('dx', 3649), ('pcl', 3609), ('p', 3545), ('ay', 3242), ('ah', 3185), ('f', 3128),\n",
    "  ('ey', 3088), ('b', 3067), ('sh', 3034), ('gcl', 3031), ('ow', 2913), ('er', 2846),\n",
    "  ('g', 2772), ('v', 2704), ('bcl', 2685), ('ux', 2488), ('y', 2349), ('epi', 2000),\n",
    "  ('ng', 1744), ('jh', 1581), ('hv', 1523), ('pau', 1343), ('nx', 1331), ('hh', 1313),\n",
    "  ('el', 1294), ('ch', 1081), ('th', 1018), ('en', 974), ('oy', 947), ('aw', 945),\n",
    "  ('uh', 756), ('uw', 725), ('ax-h', 493), ('zh', 225), ('em', 171), ('eng', 43), ])\n",
    "\n",
    "total_phonemes = sorted(FOUND_LABELS.items(), key=lambda x:-x[1])\n",
    "\n",
    "for row in zip(total_phonemes[::3], total_phonemes[1::3], total_phonemes[2::3]):\n",
    "    for i, (p, c) in enumerate(row):\n",
    "        print(i and \"\\t\\t\" or \"\", f\"{p}\\t{c}\", end=\"\", sep=\"\")\n",
    "    print()\n",
    "for i, (p, c) in enumerate(total_phonemes[-1:]):\n",
    "    print(i and \"\\t\\t\" or \"\", f\"{p}\\t{c}\", end=\"\", sep=\"\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsecGJE7jcsW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(sorted([(test_dataset[i].features.shape[0], i) for i in range(1300)]))[:10]\n",
    "all_datasets = [train_dataset, train_eval_dataset, test_dataset]\n",
    "for ds in all_datasets:print(len(ds))\n",
    "audio_durations = np.array(sum([[inp.features.shape[0] * ms_per_step for inp in dataset] for dataset in all_datasets], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oyvlnlebjcsb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def display_histogram(errors, name=\"\", unit='ms', plotting=False):\n",
    "    errors = errors.copy()\n",
    "    hist, bins = np.histogram(abs(errors))\n",
    "    hist = np.round(hist / len(errors) * 100, 1)\n",
    "    hist = np.cumsum(hist)\n",
    "\n",
    "    print(f\"[{name}] DIFF abs mean: {abs(errors).mean():.2f}{unit} ({errors.mean():.2f}) min:{abs(errors).min():.2f}{unit} max:{abs(errors).max():.2f}{unit}\")\n",
    "    rows = list(zip(hist, bins, bins[1:]))\n",
    "    for R  in zip(rows[::2], rows[1::2]):\n",
    "        s = \"\"\n",
    "        for h, b, e in R:\n",
    "            s += f\"\\t{h:.1f}%\\t < {e:.0f}{unit}\\t\"\n",
    "        print(s)\n",
    "\n",
    "    f, axarr = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    axarr[0].bar(range(len(bins)-1), np.round(hist).astype(int))\n",
    "    axarr[0].set_xticklabels([f\"{x:.0f}{unit}\" for x in bins], fontdict=None, minor=False)\n",
    "    axarr[0].yaxis.set_major_formatter(FormatStrFormatter('%d%%'))\n",
    "\n",
    "    axarr[1].hist(errors, bins=15)\n",
    "    axarr[1].xaxis.set_major_formatter(FormatStrFormatter(f'%d{unit}'))\n",
    "    return [f, *axarr]\n",
    "\n",
    "f, ax1, ax2 = display_histogram(audio_durations, plotting=True)\n",
    "ax1.title.set_text(\"1. Audio durations less than X by percentage\")\n",
    "ax2.title.set_text(\"2. Audio duration occurence count\")\n",
    "ax1.set_xlabel('Audio duration', fontsize=12)\n",
    "ax1.set_ylabel('Percentage', fontsize=12)\n",
    "ax1.tick_params(axis='both', which='both', labelsize=12)\n",
    "ax2.set_xlabel('Audio duration', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.tick_params(axis='both', which='both', labelsize=12)\n",
    "f.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lq7Jw3bujcse",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "audio_phoneme_count = np.array(sum([[inp.border.shape[0] for inp in dataset] for dataset in all_datasets], []))\n",
    "print(sum(audio_phoneme_count))\n",
    "f, ax1, ax2 = display_histogram(audio_phoneme_count, plotting=True, unit='')\n",
    "\n",
    "ax1.title.set_text(\"1. Phoneme count less than X by percentage\")\n",
    "ax2.title.set_text(\"2. Phoneme occurence count\")\n",
    "ax1.set_xlabel('Phoneme count', fontsize=12)\n",
    "ax1.set_ylabel('Percentage', fontsize=12)\n",
    "ax1.tick_params(axis='both', which='both', labelsize=12)\n",
    "ax2.set_xlabel('Phoneme count', fontsize=12)\n",
    "ax2.set_ylabel('Audio segment count', fontsize=12)\n",
    "ax2.tick_params(axis='both', which='both', labelsize=12)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "68jnPLNPjcsg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inps = sum([[inp for inp in dataset] for dataset in all_datasets], [])\n",
    "phoneme_map = defaultdict(list)\n",
    "for inp in inps:\n",
    "    phoneme_ids = np.argmax(inp.in_transcription.cpu(), axis=1).tolist()\n",
    "    phoneme_durs = (inp.out_duration.cpu()* DURATION_SCALER).tolist()\n",
    "    for pid, pdur in zip(phoneme_ids, phoneme_durs):\n",
    "        phoneme_map[pid].append(pdur)\n",
    "\n",
    "\n",
    "\n",
    "mean_phoneme_dur = sorted([[KNOWN_LABELS[pid], np.mean(val) ] for pid, val in phoneme_map.items()], key=lambda x: x[1])\n",
    "# mean_phoneme_dur = (zip(*mean_phoneme_dur))\n",
    "# plt.figure(figsize=(24, 6))\n",
    "# plt.bar(*mean_phoneme_dur)\n",
    "print(len(mean_phoneme_dur))\n",
    "for row in zip(mean_phoneme_dur[::3], mean_phoneme_dur[1::3], mean_phoneme_dur[2::3]):\n",
    "    for i, (p, c) in enumerate(row):\n",
    "        print(i and \"\\t\\t\" or \"\", f\"{p}\\t{c:.0f}ms\", end=\"\", sep=\"\")\n",
    "    print()\n",
    "for i, (p, c) in enumerate(mean_phoneme_dur[-2:]):\n",
    "    print(i and \"\\t\\t\" or \"\", f\"{p}\\t{c:.0f}ms\", end=\"\", sep=\"\")\n",
    "print(\"\\n.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VrJ-Sq44jcsk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp: Utterance = test_dataset[36]\n",
    "audio_file = inp.label_file\n",
    "audio = sf.read(audio_file)[0]\n",
    "plt.figure()\n",
    "plt.plot(audio, '-', );\n",
    "plt.figure()\n",
    "plt.plot(audio[100:132], '-', );\n",
    "\n",
    "s, audio2 = wavfile.read(audio_file)\n",
    "print(audio.min(), audio.max())\n",
    "print(audio2.min(), audio2.max())\n",
    "print((audio2[:3] / audio[:3]).mean())\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(9, 3), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(ncols=2, nrows=1)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "\n",
    "ax1.plot(audio, '-', )\n",
    "ax1.title.set_text(\"1. Audio waveform\")\n",
    "ax1.set_xlabel('Audio sample index', fontsize=13)\n",
    "ax1.set_ylabel('Audio intensity', fontsize=13)\n",
    "ax1.tick_params(axis='both', which='both', labelsize=13)\n",
    "\n",
    "ax2.plot(audio, '-', )\n",
    "ax2.title.set_text(\"2. Audio waveform magnified\")\n",
    "ax2.set_xlabel('Audio sample index', fontsize=13)\n",
    "ax2.set_ylabel('Audio intensity', fontsize=13)\n",
    "ax2.tick_params(axis='both', which='both', labelsize=13)\n",
    "ax2.set_xlim([100, 131])\n",
    "ax2.set_ylim([-0.0009, 0.0009])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFlmalK6jcsn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pos = PositionalEncoding(26, 0.0, scale=1)\n",
    "pos_map = pos.pe.transpose(0, 1)[0]\n",
    "\n",
    "inp: Utterance = test_dataset[36]\n",
    "\n",
    "audio = inp.features\n",
    "length = audio.shape[0]\n",
    "\n",
    "label_vec = inp.label_vec\n",
    "actual_borders = inp.border\n",
    "pos_feature = pos_map[:length,:]\n",
    "print(actual_borders.long())\n",
    "\n",
    "combined = pos_feature + audio.cpu() / 2\n",
    "\n",
    "highlighted = torch.zeros(length, 8)\n",
    "highlighted[actual_borders.long()] = 1\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.xlabel('Audio frame index', fontsize=13)\n",
    "plt.ylabel('Position encoding features', fontsize=13)\n",
    "showTensor(pos_feature, 'Position encoding', fig=False)\n",
    "separation()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.xlabel('Audio frame index', fontsize=13)\n",
    "plt.ylabel('Transformed audio features', fontsize=13)\n",
    "showTensor(audio.cpu(), 'Transformed audio representation', fig=False)\n",
    "print(\"We can see the border at 57\")\n",
    "\n",
    "\n",
    "showTensor(combined, 'Audio combined with position encodingss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxZDmnVQjcsp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels_ids = np.argmax(inp.in_transcription.cpu(), axis=1).tolist()\n",
    "labels = [KNOWN_LABELS[lab] for lab in labels_ids]\n",
    "labels_pos = inp.border.long().cpu().tolist()\n",
    "print(labels, len(labels))\n",
    "\n",
    "def show_numbers(numbers, labels, title):\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(numbers, '-', );\n",
    "    for i,j in enumerate(numbers):\n",
    "        plt.annotate(f\"{labels[i]}\\n{j:.0f}\",xy=(i-1,j-1))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# showTensor(audio.cpu(), 'Audio')\n",
    "# showTensor(label_vec.cpu(), 'Labels one-hot encoded', labels=zip(labels_pos, labels_ids, labels), figsize=(12, 6))\n",
    "# show_numbers(inp.border.cpu().tolist(), labels, 'In order of occurence phoneme border position as index of audio segment');\n",
    "# show_numbers((inp.out_duration * DURATION_SCALER).cpu().tolist(), labels, 'In order of occurence phoneme durations in milliseconds');\n",
    "\n",
    "fig = plt.figure(figsize=(9, 11), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(ncols=1, nrows=4, height_ratios=[3, 6, 3, 3])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax2 = fig.add_subplot(gs[1], sharex=ax1)\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax4 = fig.add_subplot(gs[3], sharex=ax3)\n",
    "\n",
    "\n",
    "ax1.imshow(audio.cpu().numpy().T, aspect='auto')\n",
    "ax1.title.set_text(\"1. Audio\")\n",
    "ax1.set_xlabel('Audio frame index', fontsize=13)\n",
    "ax1.set_ylabel('Features', fontsize=13)\n",
    "ax1.tick_params(axis='both', which='both', labelsize=13)\n",
    "\n",
    "ax2.imshow(label_vec.cpu().numpy().T, aspect='auto')\n",
    "ax2.title.set_text(\"2. Labels one-hot encoded\")\n",
    "ax2.set_xlabel('Audio frame index', fontsize=13)\n",
    "ax2.set_ylabel('One-hot encoded phonemes', fontsize=13)\n",
    "ax2.tick_params(axis='both', which='both', labelsize=13)\n",
    "for x, y, label in zip(labels_pos, labels_ids, labels):\n",
    "    ax2.annotate(f\"{label}\",xy=(x + 2, y), color='white')\n",
    "ax2.annotate(f\"{label}\",xy=(x - 3, y + 1), color='white')\n",
    "\n",
    "numbers =inp.border.cpu().tolist()\n",
    "ax3.plot(numbers, '-', );\n",
    "for i,j in enumerate(numbers):\n",
    "    ax3.annotate(f\"{labels[i]}\\n{j:.0f}\",xy=(i,j))\n",
    "\n",
    "ax3.title.set_text(\"3. In order of occurence phoneme border position as index of audio segment\")\n",
    "ax3.set_xlabel('Phoneme order number', fontsize=13)\n",
    "ax3.set_ylabel('Audio index', fontsize=13)\n",
    "ax3.tick_params(axis='both', which='both', labelsize=13)\n",
    "ax3.set_ylim([0, 140])\n",
    "\n",
    "numbers = (inp.out_duration * DURATION_SCALER).cpu().tolist()\n",
    "ax4.plot(numbers, '-', );\n",
    "for i,j in enumerate(numbers):\n",
    "    ax4.annotate(f\"{labels[i]}\\n{j:.0f}\",xy=(i,j))\n",
    "\n",
    "ax4.title.set_text(\"4. In order of occurence phoneme durations in milliseconds\")\n",
    "ax4.set_xlabel('Phoneme order number', fontsize=13)\n",
    "ax4.set_ylabel('Phoneme duration', fontsize=13)\n",
    "ax4.tick_params(axis='both', which='both', labelsize=13)\n",
    "ax4.yaxis.set_major_formatter(FormatStrFormatter('%dms'))\n",
    "ax4.set_ylim([0, 200])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2tppQSMjcsr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vec = pos_feature[57].clone()\n",
    "pos_feature_high = pos_feature.clone()\n",
    "pos_feature_high[57] = 0\n",
    "# showTensor(pos_feature_high, 'Position encoding with border position')\n",
    "# print(\"Highlighted section on the positon encoding matchin the temporal positon. Extracted as a vector to the left.\")\n",
    "\n",
    "vec_noised = (torch.rand_like(vec) * 1 + vec) / 2\n",
    "\n",
    "# showTensor(vec.unsqueeze(0), 'Vector', figsize=(12, 3), hide_x=True)\n",
    "# showTensor(vec_noised.unsqueeze(0), 'Vector\\'', figsize=(12, 3), hide_x=True)\n",
    "\n",
    "attn = F.softmax(torch.bmm(vec.unsqueeze(0).unsqueeze(0), pos_feature.unsqueeze(0).transpose(1, 2)).transpose(1, 2).squeeze(0), dim=0)\n",
    "attn_noised = F.softmax(torch.bmm(vec_noised.unsqueeze(0).unsqueeze(0), pos_feature.unsqueeze(0).transpose(1, 2)).transpose(1, 2).squeeze(0), dim=0)\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(12, 6))\n",
    "# gs = gridspec.GridSpec(ncols=1, nrows=2, height_ratios=[5, 1], hspace=0.01)\n",
    "\n",
    "# ax1 = fig.add_subplot(gs[0])\n",
    "# ax2 = fig.add_subplot(gs[1], sharex=ax1)\n",
    "# # plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "\n",
    "# # plt.setp([ax1, ax2], title='Test')\n",
    "# # fig.suptitle('An overall title', size=20)\n",
    "\n",
    "\n",
    "# # fig, a =  plt.subplots(nrows=2, figsize=, gridspec_kw={'hspace': 0., 'wspace': 0.}, sharex=True)\n",
    "# ax1.imshow(pos_feature_high.numpy().T)\n",
    "# ax1.title.set_text(\"Position encoding with a single vector extracted\")\n",
    "# ax2.plot(attn.numpy(), label='Vector')\n",
    "# ax2.title.set_text(\"The activation of the vector over the positions\")\n",
    "# ax2.plot(attn_noised.numpy(), label='Vector with added noise')\n",
    "# ax2.title.set_text(\"The activation of the vector over the positions\")\n",
    "# ax2.legend()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6), constrained_layout=True,)\n",
    "gs = gridspec.GridSpec(ncols=2, nrows=2, width_ratios=[1, 10], height_ratios=[5, 5])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "ax2 = fig.add_subplot(gs[1, 1], sharex=ax1)\n",
    "ax3 = fig.add_subplot(gs[0, 0])\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "ax3.title.set_text(\"2. Vector[57]\")\n",
    "ax3.imshow(vec.unsqueeze(0).cpu().numpy().T)\n",
    "ax3.set_xticks([57])\n",
    "\n",
    "vec_noised = (torch.rand_like(vec) + vec) / 2\n",
    "ax4.title.set_text(\"2. Vector[57]+noise\")\n",
    "ax4.imshow(vec_noised.unsqueeze(0).cpu().numpy().T)\n",
    "ax4.set_xticks([57])\n",
    "\n",
    "ax1.imshow(pos_feature_high.numpy().T, aspect='auto')\n",
    "ax1.title.set_text(\"1. Position encoding with a single vector extracted at index 57\")\n",
    "ax1.set_xlabel('Audio frame index', fontsize=13)\n",
    "ax1.set_ylabel('Position encoding features', fontsize=13)\n",
    "ax1.tick_params(axis='both', which='both', labelsize=13)\n",
    "\n",
    "ax2.title.set_text(\"3. The activation of the vector over the positions\")\n",
    "ax2.plot(attn.numpy(), label='Vector')\n",
    "ax2.plot(attn_noised.numpy(), label='Vector with added noise')\n",
    "ax2.tick_params(axis='both', which='both', labelsize=13)\n",
    "ax2.set_xlabel('Audio frame index', fontsize=13)\n",
    "ax2.set_ylabel('Activation', fontsize=13)\n",
    "ax2.legend()\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-PIl-z9jcsu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "durations = torch.cumsum(inp.out_duration, dim=0) * DURATION_SCALER / ms_per_step\n",
    "\n",
    "target = pos_map[durations.long()]\n",
    "\n",
    "showTensor(pos_feature[:100])\n",
    "\n",
    "showTensor(target[:100])\n",
    "showTensor(audio.cpu())\n",
    "\n",
    "att = Attention(POS_DIM)\n",
    "new_map, weights = att(target.unsqueeze(0), None, pos_feature.unsqueeze(0), None)\n",
    "showTensor(weights.transpose(1, 2))\n",
    "showTensor(label_vec.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "PYgU5efAjcsw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntXZi8XEjcsx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp: Utterance = test_dataset[36]\n",
    "\n",
    "audio = inp.features\n",
    "length = audio.shape[0]\n",
    "\n",
    "label_vec = inp.label_vec\n",
    "actual_borders = inp.border\n",
    "transcription = inp.in_transcription\n",
    "\n",
    "labels_ids = np.argmax(transcription.cpu(), axis=1).tolist()\n",
    "labels = [KNOWN_LABELS[lab] for lab in labels_ids]\n",
    "labels_pos = actual_borders.cpu().tolist()\n",
    "\n",
    "attention = position_model.with_weights(transcription.unsqueeze(0), None, audio.unsqueeze(0), None).detach().squeeze(0)\n",
    "\n",
    "audio = audio.clone()\n",
    "audio -= audio.min()\n",
    "audio /= audio.max()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 9), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(ncols=2, nrows=4, height_ratios=[2, 6, 2, 3], width_ratios=[5, 2])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "ax3 = fig.add_subplot(gs[2, 0], sharex=ax1)\n",
    "ax4 = fig.add_subplot(gs[1, 1], sharey=ax2)\n",
    "ax5 = fig.add_subplot(gs[3, 0], sharex=ax1)\n",
    "ax6 = fig.add_subplot(gs[3, 1], sharey=ax5)\n",
    "\n",
    "ax1.imshow(audio.cpu().numpy().T, origin=\"lower\", aspect='auto')\n",
    "ax1.title.set_text(\"1. Audio features\")\n",
    "ax1.set_ylabel('Audio features', fontsize=13)\n",
    "ax1.set_xlabel('Audio frames index', fontsize=13)\n",
    "\n",
    "ax2.imshow(attention.cpu().numpy(), origin=\"lower\", aspect='auto')\n",
    "ax2.title.set_text(\"3. Attention matrix\")\n",
    "ax2.set_xlabel('Audio frames index', fontsize=13)\n",
    "ax2.set_ylabel('Phoneme occurence order', fontsize=13)\n",
    "# ax2.tick_params(axis='both', which='both', labelsize=12)\n",
    "ax2.set_yticks(range(len(labels)))\n",
    "ax2.set_yticklabels([f\"{i+1}. {p}\" for i, p in enumerate(labels)])\n",
    "\n",
    "ax3.plot(attention[3].cpu().numpy(), label='Attention score')\n",
    "ax3.title.set_text(\"4. The activation of the third phoneme over the audio\")\n",
    "ax3.set_xlabel('Audio frames index', fontsize=13)\n",
    "ax3.set_ylabel('Activation', fontsize=13)\n",
    "ax3.legend()\n",
    "\n",
    "ax4.imshow(transcription.cpu().numpy(), origin=\"lower\", aspect='auto')\n",
    "ax4.title.set_text(\"2. One-hot phoneme transcriptions\")\n",
    "ax4.set_ylabel('Phoneme occurence order', fontsize=13)\n",
    "ax4.set_xlabel('One-hot phoneme ID', fontsize=13)\n",
    "\n",
    "ax5.imshow((attention[3] * audio.T).cpu().numpy(), origin=\"lower\", aspect='auto')\n",
    "ax5.title.set_text(\"5. Audio weighted by the third phoneme attention\")\n",
    "ax5.set_xlabel('Audio frames index * attention weight', fontsize=13)\n",
    "ax5.set_ylabel('Weighted features', fontsize=13)\n",
    "\n",
    "ax6.imshow((attention[3] * audio.T).sum(1).unsqueeze(0).cpu().numpy().T, origin=\"lower\", aspect=0.65)\n",
    "# ax6.title.set_text(\"6. Vector of weighted sum of the audio features\")\n",
    "ax6.title.set_text(\"6. Weighted sum audio feature vector\")\n",
    "ax6.set_xticklabels([], minor=False)\n",
    "ax6.set_yticklabels([], minor=False)\n",
    "ax6.set_ylabel('Aduio features', fontsize=13)\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxaS20Zqjcsz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp: Utterance = test_dataset[ 36  ]  # 36  1168 933\n",
    "\n",
    "audio = inp.features.clone()\n",
    "length = audio.shape[0]\n",
    "\n",
    "label_vec = inp.label_vec\n",
    "actual_borders = inp.border\n",
    "transcription = inp.in_transcription\n",
    "t_count, t_features = transcription.shape\n",
    "\n",
    "\n",
    "# audio = (audio + audio * torch.rand_like(audio) * 3) / 3\n",
    "attention = position_model.with_weights(transcription.unsqueeze(0), None, audio.unsqueeze(0), None).detach().squeeze(0)\n",
    "\n",
    "audio = audio.clone()\n",
    "audio -= audio.min()\n",
    "audio /= audio.max()\n",
    "\n",
    "labels_ids = np.argmax(transcription.cpu(), axis=1).tolist()\n",
    "labels = [KNOWN_LABELS[lab] for lab in labels_ids]\n",
    "labels_pos = actual_borders.cpu().tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 9), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(ncols=1, nrows=3, height_ratios=[3, 10, 3])#, width_ratios=[5, 2])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "ax3 = fig.add_subplot(gs[2, 0], sharex=ax1)\n",
    "# ax4 = fig.add_subplot(gs[1, 1], sharey=ax2)\n",
    "\n",
    "ax1.imshow(audio.cpu().numpy().T, origin=\"lower\", aspect='auto')\n",
    "ax1.title.set_text(\"1. Audio features\")\n",
    "ax1.set_ylabel('Audio features', fontsize=13)\n",
    "ax1.set_xlabel('Audio frames index', fontsize=13)\n",
    "\n",
    "\n",
    "ax2.imshow(attention.cpu().numpy(), origin=\"lower\", aspect='auto')\n",
    "ax2.title.set_text(\"3. Attention matrix\")\n",
    "ax2.set_xlabel('Audio frames index', fontsize=13)\n",
    "ax2.set_ylabel('Phoneme index', fontsize=13)\n",
    "ax2.tick_params(axis='both', which='both', labelsize=13)\n",
    "ax2.set_yticks(list( range(t_count)))\n",
    "ax2.set_yticklabels([f\"{i+1}. {p}{(i%2 == 6) * ' - - '}\" for i, p in enumerate(labels)])\n",
    "\n",
    "ax3.plot(attention[3].cpu().numpy(), label=f'4. {labels[3]}')\n",
    "ax3.plot(attention[10 - 1].cpu().numpy(), label=f'10. {labels[9]}')\n",
    "ax3.plot(attention[14 - 1].cpu().numpy(), label=f'14. {labels[13]}')\n",
    "ax3.plot(attention[18 - 1].cpu().numpy(), label=f'18. {labels[17]}')\n",
    "ax3.plot(attention[23 - 1].cpu().numpy(), label=f'23. {labels[22]}')\n",
    "for border in actual_borders[[3, 9, 13, 17, 22]]:\n",
    "    ax3.axvline(x=border)\n",
    "\n",
    "ax3.title.set_text(\"4. The activation of four phonemes over the audio with actual borders\")\n",
    "ax3.set_xlabel('Audio frames index', fontsize=13)\n",
    "ax3.set_ylabel('Activation', fontsize=13)\n",
    "ax3.legend()\n",
    "\n",
    "# ax4.imshow(transcription.cpu().numpy())\n",
    "# ax4.title.set_text(\"2. One-hot phoneme transcriptions transposed\")\n",
    "# ax4.set_ylabel('Phoneme index', fontsize=13)\n",
    "# ax4.set_xlabel('One-hot phoneme ID', fontsize=13)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 2), constrained_layout=True)\n",
    "plt.plot(attention[3].cpu().numpy(), label=f'4. {labels[3]}')\n",
    "plt.plot(attention[10 - 1].cpu().numpy(), label=f'10. {labels[9]}')\n",
    "plt.plot(attention[14 - 1].cpu().numpy(), label=f'14. {labels[13]}')\n",
    "plt.plot(attention[18 - 1].cpu().numpy(), label=f'18. {labels[17]}')\n",
    "plt.plot(attention[23 - 1].cpu().numpy(), label=f'23. {labels[22]}')\n",
    "for border in actual_borders[[3, 9, 13, 17, 22]]:\n",
    "    plt.axvline(x=border)\n",
    "plt.xlim([0, length-1])\n",
    "plt.title(\"The activation of four phonemes over the audio with actual borders\")\n",
    "plt.xlabel('Audio frames index', fontsize=13)\n",
    "plt.ylabel('Activation', fontsize=13)\n",
    "plt.legend()\n",
    "plt.plot()\n",
    "\n",
    "\n",
    "index = 9\n",
    "fig = plt.figure(figsize=(10, 4), constrained_layout=True)\n",
    "att_local = attention[index].cpu().numpy()\n",
    "predicted_border = (np.arange(length) * attention[index].cpu().numpy()).sum()\n",
    "\n",
    "plt.plot(att_local, label=f'{index+1}. {labels[index]}', color=[ax2._get_patches_for_fill.get_next_color() for c in '123'][1])\n",
    "# for border in actual_borders[[9]]:\n",
    "#     plt.axvline(x=border, c='g')\n",
    "\n",
    "s, e = 30, 65\n",
    "for i, y in list(enumerate(att_local))[s:e:2]:\n",
    "    plt.text(i-0.3, y, f'{y:.1f} * {i}', rotation=65)\n",
    "\n",
    "\n",
    "plt.xlim([s, e])\n",
    "plt.ylim([-0.05, 0.35])\n",
    "\n",
    "plt.axvline(x=actual_borders[index]-0.15, c='g', label=f'Actual border ({actual_borders[index]:.1f})')\n",
    "plt.axvline(x=predicted_border, c='b', label=f'Soft poitner border ({predicted_border:.1f})')\n",
    "plt.axvline(x=50, c='r', label='Argmax border (50)')\n",
    "\n",
    "\n",
    "plt.title(\"The activation value of phoneme /iy/ multiplied by the audio index value\", fontsize=13)\n",
    "plt.xlabel('Audio frames index', fontsize=13)\n",
    "plt.ylabel('Activation', fontsize=13)\n",
    "plt.legend()\n",
    "# plt.text(21, 0.1, 'Actual attention', bbox={'facecolor': 'white', 'pad': 2})\n",
    "plt.plot()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvxUiHgMjcs2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp: Utterance = test_dataset[36]\n",
    "\n",
    "audio = inp.features.clone()\n",
    "length = audio.shape[0]\n",
    "\n",
    "label_vec = inp.label_vec\n",
    "actual_borders = inp.border\n",
    "transcription = inp.in_transcription\n",
    "t_count, t_features = transcription.shape\n",
    "\n",
    "\n",
    "# audio = (audio + audio * torch.rand_like(audio) * 3) / 3\n",
    "attention = position_model.with_weights(transcription.unsqueeze(0), None, audio.unsqueeze(0), None).detach()\n",
    "positions = position_model.weights_to_positions(attention).detach().squeeze(0).cpu().numpy()\n",
    "attention = attention.squeeze(0)\n",
    "\n",
    "audio = audio.clone()\n",
    "audio -= audio.min()\n",
    "audio /= audio.max()\n",
    "\n",
    "labels_ids = np.argmax(transcription.cpu(), axis=1).tolist()[:-1]\n",
    "labels = [KNOWN_LABELS[lab] for lab in labels_ids]\n",
    "labels_pos = actual_borders.cpu().tolist()\n",
    "label_colors = [ax2._get_patches_for_fill.get_next_color() for c in labels_ids]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 2), constrained_layout=True)\n",
    "for i in range(0, len(labels_ids), 1):\n",
    "    c = label_colors[i]\n",
    "    plt.plot(attention[i].cpu().numpy(), label=f'{i+1}. {labels[i]}', c=c)\n",
    "    plt.axvline(x=labels_pos[i], c=c, linestyle='-')\n",
    "    plt.axvline(x=positions[i], c=c, linestyle='-.')\n",
    "\n",
    "plt.xlim([10, 120])\n",
    "plt.ylim([-0.05, 1.1])\n",
    "\n",
    "plt.title(\"Predicted borders (dotted) compared with actual borders (solid)\", fontsize=13)\n",
    "plt.xlabel('Audio frames index', fontsize=13)\n",
    "plt.ylabel('Activation', fontsize=13)\n",
    "# plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBMqHHk2jcs4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp_batch: UtteranceBatch = next(b for b in train_dataset.batch(128))\n",
    "\n",
    "\n",
    "position_model.eval()\n",
    "attention = position_model.with_weights(\n",
    "    inp_batch.in_transcription.padded, inp_batch.in_transcription.masks,\n",
    "    inp_batch.features.padded,\n",
    "    inp_batch.features.masks).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "batched_att = []\n",
    "for i in range(64, 128, 8):\n",
    "    ts, fs = (inp_batch.in_transcription.lengths[i], inp_batch.features.lengths[i])\n",
    "    attention[i,ts:] = 0.25\n",
    "    attention[i,:, fs:] = 0.25\n",
    "    batched_att.append(attention[i])\n",
    "\n",
    "    # plt.axvline(x=fs)\n",
    "    # plt.axhline(y=ts)\n",
    "    # plt.imshow(attention[i], origin=\"lower\", aspect='auto')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(ncols=2, nrows=2)#, height_ratios=[3, 10, 3])#, width_ratios=[5, 2])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "ax3 = fig.add_subplot(gs[0, 1], sharex=ax1, sharey=ax1)\n",
    "ax4 = fig.add_subplot(gs[1, 1], sharey=ax2, sharex=ax3)\n",
    "\n",
    "index = -1\n",
    "ax1.imshow(batched_att[index], origin=\"lower\", aspect='auto')\n",
    "ax1.title.set_text(\"1. Attention batch 1\")\n",
    "index = -2\n",
    "ax2.imshow(batched_att[index], origin=\"lower\", aspect='auto')\n",
    "ax2.title.set_text(\"2. Attention batch 2\")\n",
    "index = -3\n",
    "ax3.imshow(batched_att[index], origin=\"lower\", aspect='auto')\n",
    "ax3.title.set_text(\"3. Attention batch 3\")\n",
    "index = -4\n",
    "ax4.imshow(batched_att[index], origin=\"lower\", aspect='auto')\n",
    "ax4.title.set_text(\"4. Attention batch 4\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHCzwQldjcs6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp: Utterance = test_dataset[36]\n",
    "\n",
    "audio = inp.features.clone()\n",
    "length = audio.shape[0]\n",
    "\n",
    "label_vec = inp.label_vec\n",
    "actual_borders = inp.border\n",
    "transcription = inp.in_transcription\n",
    "t_count, t_features = transcription.shape\n",
    "\n",
    "pos_encoding_out = position_model.with_position(transcription.unsqueeze(0), None, audio.unsqueeze(0), None).detach().cpu().squeeze(0)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(ncols=3, nrows=1)\n",
    "\n",
    "labels_ids = np.argmax(transcription.cpu(), axis=1).tolist()\n",
    "labels = [KNOWN_LABELS[lab] for lab in labels_ids]\n",
    "labels_pos = actual_borders.cpu().tolist()\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax2 = fig.add_subplot(gs[1], sharey=ax1)\n",
    "ax3 = fig.add_subplot(gs[2], sharey=ax1)\n",
    "\n",
    "ax1.imshow(inp.position.cpu().numpy().T, aspect='auto')\n",
    "ax1.title.set_text(\"1. Position encoding actual\")\n",
    "ax1.set_xlabel('Phonemes ordered', fontsize=13)\n",
    "ax1.set_ylabel('Position encoding features', fontsize=13)\n",
    "ax1.tick_params(axis='both', which='both', labelsize=13)\n",
    "ax1.set_xticks(list(range(t_count - 1))[::2])\n",
    "ax1.set_xticklabels([f\"{i+1}. {p}{(i%2 == 6) * ' - - '}\" for i, p in enumerate(labels)][::2], rotation=45)\n",
    "\n",
    "ax2.imshow(pos_encoding_out.cpu().numpy().T, aspect='auto')\n",
    "ax2.title.set_text(\"2. Position encoding predicted\")\n",
    "ax2.set_xlabel('Phonemes ordered', fontsize=13)\n",
    "ax2.tick_params(axis='both', which='both', labelsize=13)\n",
    "ax2.set_xticks(list(range(t_count - 1))[::2])\n",
    "ax2.set_xticklabels([f\"{i+1}. {p}{(i%2 == 6) * ' - - '}\" for i, p in enumerate(labels)][::2], rotation=45)\n",
    "\n",
    "ax3.imshow((pos_encoding_out - inp.position).abs().cpu().numpy().T, aspect='auto')\n",
    "ax3.title.set_text(\"3. Difference\")\n",
    "ax3.set_xlabel('Phonemes ordered', fontsize=13)\n",
    "ax3.tick_params(axis='both', which='both', labelsize=13)\n",
    "ax3.set_xticks(list(range(t_count - 1))[::2])\n",
    "ax3.set_xticklabels([f\"{i+1}. {p}{(i%2 == 6) * ' - - '}\" for i, p in enumerate(labels)][::2], rotation=45)\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8VRBQZQjcs9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp: Utterance = test_dataset[ 36  ]  # 36  1168 933\n",
    "\n",
    "audio = inp.features.clone()\n",
    "length = audio.shape[0]\n",
    "\n",
    "label_vec = inp.label_vec\n",
    "actual_borders = inp.border\n",
    "transcription = inp.in_transcription\n",
    "t_count, t_features = transcription.shape\n",
    "\n",
    "n = 15\n",
    "audio[40 + n:50 + n] = audio.min()\n",
    "\n",
    "# audio = (audio + audio * torch.rand_like(audio) * 3) / 3\n",
    "attention = position_model.with_weights(transcription.unsqueeze(0), None, audio.unsqueeze(0), None).detach().squeeze(0)\n",
    "\n",
    "audio = audio.clone()\n",
    "audio -= audio.min()\n",
    "audio /= audio.max()\n",
    "\n",
    "labels_ids = np.argmax(transcription.cpu(), axis=1).tolist()\n",
    "labels = [KNOWN_LABELS[lab] for lab in labels_ids]\n",
    "labels_pos = actual_borders.cpu().tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 9), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(ncols=1, nrows=3, height_ratios=[3, 10, 3])#, width_ratios=[5, 2])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "ax3 = fig.add_subplot(gs[2, 0], sharex=ax1)\n",
    "# ax4 = fig.add_subplot(gs[1, 1], sharey=ax2)\n",
    "\n",
    "ax1.imshow(audio.cpu().numpy().T, origin=\"lower\", aspect='auto')\n",
    "ax1.title.set_text(\"1. Audio features\")\n",
    "ax1.set_ylabel('Audio features', fontsize=13)\n",
    "ax1.set_xlabel('Audio frames index', fontsize=13)\n",
    "\n",
    "\n",
    "ax2.imshow(attention.cpu().numpy(), origin=\"lower\", aspect='auto')\n",
    "ax2.title.set_text(\"3. Attention matrix\")\n",
    "ax2.set_xlabel('Audio frames index', fontsize=13)\n",
    "ax2.set_ylabel('Phoneme index', fontsize=13)\n",
    "ax2.tick_params(axis='both', which='both', labelsize=13)\n",
    "ax2.set_yticks(list( range(t_count)))\n",
    "ax2.set_yticklabels([f\"{i+1}. {p}{(i%2 == 6) * ' - - '}\" for i, p in enumerate(labels)])\n",
    "\n",
    "ax3.plot(attention[3].cpu().numpy(), label=f'4. {labels[3]}')\n",
    "ax3.plot(attention[10 - 1].cpu().numpy(), label=f'10. {labels[9]}')\n",
    "ax3.plot(attention[14 - 1].cpu().numpy(), label=f'14. {labels[13]}')\n",
    "ax3.plot(attention[18 - 1].cpu().numpy(), label=f'18. {labels[17]}')\n",
    "ax3.plot(attention[23 - 1].cpu().numpy(), label=f'23. {labels[22]}')\n",
    "for border in actual_borders[[3, 9, 13, 17, 22]]:\n",
    "    ax3.axvline(x=border)\n",
    "\n",
    "ax3.title.set_text(\"4. The activation of five phonemes over the audio with actual borders\")\n",
    "ax3.set_xlabel('Audio frames index', fontsize=13)\n",
    "ax3.set_ylabel('Activation', fontsize=13)\n",
    "ax3.legend()\n",
    "\n",
    "# ax4.imshow(transcription.cpu().numpy())\n",
    "# ax4.title.set_text(\"2. One-hot phoneme transcriptions transposed\")\n",
    "# ax4.set_ylabel('Phoneme index', fontsize=13)\n",
    "# ax4.set_xlabel('One-hot phoneme ID', fontsize=13)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjqoJIShjcs_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp: Utterance = test_dataset[ 36  ]  # 36  1168 933\n",
    "\n",
    "audio = inp.features.clone()\n",
    "length = audio.shape[0]\n",
    "\n",
    "label_vec = inp.label_vec\n",
    "actual_borders = inp.border\n",
    "transcription = inp.in_transcription\n",
    "t_count, t_features = transcription.shape\n",
    "\n",
    "# audio = (audio + audio * torch.rand_like(audio) * 3) / 3\n",
    "attention = position_model.with_weights(transcription.unsqueeze(0), None, audio.unsqueeze(0), None).detach().squeeze(0)\n",
    "\n",
    "audio = audio.clone()\n",
    "audio -= audio.min()\n",
    "audio /= audio.max()\n",
    "\n",
    "labels_ids = np.argmax(transcription.cpu(), axis=1).tolist()\n",
    "labels = [KNOWN_LABELS[lab] for lab in labels_ids]\n",
    "labels_pos = actual_borders.cpu().tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(ncols=1, nrows=2, height_ratios=[4, 5])#, width_ratios=[5, 2])\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax1 = fig.add_subplot(gs[0], sharex=ax2)\n",
    "\n",
    "\n",
    "audio = (audio + audio * torch.rand_like(audio) * 3) / 3\n",
    "a = audio.cpu().numpy()\n",
    "a[:,13:] *= np.cumsum(vec).reshape(-1, 1)\n",
    "\n",
    "ax2.imshow(a.T, origin=\"lower\", aspect='auto')\n",
    "ax2.title.set_text(\"2. Audio encoder features\")\n",
    "ax2.set_ylabel('Audio encoder features', fontsize=13)\n",
    "ax2.set_xlabel('Audio frames index', fontsize=13)\n",
    "\n",
    "\n",
    "vec = attention[9].cpu().numpy()\n",
    "print(vec.shape)\n",
    "ax1.plot(vec, label=f'9. Attention score {labels[9]}')\n",
    "ax1.plot(np.cumsum(vec), label=f'9. Cumulative sum {labels[9]}')\n",
    "ax1.title.set_text(f\"1. The attention score and cumulative sum for phoneme {labels[9]}\")\n",
    "ax1.set_xlabel('Audio frames index', fontsize=13)\n",
    "ax1.set_ylabel('Activation', fontsize=13)\n",
    "ax1.legend()\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": false,
    "id": "5DtTLAEPjctC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rt4VGjoijctC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp: Utterance = test_dataset[36]\n",
    "\n",
    "audio = inp.features\n",
    "length = audio.shape[0]\n",
    "label_vec = inp.label_vec\n",
    "actual_borders = inp.border\n",
    "transcription = inp.in_transcription\n",
    "\n",
    "\n",
    "\n",
    "        # f, axarr = plt.subplots(1, 2, figsize=(13, 3))\n",
    "        # axarr[0].bar(range(len(bins)-1), np.round(hist).astype(int))\n",
    "        # axarr[0].set_xticklabels([f\"{x:.0f}{unit}\" for x in bins], fontdict=None, minor=False)\n",
    "        # axarr[0].yaxis.set_major_formatter(FormatStrFormatter('%d%%'))\n",
    "        # axarr[1].hist(errors, bins=15)\n",
    "        # axarr[1].xaxis.set_major_formatter(FormatStrFormatter(f'%d{unit}'))\n",
    "\n",
    "\n",
    "# 10ms b-64  97\n",
    "ms_10 = [\n",
    "    [5, 47.4],\n",
    "    [10, 76.7],\n",
    "    [15, 88.1],\n",
    "    [20, 93.0],\n",
    "]\n",
    "\n",
    "# 5ms  b 16  250.6\n",
    "ms_5 = [\n",
    "    [5, 48.6],\n",
    "    [10, 76.9],\n",
    "    [15, 88.3],\n",
    "    [20, 93.0],\n",
    "]\n",
    "\n",
    "# 15ms b 128 = 26.2\n",
    "ms_15 = [\n",
    "    [5, 34.9],\n",
    "    [10, 66.4],\n",
    "    [15, 83.8],\n",
    "    [20, 91.1],\n",
    "]\n",
    "\n",
    "def autolabel(rects, ax):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 0),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(9, 4), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(ncols=2, nrows=1, width_ratios=[18, 8])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "\n",
    "x = np.arange(4) * 1.5\n",
    "width = 0.4  # the width of the bars\n",
    "rects1 = ax1.bar(x + width * 1, [e[1] for e in ms_5], width, label='5ms step, 16 batch')\n",
    "rects2 = ax1.bar(x + width * 0, [e[1] for e in ms_10], width, label='10ms step, 64 batch')\n",
    "rects3 = ax1.bar(x - width * 1, [e[1] for e in ms_15], width, label='15ms step, 128 batch')\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax1.set_ylabel('Percentage')\n",
    "ax1.set_xlabel('Agreement boundary')\n",
    "ax1.set_title('1. Agreement boundary improvements')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f\"{e[0]}ms boundary\" for e in ms_15])\n",
    "ax1.yaxis.set_major_formatter(FormatStrFormatter('%d%%'))\n",
    "ax1.legend(loc=\"lower right\")\n",
    "autolabel(rects1, ax1)\n",
    "autolabel(rects2, ax1)\n",
    "autolabel(rects3, ax1)\n",
    "\n",
    "print()\n",
    "\n",
    "rects1 = ax2.bar([5, 10, 15], [250.6 / 2, 97.1 / 2, 26.2 / 2], 3, color=[ax2._get_patches_for_fill.get_next_color() for c in '123'])\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax2.set_ylabel('Training time in seconds')\n",
    "ax2.set_xlabel('Audio frame step size')\n",
    "ax2.set_title('2. Training time improvements')\n",
    "ax2.set_xticks([5, 10, 15])\n",
    "ax2.set_xticklabels([f\"{e}ms size\" for e in [5, 10, 15]])\n",
    "ax2.yaxis.set_major_formatter(FormatStrFormatter('%dsec'))\n",
    "autolabel(rects1, ax2)\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-qHc6ppHjcrP",
    "kDPYEFWejcrZ",
    "OBgjckq0jcri",
    "Z_BTpr7ijcrm",
    "g0GGzJpWjcrr",
    "HqUHGv1kjctG",
    "w9_WdkMxjcsQ"
   ],
   "name": "Soft_Pointer_Network_RAW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}