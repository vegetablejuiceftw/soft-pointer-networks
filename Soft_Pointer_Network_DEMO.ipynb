{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "Nv4y7bwShSpy"
   },
   "outputs": [],
   "source": [
    "# Welcome\n",
    "\n",
    "This is the demo for \n",
    "https://github.com/vegetablejuiceftw/soft-pointer-networks\n",
    "\n",
    "Look for updated notebook and models in there :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-58Dfvog3U_"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61.0
    },
    "colab_type": "code",
    "id": "ZB-8EZy_SVFv",
    "outputId": "8f49e59d-d5d8-4ae9-f50d-496dd9ce7283"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "CACHE = {}\n",
    "class SkipExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        print(\"Skip execution\")\n",
    "        clear_output()\n",
    "\n",
    "try:\n",
    "    import python_speech_features\n",
    "except:\n",
    "    pass            \n",
    "else:\n",
    "    raise SkipExecution\n",
    "\n",
    "!pip install kaggle python_speech_features dtw fastdtw dtaidistance AudAugio pyrubberband --upgrade -q \n",
    "!apt install soundstretch rubberband-cli librubberband2 libsndfile1 > /dev/null\n",
    "# !kaggle datasets download -d mfekadu/darpa-timit-acousticphonetic-continuous-speech > /dev/null\n",
    "!gdown -O data.zip --id \"15MxBckNzyEjO7cpY38O38NaWnssShl2l\"\n",
    "!unzip data.zip > /dev/null\n",
    "!git clone https://github.com/vegetablejuiceftw/soft-pointer-networks spn\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-qFXGbZ8tYs"
   },
   "outputs": [],
   "source": [
    "# !cd spn && git fetch && git reset --hard origin/master\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q0KzLSVph9lN"
   },
   "outputs": [],
   "source": [
    "from spn.dataset_loader import *\n",
    "\n",
    "limit = 200  # None means unlimited, else 100 would mean to load only the first 100 files\n",
    "limit = None\n",
    "\n",
    "# To enable training files\n",
    "# train_files = DirectMaskDataset.load_csv('train', sa=False)\n",
    "# train_files, train_eval_files = train_test_split(train_files, test_size=500, random_state=42)\n",
    "# train_dataset = DirectMaskDataset(train_files, limit=limit)\n",
    "# train_eval_dataset = DirectMaskDataset(train_eval_files, limit=limit)\n",
    "\n",
    "# To enable   augmented   training files\n",
    "# train_augment_dataset = DirectMaskDataset(train_files, limit=None, augment=True, duplicate=3)\n",
    "\n",
    "test_files = DirectMaskDataset.load_csv('test', sa=False)  # do not import calibration sentences\n",
    "test_dataset = DirectMaskDataset(test_files, limit=limit, device=device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CahVjw2GqHSw"
   },
   "source": [
    "# Soft pointer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105.0
    },
    "colab_type": "code",
    "id": "A59e3ttEnAaz",
    "outputId": "a824d451-34b0-46e2-da74-7cc806456f85"
   },
   "outputs": [],
   "source": [
    "from spn.models.soft_pointer_network import SoftPointerNetwork\n",
    "\n",
    "soft_pointer_model = SoftPointerNetwork(54, 26, 256, device=device)\n",
    "soft_pointer_model.load(path='spn/trained_weights/position_model-final.pth')\n",
    "\n",
    "print(soft_pointer_model)\n",
    "print(soft_pointer_model.with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xGCRoWEIAIg"
   },
   "source": [
    "# SPN boundary agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    },
    "colab_type": "code",
    "id": "mw-0bDf1wlh8",
    "outputId": "7849474d-bc45-4d14-840e-09fce8601011"
   },
   "outputs": [],
   "source": [
    "from spn.tools import *\n",
    "show_position_batched(soft_pointer_model.with_gradient, test_dataset, duration_combined_model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMUaPLaTIrYs"
   },
   "source": [
    "# SPN visual examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    },
    "colab_type": "code",
    "id": "x--jOyq9IxGJ",
    "outputId": "e7a474af-2029-480f-e921-1057d3a41ae4"
   },
   "outputs": [],
   "source": [
    "inp: Utterance = test_dataset[ 36  ]  # 36  1168 933\n",
    "\n",
    "audio = inp.features.clone()\n",
    "length = audio.shape[0]\n",
    "\n",
    "label_vec = inp.label_vec\n",
    "actual_borders = inp.border\n",
    "transcription = inp.in_transcription\n",
    "t_count, t_features = transcription.shape\n",
    "\n",
    "attention = soft_pointer_model.with_weights(transcription.unsqueeze(0), None, audio.unsqueeze(0), None).detach().squeeze(0)\n",
    "\n",
    "audio = audio.clone()\n",
    "audio -= audio.min()\n",
    "audio /= audio.max()\n",
    "\n",
    "labels_ids = np.argmax(transcription.cpu(), axis=1).tolist()\n",
    "labels = [KNOWN_LABELS[lab] for lab in labels_ids]\n",
    "labels_pos = actual_borders.cpu().tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 9), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(ncols=1, nrows=3, height_ratios=[3, 10, 3])#, width_ratios=[5, 2])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "ax3 = fig.add_subplot(gs[2, 0], sharex=ax1)\n",
    "# ax4 = fig.add_subplot(gs[1, 1], sharey=ax2)\n",
    "\n",
    "ax1.imshow(audio.cpu().numpy().T, origin=\"lower\", aspect='auto')\n",
    "ax1.title.set_text(\"1. Audio features\")\n",
    "ax1.set_ylabel('Audio features', fontsize=13)\n",
    "ax1.set_xlabel('Audio frames index', fontsize=13)\n",
    "\n",
    "ax2.imshow(attention.cpu().numpy(), origin=\"lower\", aspect='auto')\n",
    "ax2.title.set_text(\"3. Attention matrix\")\n",
    "ax2.set_xlabel('Audio frames index', fontsize=13)\n",
    "ax2.set_ylabel('Phoneme index', fontsize=13)\n",
    "ax2.tick_params(axis='both', which='both', labelsize=13)\n",
    "ax2.set_yticks(list( range(t_count)))\n",
    "ax2.set_yticklabels([f\"{i+1}. {p}{(i%2 == 6) * ' - - '}\" for i, p in enumerate(labels)])\n",
    "\n",
    "ax3.plot(attention[3].cpu().numpy(), label=f'4. {labels[3]}')\n",
    "ax3.plot(attention[10 - 1].cpu().numpy(), label=f'10. {labels[9]}')\n",
    "ax3.plot(attention[14 - 1].cpu().numpy(), label=f'14. {labels[13]}')\n",
    "ax3.plot(attention[18 - 1].cpu().numpy(), label=f'18. {labels[17]}')\n",
    "ax3.plot(attention[23 - 1].cpu().numpy(), label=f'23. {labels[22]}')\n",
    "for border in actual_borders[[3, 9, 13, 17, 22]]:\n",
    "    ax3.axvline(x=border)\n",
    "\n",
    "ax3.title.set_text(\"4. The activation of four phonemes over the audio with actual borders\")\n",
    "ax3.set_xlabel('Audio frames index', fontsize=13)\n",
    "ax3.set_ylabel('Activation', fontsize=13)\n",
    "ax3.legend()\n",
    "\n",
    "# ax4.imshow(transcription.cpu().numpy())\n",
    "# ax4.title.set_text(\"2. One-hot phoneme transcriptions transposed\")\n",
    "# ax4.set_ylabel('Phoneme index', fontsize=13)\n",
    "# ax4.set_xlabel('One-hot phoneme ID', fontsize=13)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 2), constrained_layout=True)\n",
    "plt.plot(attention[3].cpu().numpy(), label=f'4. {labels[3]}')\n",
    "plt.plot(attention[10 - 1].cpu().numpy(), label=f'10. {labels[9]}')\n",
    "plt.plot(attention[14 - 1].cpu().numpy(), label=f'14. {labels[13]}')\n",
    "plt.plot(attention[18 - 1].cpu().numpy(), label=f'18. {labels[17]}')\n",
    "plt.plot(attention[23 - 1].cpu().numpy(), label=f'23. {labels[22]}')\n",
    "for border in actual_borders[[3, 9, 13, 17, 22]]:\n",
    "    plt.axvline(x=border)\n",
    "plt.xlim([0, length-1])\n",
    "plt.title(\"The activation of four phonemes over the audio with actual borders\")\n",
    "plt.xlabel('Audio frames index', fontsize=13)\n",
    "plt.ylabel('Activation', fontsize=13)\n",
    "plt.legend()\n",
    "plt.plot()\n",
    "\n",
    "\n",
    "index = 9\n",
    "fig = plt.figure(figsize=(10, 4), constrained_layout=True)\n",
    "att_local = attention[index].cpu().numpy()\n",
    "predicted_border = (np.arange(length) * attention[index].cpu().numpy()).sum()\n",
    "\n",
    "plt.plot(att_local, label=f'{index+1}. {labels[index]}', color=[ax2._get_patches_for_fill.get_next_color() for c in '123'][1])\n",
    "# for border in actual_borders[[9]]:\n",
    "#     plt.axvline(x=border, c='g')\n",
    "\n",
    "s, e = 30, 65\n",
    "for i, y in list(enumerate(att_local))[s:e:2]:\n",
    "    plt.text(i-0.3, y, f'{y:.1f} * {i}', rotation=65)\n",
    "\n",
    "\n",
    "plt.xlim([s, e])\n",
    "plt.ylim([-0.05, 0.35])\n",
    "\n",
    "plt.axvline(x=actual_borders[index]-0.15, c='g', label=f'Actual border ({actual_borders[index]:.1f})')\n",
    "plt.axvline(x=predicted_border, c='b', label=f'Soft poitner border ({predicted_border:.1f})')\n",
    "plt.axvline(x=50, c='r', label='Argmax border (50)')\n",
    "\n",
    "plt.title(\"The activation value of phoneme /iy/ multiplied by the audio index value\", fontsize=13)\n",
    "plt.xlabel('Audio frames index', fontsize=13)\n",
    "plt.ylabel('Activation', fontsize=13)\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaiPb3OVIxyo"
   },
   "outputs": [],
   "source": [
    "def display_uterance(inp: Utterance):\n",
    "    audio = inp.features.clone()\n",
    "    length = audio.shape[0]\n",
    "\n",
    "    label_vec = inp.label_vec\n",
    "    actual_borders = inp.border\n",
    "    transcription = inp.in_transcription\n",
    "    t_count, t_features = transcription.shape\n",
    "\n",
    "\n",
    "    # audio = (audio + audio * torch.rand_like(audio) * 3) / 3\n",
    "    attention = soft_pointer_model.with_weights(transcription.unsqueeze(0), None, audio.unsqueeze(0), None).detach()\n",
    "    positions = soft_pointer_model.weights_to_positions(attention).detach().squeeze(0).cpu().numpy()\n",
    "    attention = attention.squeeze(0)\n",
    "\n",
    "    audio = audio.clone()\n",
    "    audio -= audio.min()\n",
    "    audio /= audio.max()\n",
    "\n",
    "    labels_ids = np.argmax(transcription.cpu(), axis=1).tolist()[:-1]\n",
    "    labels = [KNOWN_LABELS[lab] for lab in labels_ids]\n",
    "    labels_pos = actual_borders.cpu().tolist()\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 16), constrained_layout=True)\n",
    "    gs = gridspec.GridSpec(ncols=1, nrows=3, height_ratios=[3, 3, 6])#, width_ratios=[5, 2])\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "    ax3 = fig.add_subplot(gs[2, 0], sharex=ax1)\n",
    "\n",
    "    ax1.imshow(audio.cpu().numpy().T, origin=\"lower\", aspect='auto')\n",
    "    ax1.title.set_text(\"1. Audio features\")\n",
    "    ax1.set_ylabel('Audio features', fontsize=13)\n",
    "    ax1.set_xlabel('Audio frames index', fontsize=13)\n",
    "\n",
    "    label_colors = [ax2._get_patches_for_fill.get_next_color() for c in labels_ids]\n",
    "    for i in range(0, len(labels_ids), 1):\n",
    "        c = label_colors[i]\n",
    "        ax2.plot(attention[i].cpu().numpy(), label=f'{i+1}. {labels[i]}', c=c)\n",
    "        ax2.axvline(x=labels_pos[i], c=c, linestyle='-')\n",
    "        ax2.axvline(x=positions[i], c=c, linestyle='-.')\n",
    "\n",
    "    # ax2.set_xlim([10, 120])\n",
    "    ax2.set_ylim([-0.05, 1.1])\n",
    "\n",
    "    ax2.set_title(\"Predicted borders (dotted) compared with actual borders (solid)\", fontsize=13)\n",
    "    ax2.set_xlabel('Audio frames index', fontsize=13)\n",
    "    ax2.set_ylabel('Activation', fontsize=13)\n",
    "\n",
    "    ax3.imshow(attention.cpu().numpy(), origin=\"lower\", aspect='auto')\n",
    "    ax3.title.set_text(\"3. Attention matrix\")\n",
    "    ax3.set_xlabel('Audio frames index', fontsize=13)\n",
    "    ax3.set_ylabel('Phoneme index', fontsize=13)\n",
    "    ax3.tick_params(axis='both', which='both', labelsize=13)\n",
    "    ax3.set_yticks(list( range(t_count)))\n",
    "    ax3.set_yticklabels([f\"{i+1}. {p}{(i%2 == 6) * ' - - '}\" for i, p in enumerate(labels)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4oKKo-rL-9n"
   },
   "source": [
    "## EX 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868.0
    },
    "colab_type": "code",
    "id": "wmaQmOaJKAS5",
    "outputId": "94749341-92d2-4433-dac3-0457524ce131"
   },
   "outputs": [],
   "source": [
    "inp: Utterance = test_dataset[36]   # < - - - - - - - change this\n",
    "display_uterance(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kY8d2Rv8MBBk"
   },
   "source": [
    "## EX 136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868.0
    },
    "colab_type": "code",
    "id": "vvIudUd2MC97",
    "outputId": "f963613d-2c93-49c8-d2a3-d4be0b1b4243"
   },
   "outputs": [],
   "source": [
    "inp: Utterance = test_dataset[136]   # < - - - - - - - change this\n",
    "display_uterance(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCIgmFcaQ2BI"
   },
   "source": [
    "# SPN Position encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275.0
    },
    "colab_type": "code",
    "id": "Oicf58FYMHgZ",
    "outputId": "50405784-1b42-46f5-a3de-5cb70a8b3c43"
   },
   "outputs": [],
   "source": [
    "inp: Utterance = test_dataset[36]\n",
    "\n",
    "audio = inp.features.clone()\n",
    "length = audio.shape[0]\n",
    "\n",
    "label_vec = inp.label_vec\n",
    "actual_borders = inp.border\n",
    "transcription = inp.in_transcription\n",
    "t_count, t_features = transcription.shape\n",
    "\n",
    "# return position encodings\n",
    "pos_encoding_out = soft_pointer_model.with_position(transcription.unsqueeze(0), None, audio.unsqueeze(0), None).detach().cpu().squeeze(0)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(ncols=3, nrows=1)\n",
    "\n",
    "labels_ids = np.argmax(transcription.cpu(), axis=1).tolist()\n",
    "labels = [KNOWN_LABELS[lab] for lab in labels_ids]\n",
    "labels_pos = actual_borders.cpu().tolist()\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax2 = fig.add_subplot(gs[1], sharey=ax1)\n",
    "ax3 = fig.add_subplot(gs[2], sharey=ax1)\n",
    "\n",
    "ax1.imshow(inp.position.cpu().numpy().T, aspect='auto')\n",
    "ax1.title.set_text(\"1. Position encoding actual\")\n",
    "ax1.set_xlabel('Phonemes ordered', fontsize=13)\n",
    "ax1.set_ylabel('Position encoding features', fontsize=13)\n",
    "ax1.tick_params(axis='both', which='both', labelsize=13)\n",
    "ax1.set_xticks(list(range(t_count - 1))[::2])\n",
    "ax1.set_xticklabels([f\"{i+1}. {p}{(i%2 == 6) * ' - - '}\" for i, p in enumerate(labels)][::2], rotation=45)\n",
    "\n",
    "ax2.imshow(pos_encoding_out.cpu().numpy().T, aspect='auto')\n",
    "ax2.title.set_text(\"2. Position encoding predicted\")\n",
    "ax2.set_xlabel('Phonemes ordered', fontsize=13)\n",
    "ax2.tick_params(axis='both', which='both', labelsize=13)\n",
    "ax2.set_xticks(list(range(t_count - 1))[::2])\n",
    "ax2.set_xticklabels([f\"{i+1}. {p}{(i%2 == 6) * ' - - '}\" for i, p in enumerate(labels)][::2], rotation=45)\n",
    "\n",
    "ax3.imshow((pos_encoding_out - inp.position).abs().cpu().numpy().T, aspect='auto')\n",
    "ax3.title.set_text(\"3. Difference\")\n",
    "ax3.set_xlabel('Phonemes ordered', fontsize=13)\n",
    "ax3.tick_params(axis='both', which='both', labelsize=13)\n",
    "ax3.set_xticks(list(range(t_count - 1))[::2])\n",
    "ax3.set_xticklabels([f\"{i+1}. {p}{(i%2 == 6) * ' - - '}\" for i, p in enumerate(labels)][::2], rotation=45)\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujOP7T9nQ5FD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Soft Pointer Network DEMO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
